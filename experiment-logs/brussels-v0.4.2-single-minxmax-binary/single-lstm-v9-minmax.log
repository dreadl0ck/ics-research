2020-02-08 00:31:26.755359: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-02-08 00:31:26.755412: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-02-08 00:31:26.755419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-02-08 00:31:27.246869: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-02-08 00:31:27.246888: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-02-08 00:31:27.246903: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (brussels): /proc/driver/nvidia/version does not exist
2020-02-08 00:31:27.247009: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-08 00:31:27.269812: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3312000000 Hz
2020-02-08 00:31:27.270002: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47fa0f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-08 00:31:27.270013: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow backend.
WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
2020-02-08 00:31:30.365508: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:31:30.387798: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:30.396797: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:30.474469: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-08 00:31:30.478885: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:31:30.485252: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:30.491318: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:30.509180: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.395031: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:31:33.399892: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.402253: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.418656: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.419758: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:31:33.421341: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.422914: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:31:33.428999: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
=================================================
        TRAINING v0.4.2 (binaryClasses)
=================================================
Date: 2020-02-08 00:31:27.243507
[INFO] input_shape (32, 16)
[INFO] LSTM first and last layer neurons: 8
[INFO] adding core layer 0
[INFO] adding core layer 1
[INFO] adding core layer 2
wrapLayerSize 8
coreLayerSize 32
numCoreLayers 3
outputLayerActivation sigmoid
output_dim 2
loss binary_crossentropy
optimizer sgd
[INFO] created DNN
[33m[INFO] epoch 1/10[0m
[33m[INFO] loading file 1-2/1 on epoch 1/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 4s - loss: 0.6122 - tp: 79141.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 859.0000 - accuracy: 0.9946 - precision: 1.0000 - recall: 0.9893 - auc: 0.9987 - val_loss: 0.5354 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.4924 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.4311 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.4156 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9967 - val_loss: 0.7676 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.7636 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5311 - val_loss: 0.7288 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.7267 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5311 - val_loss: 0.7111 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-2-batch-400000-500000
[33m[LOSS] 0.7110974405288696[0m
[33m[INFO] epoch 2/10[0m
[33m[INFO] loading file 1-2/1 on epoch 2/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.4285 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3761 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-002-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3610 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3113 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-002-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3152 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9975 - val_loss: 0.8371 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-002-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8204 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5314 - val_loss: 0.7701 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-002-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.7600 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5311 - val_loss: 0.7371 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-002-files-0-2-batch-400000-500000
[33m[LOSS] 0.7371424837112427[0m
[33m[INFO] epoch 3/10[0m
[33m[INFO] loading file 1-2/1 on epoch 3/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3611 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3177 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-003-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3061 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2656 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-003-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2718 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9968 - val_loss: 0.8748 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-003-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8492 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5301 - val_loss: 0.7960 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-003-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.7801 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5306 - val_loss: 0.7554 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-003-files-0-2-batch-400000-500000
[33m[LOSS] 0.7553514332771302[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.7553514332771302  <  0.001
[33m[INFO] epoch 4/10[0m
[33m[INFO] loading file 1-2/1 on epoch 4/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3271 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2898 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-004-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2777 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2435 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-004-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2474 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9973 - val_loss: 0.8970 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-004-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8655 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5306 - val_loss: 0.8133 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-004-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.7934 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5297 - val_loss: 0.7685 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-004-files-0-2-batch-400000-500000
[33m[LOSS] 0.7684878932952881[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.7684878932952881  <  0.001
[33m[INFO] epoch 5/10[0m
[33m[INFO] loading file 1-2/1 on epoch 5/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.3064 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2732 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-005-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2599 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2302 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-005-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2321 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9972 - val_loss: 0.9121 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-005-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8764 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5300 - val_loss: 0.8261 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-005-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8035 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5296 - val_loss: 0.7785 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-005-files-0-2-batch-400000-500000
[33m[LOSS] 0.7785089614868164[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.7785089614868164  <  0.001
[33m[INFO] epoch 6/10[0m
[33m[INFO] loading file 1-2/1 on epoch 6/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2931 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2622 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-006-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2479 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2215 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-006-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2216 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9973 - val_loss: 0.9230 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-006-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8845 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5311 - val_loss: 0.8356 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-006-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8110 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5311 - val_loss: 0.7862 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-006-files-0-2-batch-400000-500000
[33m[LOSS] 0.7862311559677124[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.7862311559677124  <  0.001
[33m[INFO] epoch 7/10[0m
[33m[INFO] loading file 1-2/1 on epoch 7/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2824 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2538 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-007-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2384 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2146 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-007-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2138 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9969 - val_loss: 0.9321 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-007-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8914 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5313 - val_loss: 0.8439 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-007-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8179 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5323 - val_loss: 0.7934 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-007-files-0-2-batch-400000-500000
[33m[LOSS] 0.7933923038482666[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.7933923038482666  <  0.001
[33m[INFO] epoch 8/10[0m
[33m[INFO] loading file 1-2/1 on epoch 8/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2739 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2467 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-008-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2316 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2092 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-008-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2073 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9969 - val_loss: 0.9394 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-008-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8975 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5305 - val_loss: 0.8497 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-008-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8231 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5300 - val_loss: 0.8011 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-008-files-0-2-batch-400000-500000
[33m[LOSS] 0.801121257019043[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.801121257019043  <  0.001
[33m[INFO] epoch 9/10[0m
[33m[INFO] loading file 1-2/1 on epoch 9/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2725 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2466 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-009-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2292 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2083 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-009-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2045 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9975 - val_loss: 0.9413 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-009-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8988 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5303 - val_loss: 0.8529 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-009-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8275 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5310 - val_loss: 0.8100 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-009-files-0-2-batch-400000-500000
[33m[LOSS] 0.809992251586914[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.809992251586914  <  0.001
[33m[INFO] epoch 10/10[0m
[33m[INFO] loading file 1-2/1 on epoch 10/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2664 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2430 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-010-files-0-2-batch-0-100000
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2253 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 80000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2054 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 20000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-010-files-0-2-batch-100000-200000
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.2009 - tp: 79766.0000 - fp: 234.0000 - tn: 79766.0000 - fn: 234.0000 - accuracy: 0.9971 - precision: 0.9971 - recall: 0.9971 - auc: 0.9972 - val_loss: 0.9456 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 10606.0000 - val_fn: 9394.0000 - val_accuracy: 0.5303 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.5303
[INFO] saving weights to checkpoints/lstm-epoch-010-files-0-2-batch-200000-300000
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.9020 - tp: 42447.0000 - fp: 37553.0000 - tn: 42447.0000 - fn: 37553.0000 - accuracy: 0.5306 - precision: 0.5306 - recall: 0.5306 - auc: 0.5314 - val_loss: 0.8576 - val_tp: 10626.0000 - val_fp: 9374.0000 - val_tn: 10626.0000 - val_fn: 9374.0000 - val_accuracy: 0.5313 - val_precision: 0.5313 - val_recall: 0.5313 - val_auc: 0.5313
[INFO] saving weights to checkpoints/lstm-epoch-010-files-0-2-batch-300000-400000
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] using LSTM layers
[INFO] fitting model
Train on 2500 samples, validate on 625 samples
Epoch 1/1
 - 3s - loss: 0.8332 - tp: 42438.0000 - fp: 37562.0000 - tn: 42438.0000 - fn: 37562.0000 - accuracy: 0.5305 - precision: 0.5305 - recall: 0.5305 - auc: 0.5300 - val_loss: 0.8176 - val_tp: 10603.0000 - val_fp: 9397.0000 - val_tn: 10603.0000 - val_fn: 9397.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-010-files-0-2-batch-400000-500000
[33m[LOSS] 0.8176026527404785[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.8176026527404785  <  0.001
--- 139.96242547035217 seconds ---
2020-02-08 00:33:48.098109: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-02-08 00:33:48.098154: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-02-08 00:33:48.098160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-02-08 00:33:48.586814: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-02-08 00:33:48.586834: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-02-08 00:33:48.586848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (brussels): /proc/driver/nvidia/version does not exist
2020-02-08 00:33:48.586956: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-08 00:33:48.609849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3312000000 Hz
2020-02-08 00:33:48.610139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5134560 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-08 00:33:48.610166: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow backend.
WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
2020-02-08 00:33:51.051719: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:33:51.056132: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:33:51.058299: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:33:51.073264: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-08 00:33:51.074273: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-08 00:33:51.075719: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:33:51.077140: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-08 00:33:51.080862: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
=================================================
        SCORING v0.4.2 (binaryClasses)
=================================================
Date: 2020-02-08 00:33:48.583525
[INFO] input_shape (32, 16)
[INFO] LSTM first and last layer neurons: 8
[INFO] adding core layer 0
[INFO] adding core layer 1
[INFO] adding core layer 2
wrapLayerSize 8
coreLayerSize 32
numCoreLayers 3
outputLayerActivation sigmoid
output_dim 2
loss binary_crossentropy
optimizer sgd
[INFO] created DNN
loading weights: checkpoints/*
loading file checkpoints/lstm-epoch-010-files-0-2-batch-400000-500000
[33m[INFO] model summary:[0m
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 32, 8)             800       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 8)             0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 32, 32)            5248      
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32)            0         
_________________________________________________________________
lstm_3 (LSTM)                (None, 32, 32)            8320      
_________________________________________________________________
dropout_3 (Dropout)          (None, 32, 32)            0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 32, 32)            8320      
_________________________________________________________________
dropout_4 (Dropout)          (None, 32, 32)            0         
_________________________________________________________________
lstm_5 (LSTM)                (None, 32, 8)             1312      
_________________________________________________________________
dropout_5 (Dropout)          (None, 32, 8)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32, 1)             9         
_________________________________________________________________
dropout_6 (Dropout)          (None, 32, 1)             0         
_________________________________________________________________
dense_2 (Dense)              (None, 32, 2)             4         
=================================================================
Total params: 24,013
Trainable params: 24,013
Non-trainable params: 0
_________________________________________________________________
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 17)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** unixtime:1359 (0.2718%)
[INFO] ** orig:[0.5:100.0%]
[INFO] ** type:[1.0:100.0%]
[INFO] ** i/f_name:[0.0:99.9996%,0.5:0.0004%]
[INFO] ** i/f_dir:[0.0:99.9996%,1.0:0.0004%]
[INFO] ** src:[0.0666666667:38.3832%,0.9333333333:25.9606%,0.6666666667:18.1352%,0.3333333333:17.52%,0.1333333333:0.0004%,1.0:0.0004%,0.5333333333:0.0002%]
[INFO] ** dst:[0.1764705882:43.3108%,0.9411764706:21.0326%,0.8823529412000001:18.135%,0.5294117647:17.5198%,0.0:0.0008%,0.2941176471:0.0004%,0.4117647059:0.0004%,0.23529411760000002:0.0002%]
[INFO] ** proto:[0.5:99.9996%,0.0:0.0004%]
[INFO] ** appi_name:[0.0:99.9992%,0.0384615385:0.0004%,0.5384615385:0.0002%,0.7307692308:0.0002%]
[INFO] ** proxy_src_ip:[0.6666666667:38.3832%,0.0:25.9606%,0.9333333333:18.1352%,0.4:17.52%,0.5333333333:0.0004%,0.7333333333:0.0004%,0.6:0.0002%]
[INFO] ** modbus_function_code:[0.9743589744:99.9992%,0.0:0.0008%]
[INFO] ** modbus_function_description:[0.2:50.0%,0.0:49.9992%,0.4:0.0008%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[1.0:25.9596%,0.6:21.0326%,0.8:18.1348%,0.0:17.5194%,0.4:17.3504%,0.2:0.0032%]
[INFO] ** service:[0.7025755984000001:99.9992%,0.0:0.0004%,0.0012540954:0.0002%,0.002116286:0.0002%]
[INFO] ** s_port:[0.834280824:25.96%,0.8139250565:21.0326%,0.8029830673999999:18.1348%,0.8149336757:17.5196%,0.8137722355:17.3506%,0.8343113882:0.0004%,0.0:0.0004%,0.8059172320999999:0.0002%,0.8030136315999999:0.0002%,0.8147502904:0.0002%,0.8030441959000001:0.0002%,0.814841983:0.0002%,0.9621003729000001:0.0002%,0.8345253376999999:0.0002%,0.7788831836:0.0002%]
[INFO] ** classification:[normal:72.7902%,Single Stage Single Point:27.2098%]
[INFO] columns with count within 2-10 {'i/f_name': 2, 'i/f_dir': 2, 'src': 7, 'dst': 8, 'proto': 2, 'appi_name': 4, 'proxy_src_ip': 7, 'modbus_function_code': 2, 'modbus_function_description': 3, 'scada_tag': 6, 'service': 4, 'classification': 2}
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 16)
y_eval [1 0 1 ... 0 1 0] (100000,)
[INFO] reshape for using LSTM layers
[33m[INFO] metrics:[0m
loss :  0.8174985063171387
tp :  53024.0
fp :  46976.0
tn :  53024.0
fn :  46976.0
accuracy :  0.530239999294281
precision :  0.530239999294281
recall :  0.530239999294281
auc :  0.5302400588989258

y_eval {0: 53024, 1: 46976}
pred {0: 100000}
[INFO] confusion matrix for file 
[[53024     0]
 [46976     0]]
[INFO] confusion matrix after adding it to total:
[[53024     0]
 [46976     0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 16)
y_eval [0 1 0 ... 0 0 0] (100000,)
[INFO] reshape for using LSTM layers
[33m[INFO] metrics:[0m
loss :  0.4171502990245819
tp :  87619.0
fp :  12381.0
tn :  87619.0
fn :  12381.0
accuracy :  0.8761900067329407
precision :  0.8761900067329407
recall :  0.8761900067329407
auc :  0.8761899471282959

y_eval {0: 87619, 1: 12381}
pred {0: 100000}
[INFO] confusion matrix for file 
[[87619     0]
 [12381     0]]
[INFO] confusion matrix after adding it to total:
[[140643      0]
 [ 59357      0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 16)
y_eval [0 0 0 ... 1 0 1] (100000,)
[INFO] reshape for using LSTM layers
[33m[INFO] metrics:[0m
loss :  0.4703140423202515
tp :  83025.0
fp :  16975.0
tn :  83025.0
fn :  16975.0
accuracy :  0.8302500247955322
precision :  0.8302500247955322
recall :  0.8302500247955322
auc :  0.8302500247955322

y_eval {0: 83025, 1: 16975}
pred {0: 100000}
[INFO] confusion matrix for file 
[[83025     0]
 [16975     0]]
[INFO] confusion matrix after adding it to total:
[[223668      0]
 [ 76332      0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 16)
y_eval [1 0 0 ... 1 1 0] (100000,)
[INFO] reshape for using LSTM layers
[33m[INFO] metrics:[0m
loss :  0.8186673163986206
tp :  52923.0
fp :  47077.0
tn :  52923.0
fn :  47077.0
accuracy :  0.529229998588562
precision :  0.529229998588562
recall :  0.529229998588562
auc :  0.529229998588562

y_eval {0: 52923, 1: 47077}
pred {0: 100000}
[INFO] confusion matrix for file 
[[52923     0]
 [47077     0]]
[INFO] confusion matrix after adding it to total:
[[276591      0]
 [123409      0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 16)
y_eval [0 0 1 ... 0 0 0] (100000,)
[INFO] reshape for using LSTM layers
[33m[INFO] metrics:[0m
loss :  0.42014756373405454
tp :  87360.0
fp :  12640.0
tn :  87360.0
fn :  12640.0
accuracy :  0.8736000061035156
precision :  0.8736000061035156
recall :  0.8736000061035156
auc :  0.8736000061035156

y_eval {0: 87360, 1: 12640}
pred {0: 100000}
[INFO] confusion matrix for file 
[[87360     0]
 [12640     0]]
[INFO] confusion matrix after adding it to total:
[[363951      0]
 [136049      0]]
--- 9.340070247650146 seconds ---
