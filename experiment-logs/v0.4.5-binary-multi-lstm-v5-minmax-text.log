2020-02-09 23:48:55.964302: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
2020-02-09 23:48:55.964478: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
2020-02-09 23:48:55.964494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-02-09 23:48:56.766508: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-02-09 23:48:56.766564: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
2020-02-09 23:48:56.766592: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bastia): /proc/driver/nvidia/version does not exist
2020-02-09 23:48:56.766747: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-09 23:48:56.775964: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112050000 Hz
2020-02-09 23:48:56.777243: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5261ca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-09 23:48:56.777281: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow backend.
WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
2020-02-09 23:54:31.914320: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-09 23:54:31.931254: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:31.938864: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:31.998660: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-09 23:54:32.002274: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-09 23:54:32.007528: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:32.012787: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:32.026591: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.893770: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-09 23:54:40.898330: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.900462: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.916284: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.917351: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:561] remapper failed: Invalid argument: MutableGraphView::MutableGraphView error: node 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat' has self cycle fanin 'loss/dense_2_loss/binary_crossentropy/weighted_loss/concat'.
2020-02-09 23:54:40.918875: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.920404: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:717] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2020-02-09 23:54:40.925558: W tensorflow/core/common_runtime/process_function_library_runtime.cc:697] Ignoring multi-device function optimization failure: Invalid argument: The graph couldn't be sorted in topological order.
=================================================
        TRAINING v0.4.5 (binary)
=================================================
Date: 2020-02-09 23:48:56.759773
------------DNN info-------------
dnnBatchSize 16
wrapLayerSize 16
coreLayerSize 64
numCoreLayers 1
outputLayerActivation sigmoid
output_dim 2
loss binary_crossentropy
optimizer sgd
------------DNN info-------------
[INFO] input_shape (16, 107)
[INFO] LSTM first and last layer neurons: 16
[INFO] adding core layer 0
[INFO] created DNN
[33m[INFO] epoch 1/3[0m
[33m[INFO] loading file 1-50/50 on epoch 1/3[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part01_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part02_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part03_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part05_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part06_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part08_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-28_164554_99.log.part09_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_134830_103.log.part08_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_134830_103.log.part09_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_134830_103.log.part10_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_190411_104.log.part03_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_190411_104.log.part04_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_190411_104.log.part13_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-29_190411_104.log.part14_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_001940_105.log.part11_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_053515_106.log.part05_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_105053_107.log.part12_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_105053_107.log.part13_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_212042_109.log.part03_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_212042_109.log.part04_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-30_212042_109.log.part05_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_023431_110.log.part11_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_023431_110.log.part12_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_023431_110.log.part13_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_023431_110.log.part14_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part01_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part02_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part03_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part04_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part05_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part06_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part07_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part08_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part09_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part10_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part11_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part12_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part13_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_074818_111.log.part14_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part01_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part02_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part03_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part04_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part05_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part06_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part07_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part08_sorted-labeled.csv
[INFO] reading file data/SWaT2015-Attack-Files-v0.4.3-minmax-text/train/2015-12-31_130240_112.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (24819975, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
[INFO] columns: Index(['unixtime', 'orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst',
       'proto', 'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] Shape when encoding dataset: (24819975, 17)
[33mencode_text_dummy orig[0m
[33mencode_text_dummy type[0m
[33mencode_text_dummy i/f_name[0m
[33mencode_text_dummy i/f_dir[0m
[33mencode_text_dummy src[0m
[33mencode_text_dummy dst[0m
[33mencode_text_dummy proto[0m
[33mencode_text_dummy appi_name[0m
[33mencode_text_dummy proxy_src_ip[0m
[33mencode_text_dummy modbus_function_description[0m
[33mencode_text_dummy scada_tag[0m
missing -1
len(df.columns) 108
numFeatures 107
[INFO] Shape AFTER encoding dataset: (24819975, 108)
[INFO] processing batch 0-256000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 11s - loss: 0.5173 - tp: 191173.0000 - fp: 404.0000 - tn: 191596.0000 - fn: 827.0000 - accuracy: 0.9968 - precision: 0.9979 - recall: 0.9957 - auc: 0.9998 - val_loss: 0.3859 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-0-256000
[INFO] processing batch 256000-512000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.6900 - tp: 112918.0000 - fp: 79082.0000 - tn: 112918.0000 - fn: 79082.0000 - accuracy: 0.5881 - precision: 0.5881 - recall: 0.5881 - auc: 0.5878 - val_loss: 0.7208 - val_tp: 33927.0000 - val_fp: 30073.0000 - val_tn: 33927.0000 - val_fn: 30073.0000 - val_accuracy: 0.5301 - val_precision: 0.5301 - val_recall: 0.5301 - val_auc: 0.5301
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-256000-512000
[INFO] processing batch 512000-768000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.6007 - tp: 138284.0000 - fp: 53716.0000 - tn: 138284.0000 - fn: 53716.0000 - accuracy: 0.7202 - precision: 0.7202 - recall: 0.7202 - auc: 0.7225 - val_loss: 0.4176 - val_tp: 62058.0000 - val_fp: 1942.0000 - val_tn: 62058.0000 - val_fn: 1942.0000 - val_accuracy: 0.9697 - val_precision: 0.9697 - val_recall: 0.9697 - val_auc: 0.9697
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-512000-768000
[INFO] processing batch 768000-1024000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.6742 - tp: 117250.0000 - fp: 74750.0000 - tn: 117250.0000 - fn: 74750.0000 - accuracy: 0.6107 - precision: 0.6107 - recall: 0.6107 - auc: 0.6114 - val_loss: 0.5105 - val_tp: 55591.0000 - val_fp: 8409.0000 - val_tn: 55591.0000 - val_fn: 8409.0000 - val_accuracy: 0.8686 - val_precision: 0.8686 - val_recall: 0.8686 - val_auc: 0.8686
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-768000-1024000
[INFO] processing batch 1024000-1280000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.6160 - tp: 133967.0000 - fp: 58033.0000 - tn: 133967.0000 - fn: 58033.0000 - accuracy: 0.6977 - precision: 0.6977 - recall: 0.6977 - auc: 0.6965 - val_loss: 0.4043 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-1024000-1280000
[INFO] processing batch 1280000-1536000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.3235 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2601 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-1280000-1536000
[INFO] processing batch 1536000-1792000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.2199 - tp: 191999.0000 - fp: 1.0000 - tn: 191999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1867 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-1536000-1792000
[INFO] processing batch 1792000-2048000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.5451 - tp: 148221.0000 - fp: 43779.0000 - tn: 148221.0000 - fn: 43779.0000 - accuracy: 0.7720 - precision: 0.7720 - recall: 0.7720 - auc: 0.7711 - val_loss: 0.3444 - val_tp: 57941.0000 - val_fp: 6059.0000 - val_tn: 57941.0000 - val_fn: 6059.0000 - val_accuracy: 0.9053 - val_precision: 0.9053 - val_recall: 0.9053 - val_auc: 0.9053
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-1792000-2048000
[INFO] processing batch 2048000-2304000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.5887 - tp: 141714.0000 - fp: 50286.0000 - tn: 141714.0000 - fn: 50286.0000 - accuracy: 0.7381 - precision: 0.7381 - recall: 0.7381 - auc: 0.7383 - val_loss: 0.2280 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-2048000-2304000
[INFO] processing batch 2304000-2560000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1957 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1686 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-2304000-2560000
[INFO] processing batch 2560000-2816000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1492 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1324 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-2560000-2816000
[INFO] processing batch 2816000-3072000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.5647 - tp: 148720.0000 - fp: 43280.0000 - tn: 148720.0000 - fn: 43280.0000 - accuracy: 0.7746 - precision: 0.7746 - recall: 0.7746 - auc: 0.7740 - val_loss: 0.7974 - val_tp: 40961.0000 - val_fp: 23039.0000 - val_tn: 40961.0000 - val_fn: 23039.0000 - val_accuracy: 0.6400 - val_precision: 0.6400 - val_recall: 0.6400 - val_auc: 0.6400
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-2816000-3072000
[INFO] processing batch 3072000-3328000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.6939 - tp: 131495.0000 - fp: 60505.0000 - tn: 131495.0000 - fn: 60505.0000 - accuracy: 0.6849 - precision: 0.6849 - recall: 0.6849 - auc: 0.6855 - val_loss: 0.2016 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-3072000-3328000
[INFO] processing batch 3328000-3584000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1753 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1529 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-3328000-3584000
[INFO] processing batch 3584000-3840000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1366 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1223 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-3584000-3840000
[INFO] processing batch 3840000-4096000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.4850 - tp: 157534.0000 - fp: 34466.0000 - tn: 157534.0000 - fn: 34466.0000 - accuracy: 0.8205 - precision: 0.8205 - recall: 0.8205 - auc: 0.8203 - val_loss: 0.4908 - val_tp: 52219.0000 - val_fp: 11781.0000 - val_tn: 52219.0000 - val_fn: 11781.0000 - val_accuracy: 0.8159 - val_precision: 0.8159 - val_recall: 0.8159 - val_auc: 0.8159
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-3840000-4096000
[INFO] processing batch 4096000-4352000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1424 - tp: 190188.0000 - fp: 1812.0000 - tn: 190188.0000 - fn: 1812.0000 - accuracy: 0.9906 - precision: 0.9906 - recall: 0.9906 - auc: 0.9903 - val_loss: 0.1123 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-4096000-4352000
[INFO] processing batch 4352000-4608000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1029 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0944 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-4352000-4608000
[INFO] processing batch 4608000-4864000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.4584 - tp: 161451.0000 - fp: 30549.0000 - tn: 161451.0000 - fn: 30549.0000 - accuracy: 0.8409 - precision: 0.8409 - recall: 0.8409 - auc: 0.8420 - val_loss: 0.1058 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-4608000-4864000
[INFO] processing batch 4864000-5120000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0973 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0896 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-4864000-5120000
[INFO] processing batch 5120000-5376000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0834 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0776 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-5120000-5376000
[INFO] processing batch 5376000-5632000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0728 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0684 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-5376000-5632000
[INFO] processing batch 5632000-5888000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.5546 - tp: 155809.0000 - fp: 36191.0000 - tn: 155809.0000 - fn: 36191.0000 - accuracy: 0.8115 - precision: 0.8115 - recall: 0.8115 - auc: 0.8128 - val_loss: 0.0842 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-5632000-5888000
[INFO] processing batch 5888000-6144000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0786 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0735 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-5888000-6144000
[INFO] processing batch 6144000-6400000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.3123 - tp: 174156.0000 - fp: 17844.0000 - tn: 174156.0000 - fn: 17844.0000 - accuracy: 0.9071 - precision: 0.9071 - recall: 0.9071 - auc: 0.9071 - val_loss: 0.9831 - val_tp: 41108.0000 - val_fp: 22892.0000 - val_tn: 41108.0000 - val_fn: 22892.0000 - val_accuracy: 0.6423 - val_precision: 0.6423 - val_recall: 0.6423 - val_auc: 0.6423
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-6144000-6400000
[INFO] processing batch 6400000-6656000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.7436 - tp: 138766.0000 - fp: 53234.0000 - tn: 138766.0000 - fn: 53234.0000 - accuracy: 0.7227 - precision: 0.7227 - recall: 0.7227 - auc: 0.7243 - val_loss: 0.1072 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-6400000-6656000
[INFO] processing batch 6656000-6912000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0986 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0907 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-6656000-6912000
[INFO] processing batch 6912000-7168000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0843 - tp: 191998.0000 - fp: 2.0000 - tn: 191998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0785 - val_tp: 63999.0000 - val_fp: 1.0000 - val_tn: 63999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-6912000-7168000
[INFO] processing batch 7168000-7424000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.0735 - tp: 191999.0000 - fp: 1.0000 - tn: 191999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.8795 - val_tp: 44344.0000 - val_fp: 19656.0000 - val_tn: 44344.0000 - val_fn: 19656.0000 - val_accuracy: 0.6929 - val_precision: 0.6929 - val_recall: 0.6929 - val_auc: 0.6929
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-7168000-7424000
[INFO] processing batch 7424000-7680000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 1.1671 - tp: 102062.0000 - fp: 89938.0000 - tn: 102062.0000 - fn: 89938.0000 - accuracy: 0.5316 - precision: 0.5316 - recall: 0.5316 - auc: 0.5316 - val_loss: 0.5627 - val_tp: 49859.0000 - val_fp: 14141.0000 - val_tn: 49859.0000 - val_fn: 14141.0000 - val_accuracy: 0.7790 - val_precision: 0.7790 - val_recall: 0.7790 - val_auc: 0.7790
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-7424000-7680000
[INFO] processing batch 7680000-7936000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1216 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1100 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-7680000-7936000
[INFO] processing batch 7936000-8192000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1009 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0927 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-7936000-8192000
[INFO] processing batch 8192000-8448000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.4557 - tp: 161781.0000 - fp: 30219.0000 - tn: 161781.0000 - fn: 30219.0000 - accuracy: 0.8426 - precision: 0.8426 - recall: 0.8426 - auc: 0.8418 - val_loss: 0.1257 - val_tp: 63369.0000 - val_fp: 631.0000 - val_tn: 63369.0000 - val_fn: 631.0000 - val_accuracy: 0.9901 - val_precision: 0.9901 - val_recall: 0.9901 - val_auc: 0.9901
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-8192000-8448000
[INFO] processing batch 8448000-8704000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.8398 - tp: 123282.0000 - fp: 68718.0000 - tn: 123282.0000 - fn: 68718.0000 - accuracy: 0.6421 - precision: 0.6421 - recall: 0.6421 - auc: 0.6416 - val_loss: 0.8294 - val_tp: 39692.0000 - val_fp: 24308.0000 - val_tn: 39692.0000 - val_fn: 24308.0000 - val_accuracy: 0.6202 - val_precision: 0.6202 - val_recall: 0.6202 - val_auc: 0.6202
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-8448000-8704000
[INFO] processing batch 8704000-8960000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1814 - tp: 188028.0000 - fp: 3972.0000 - tn: 188028.0000 - fn: 3972.0000 - accuracy: 0.9793 - precision: 0.9793 - recall: 0.9793 - auc: 0.9791 - val_loss: 0.1294 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/lstm-epoch-001-files-0-50-batch-8704000-8960000
[INFO] processing batch 8960000-9216000/24819975
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.25
[INFO] using LSTM layers
[INFO] fitting model. xtrain.shape: (12000, 16, 107) y_train.shape: (12000, 16, 2)
Train on 12000 samples, validate on 4000 samples
Epoch 1/1
 - 9s - loss: 0.1172 - tp: 192000.0000 - fp: 0.0000e+00 - tn: 192000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1064 - val_tp: 64000.0000 - val_fp: 0.0000e+00 - val_tn: 64000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
