2020-02-03 02:54:01.274916: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-03 02:54:01.285439: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff567299170 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-03 02:54:01.285464: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
=============================
        TRAINING v0.3
=============================
Date: 2020-02-03 02:54:01.269917
[33m[INFO] using Sequential Dense layers[0m
[INFO] adding core layer 0
[INFO] created DNN
[33m[INFO] epoch 1/30[0m
[33m[INFO] loading file 1-1/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0812 - tp: 75807.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 4193.0000 - accuracy: 0.9895 - precision: 1.0000 - recall: 0.9476 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79930.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 70.0000 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0354 - tp: 79705.0000 - fp: 234.0000 - tn: 319766.0000 - fn: 295.0000 - accuracy: 0.9987 - precision: 0.9971 - recall: 0.9963 - auc: 0.9982 - val_loss: 3.3019 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2121 - tp: 73607.0000 - fp: 3947.0000 - tn: 316053.0000 - fn: 6393.0000 - accuracy: 0.9742 - precision: 0.9491 - recall: 0.9201 - auc: 0.9958 - val_loss: 0.0340 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0573 - tp: 79130.0000 - fp: 870.0000 - tn: 319130.0000 - fn: 870.0000 - accuracy: 0.9957 - precision: 0.9891 - recall: 0.9891 - auc: 0.9991 - val_loss: 0.1798 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[33m[INFO] loading file 2-2/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0506 - tp: 79741.0000 - fp: 259.0000 - tn: 319741.0000 - fn: 259.0000 - accuracy: 0.9987 - precision: 0.9968 - recall: 0.9968 - auc: 0.9999 - val_loss: 0.0508 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-1-2
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4584 - tp: 62872.0000 - fp: 17128.0000 - tn: 302872.0000 - fn: 17128.0000 - accuracy: 0.9144 - precision: 0.7859 - recall: 0.7859 - auc: 0.9752 - val_loss: 0.2495 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-1-2
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2737 - tp: 70454.0000 - fp: 9543.0000 - tn: 310457.0000 - fn: 9546.0000 - accuracy: 0.9523 - precision: 0.8807 - recall: 0.8807 - auc: 0.9879 - val_loss: 0.6076 - val_tp: 10593.0000 - val_fp: 9406.0000 - val_tn: 70594.0000 - val_fn: 9407.0000 - val_accuracy: 0.8119 - val_precision: 0.5297 - val_recall: 0.5296 - val_auc: 0.9447
[INFO] saving weights to checkpoints/dnn-epoch-001-files-1-2
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0551 - tp: 79683.0000 - fp: 312.0000 - tn: 319688.0000 - fn: 317.0000 - accuracy: 0.9984 - precision: 0.9961 - recall: 0.9960 - auc: 0.9999 - val_loss: 0.0120 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-1-2
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3586 - tp: 63462.0000 - fp: 16538.0000 - tn: 303462.0000 - fn: 16538.0000 - accuracy: 0.9173 - precision: 0.7933 - recall: 0.7933 - auc: 0.9768 - val_loss: 0.1893 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-1-2
[33m[INFO] loading file 3-3/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1559 - tp: 76764.0000 - fp: 3235.0000 - tn: 316765.0000 - fn: 3236.0000 - accuracy: 0.9838 - precision: 0.9596 - recall: 0.9596 - auc: 0.9953 - val_loss: 0.0621 - val_tp: 19998.0000 - val_fp: 2.0000 - val_tn: 79998.0000 - val_fn: 2.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 0.9999
[INFO] saving weights to checkpoints/dnn-epoch-001-files-2-3
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3369 - tp: 63521.0000 - fp: 16479.0000 - tn: 303521.0000 - fn: 16479.0000 - accuracy: 0.9176 - precision: 0.7940 - recall: 0.7940 - auc: 0.9765 - val_loss: 0.4430 - val_tp: 10612.0000 - val_fp: 9388.0000 - val_tn: 70612.0000 - val_fn: 9388.0000 - val_accuracy: 0.8122 - val_precision: 0.5306 - val_recall: 0.5306 - val_auc: 0.9449
[INFO] saving weights to checkpoints/dnn-epoch-001-files-2-3
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0035 - tp: 79872.0000 - fp: 128.0000 - tn: 319872.0000 - fn: 128.0000 - accuracy: 0.9994 - precision: 0.9984 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-2-3
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0152 - tp: 79926.0000 - fp: 74.0000 - tn: 319926.0000 - fn: 74.0000 - accuracy: 0.9996 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 2.4079e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-2-3
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.0465e-04 - tp: 79919.0000 - fp: 81.0000 - tn: 319919.0000 - fn: 81.0000 - accuracy: 0.9996 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-2-3
[33m[INFO] loading file 4-4/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1576e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-3-4
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9699e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-3-4
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.7646e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-3-4
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3735 - tp: 71524.0000 - fp: 8475.0000 - tn: 311525.0000 - fn: 8476.0000 - accuracy: 0.9576 - precision: 0.8941 - recall: 0.8941 - auc: 0.9726 - val_loss: 1.3946 - val_tp: 7806.0000 - val_fp: 12194.0000 - val_tn: 67806.0000 - val_fn: 12194.0000 - val_accuracy: 0.7561 - val_precision: 0.3903 - val_recall: 0.3903 - val_auc: 0.8557
[INFO] saving weights to checkpoints/dnn-epoch-001-files-3-4
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5145 - tp: 56866.0000 - fp: 23124.0000 - tn: 296876.0000 - fn: 23134.0000 - accuracy: 0.8844 - precision: 0.7109 - recall: 0.7108 - auc: 0.9520 - val_loss: 0.3698 - val_tp: 19999.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-3-4
[33m[INFO] loading file 5-5/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2720 - tp: 67326.0000 - fp: 12664.0000 - tn: 307336.0000 - fn: 12674.0000 - accuracy: 0.9367 - precision: 0.8417 - recall: 0.8416 - auc: 0.9847 - val_loss: 0.1827 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-4-5
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0468 - tp: 79715.0000 - fp: 265.0000 - tn: 319735.0000 - fn: 285.0000 - accuracy: 0.9986 - precision: 0.9967 - recall: 0.9964 - auc: 0.9999 - val_loss: 0.7096 - val_tp: 15795.0000 - val_fp: 4205.0000 - val_tn: 75795.0000 - val_fn: 4205.0000 - val_accuracy: 0.9159 - val_precision: 0.7897 - val_recall: 0.7897 - val_auc: 0.9812
[INFO] saving weights to checkpoints/dnn-epoch-001-files-4-5
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0074 - tp: 79804.0000 - fp: 196.0000 - tn: 319804.0000 - fn: 196.0000 - accuracy: 0.9990 - precision: 0.9976 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-4-5
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0025 - tp: 79914.0000 - fp: 86.0000 - tn: 319914.0000 - fn: 86.0000 - accuracy: 0.9996 - precision: 0.9989 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-4-5
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0017 - tp: 79921.0000 - fp: 79.0000 - tn: 319921.0000 - fn: 79.0000 - accuracy: 0.9996 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-4-5
[33m[INFO] loading file 6-6/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.2503e-04 - tp: 79926.0000 - fp: 74.0000 - tn: 319926.0000 - fn: 74.0000 - accuracy: 0.9996 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-5-6
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1517e-04 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-5-6
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.5122e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-5-6
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.1963e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 44.3374 - val_tp: 15632.0000 - val_fp: 4368.0000 - val_tn: 75632.0000 - val_fn: 4368.0000 - val_accuracy: 0.9126 - val_precision: 0.7816 - val_recall: 0.7816 - val_auc: 0.8635
[INFO] saving weights to checkpoints/dnn-epoch-001-files-5-6
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2960 - tp: 78970.0000 - fp: 1030.0000 - tn: 318970.0000 - fn: 1030.0000 - accuracy: 0.9949 - precision: 0.9871 - recall: 0.9871 - auc: 0.9966 - val_loss: 6.7505e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-5-6
[33m[INFO] loading file 7-7/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0026 - tp: 79845.0000 - fp: 155.0000 - tn: 319845.0000 - fn: 155.0000 - accuracy: 0.9992 - precision: 0.9981 - recall: 0.9981 - auc: 1.0000 - val_loss: 2.0149e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-6-7
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0015 - tp: 79939.0000 - fp: 61.0000 - tn: 319939.0000 - fn: 61.0000 - accuracy: 0.9997 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 1.7039e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-6-7
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5177 - tp: 65677.0000 - fp: 14323.0000 - tn: 305677.0000 - fn: 14323.0000 - accuracy: 0.9284 - precision: 0.8210 - recall: 0.8210 - auc: 0.9796 - val_loss: 0.2725 - val_tp: 12833.0000 - val_fp: 7167.0000 - val_tn: 72833.0000 - val_fn: 7167.0000 - val_accuracy: 0.8567 - val_precision: 0.6417 - val_recall: 0.6417 - val_auc: 0.9679
[INFO] saving weights to checkpoints/dnn-epoch-001-files-6-7
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0049 - tp: 79807.0000 - fp: 193.0000 - tn: 319807.0000 - fn: 193.0000 - accuracy: 0.9990 - precision: 0.9976 - recall: 0.9976 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-6-7
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.4641e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-6-7
[33m[INFO] loading file 8-8/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.0074e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-7-8
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.9319e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-7-8
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1816e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-7-8
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2066 - tp: 73113.0000 - fp: 6887.0000 - tn: 313113.0000 - fn: 6887.0000 - accuracy: 0.9656 - precision: 0.9139 - recall: 0.9139 - auc: 0.9935 - val_loss: 0.1503 - val_tp: 16368.0000 - val_fp: 3632.0000 - val_tn: 76368.0000 - val_fn: 3632.0000 - val_accuracy: 0.9274 - val_precision: 0.8184 - val_recall: 0.8184 - val_auc: 0.9918
[INFO] saving weights to checkpoints/dnn-epoch-001-files-7-8
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0098 - tp: 79884.0000 - fp: 116.0000 - tn: 319884.0000 - fn: 116.0000 - accuracy: 0.9994 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 4.0135e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-7-8
[33m[INFO] loading file 9-9/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0052 - tp: 79953.0000 - fp: 47.0000 - tn: 319953.0000 - fn: 47.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 1.4469e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-8-9
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0813 - tp: 78777.0000 - fp: 1223.0000 - tn: 318777.0000 - fn: 1223.0000 - accuracy: 0.9939 - precision: 0.9847 - recall: 0.9847 - auc: 0.9975 - val_loss: 0.0080 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-8-9
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0017 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1945e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-8-9
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.3121e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-8-9
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.0168e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-8-9
[33m[INFO] loading file 10-10/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0319e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-9-10
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0076 - tp: 79936.0000 - fp: 64.0000 - tn: 319936.0000 - fn: 64.0000 - accuracy: 0.9997 - precision: 0.9992 - recall: 0.9992 - auc: 0.9996 - val_loss: 1.8943 - val_tp: 12843.0000 - val_fp: 7157.0000 - val_tn: 72843.0000 - val_fn: 7157.0000 - val_accuracy: 0.8569 - val_precision: 0.6421 - val_recall: 0.6421 - val_auc: 0.8801
[INFO] saving weights to checkpoints/dnn-epoch-001-files-9-10
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2524 - tp: 70715.0000 - fp: 9285.0000 - tn: 310715.0000 - fn: 9285.0000 - accuracy: 0.9536 - precision: 0.8839 - recall: 0.8839 - auc: 0.9896 - val_loss: 0.5615 - val_tp: 12813.0000 - val_fp: 7187.0000 - val_tn: 72813.0000 - val_fn: 7187.0000 - val_accuracy: 0.8563 - val_precision: 0.6406 - val_recall: 0.6406 - val_auc: 0.9677
[INFO] saving weights to checkpoints/dnn-epoch-001-files-9-10
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0072 - tp: 79619.0000 - fp: 381.0000 - tn: 319619.0000 - fn: 381.0000 - accuracy: 0.9981 - precision: 0.9952 - recall: 0.9952 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-9-10
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.4335e-04 - tp: 79922.0000 - fp: 78.0000 - tn: 319922.0000 - fn: 78.0000 - accuracy: 0.9996 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-9-10
[33m[INFO] loading file 11-11/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.4181e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-10-11
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.2369e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-10-11
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4007e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-10-11
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0288e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-10-11
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.5133e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-10-11
[33m[INFO] loading file 12-12/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1486e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-11-12
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2098e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 25.6685 - val_tp: 16689.0000 - val_fp: 3311.0000 - val_tn: 76689.0000 - val_fn: 3311.0000 - val_accuracy: 0.9338 - val_precision: 0.8345 - val_recall: 0.8345 - val_auc: 0.8965
[INFO] saving weights to checkpoints/dnn-epoch-001-files-11-12
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1779 - tp: 79490.0000 - fp: 510.0000 - tn: 319490.0000 - fn: 510.0000 - accuracy: 0.9975 - precision: 0.9936 - recall: 0.9936 - auc: 0.9985 - val_loss: 235.7576 - val_tp: 17100.0000 - val_fp: 2900.0000 - val_tn: 77100.0000 - val_fn: 2900.0000 - val_accuracy: 0.9420 - val_precision: 0.8550 - val_recall: 0.8550 - val_auc: 0.9094
[INFO] saving weights to checkpoints/dnn-epoch-001-files-11-12
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.6613 - tp: 79854.0000 - fp: 146.0000 - tn: 319854.0000 - fn: 146.0000 - accuracy: 0.9993 - precision: 0.9982 - recall: 0.9982 - auc: 0.9989 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-11-12
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.2151e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-11-12
[33m[INFO] loading file 13-13/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0062e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-12-13
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2343e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-12-13
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8006e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.9117 - val_tp: 15075.0000 - val_fp: 4925.0000 - val_tn: 75075.0000 - val_fn: 4925.0000 - val_accuracy: 0.9015 - val_precision: 0.7538 - val_recall: 0.7538 - val_auc: 0.8461
[INFO] saving weights to checkpoints/dnn-epoch-001-files-12-13
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0437 - tp: 79415.0000 - fp: 585.0000 - tn: 319415.0000 - fn: 585.0000 - accuracy: 0.9971 - precision: 0.9927 - recall: 0.9927 - auc: 0.9991 - val_loss: 1.6113e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-12-13
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0029 - tp: 79883.0000 - fp: 117.0000 - tn: 319883.0000 - fn: 117.0000 - accuracy: 0.9994 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-12-13
[33m[INFO] loading file 14-14/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4037 - tp: 66043.0000 - fp: 13957.0000 - tn: 306043.0000 - fn: 13957.0000 - accuracy: 0.9302 - precision: 0.8255 - recall: 0.8255 - auc: 0.9817 - val_loss: 0.3119 - val_tp: 12810.0000 - val_fp: 7190.0000 - val_tn: 72810.0000 - val_fn: 7190.0000 - val_accuracy: 0.8562 - val_precision: 0.6405 - val_recall: 0.6405 - val_auc: 0.9677
[INFO] saving weights to checkpoints/dnn-epoch-001-files-13-14
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0105 - tp: 79772.0000 - fp: 228.0000 - tn: 319772.0000 - fn: 228.0000 - accuracy: 0.9989 - precision: 0.9972 - recall: 0.9972 - auc: 1.0000 - val_loss: 1.0797e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-13-14
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.5080e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-13-14
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.6928e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-13-14
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.3451e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-13-14
[33m[INFO] loading file 15-15/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0030 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4349e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-14-15
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0015 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.3768e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-14-15
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3834e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-14-15
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.6222e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 24.7379 - val_tp: 11586.0000 - val_fp: 8414.0000 - val_tn: 71586.0000 - val_fn: 8414.0000 - val_accuracy: 0.8317 - val_precision: 0.5793 - val_recall: 0.5793 - val_auc: 0.7371
[INFO] saving weights to checkpoints/dnn-epoch-001-files-14-15
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1721 - tp: 73348.0000 - fp: 6652.0000 - tn: 313348.0000 - fn: 6652.0000 - accuracy: 0.9667 - precision: 0.9168 - recall: 0.9168 - auc: 0.9523 - val_loss: 0.0088 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-14-15
[33m[INFO] loading file 16-16/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (439690, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/439690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0142 - tp: 79876.0000 - fp: 124.0000 - tn: 319876.0000 - fn: 124.0000 - accuracy: 0.9994 - precision: 0.9984 - recall: 0.9984 - auc: 0.9996 - val_loss: 0.0015 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-15-16
[INFO] processing batch 100000-200000/439690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.2208 - tp: 54765.0000 - fp: 25179.0000 - tn: 294821.0000 - fn: 25235.0000 - accuracy: 0.8740 - precision: 0.6850 - recall: 0.6846 - auc: 0.8246 - val_loss: 0.0199 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-15-16
[INFO] processing batch 200000-300000/439690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0088 - tp: 79975.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0039 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-15-16
[INFO] processing batch 300000-400000/439690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0024 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-15-16
[INFO] processing batch 400000-500000/439690
[33m[INFO] loading file 17-17/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_001940_105.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539690, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0498 - tp: 79988.0000 - fp: 12.0000 - tn: 319988.0000 - fn: 12.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 0.9999 - val_loss: 0.0878 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-16-17
[INFO] processing batch 100000-200000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0089 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.5990e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-16-17
[INFO] processing batch 200000-300000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.2764e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.0804e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-16-17
[INFO] processing batch 300000-400000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5186 - tp: 68898.0000 - fp: 10281.0000 - tn: 309719.0000 - fn: 11102.0000 - accuracy: 0.9465 - precision: 0.8702 - recall: 0.8612 - auc: 0.9309 - val_loss: 2.1467 - val_tp: 16369.0000 - val_fp: 3631.0000 - val_tn: 76369.0000 - val_fn: 3631.0000 - val_accuracy: 0.9274 - val_precision: 0.8184 - val_recall: 0.8184 - val_auc: 0.8865
[INFO] saving weights to checkpoints/dnn-epoch-001-files-16-17
[INFO] processing batch 400000-500000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2791 - tp: 77703.0000 - fp: 2254.0000 - tn: 317746.0000 - fn: 2297.0000 - accuracy: 0.9886 - precision: 0.9718 - recall: 0.9713 - auc: 0.9852 - val_loss: 6.5904 - val_tp: 17741.0000 - val_fp: 2259.0000 - val_tn: 77741.0000 - val_fn: 2259.0000 - val_accuracy: 0.9548 - val_precision: 0.8870 - val_recall: 0.8870 - val_auc: 0.9294
[INFO] saving weights to checkpoints/dnn-epoch-001-files-16-17
[INFO] processing batch 500000-600000/539690
[33m[INFO] loading file 18-18/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_053515_106.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539690, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.7416 - tp: 70716.0000 - fp: 9194.0000 - tn: 310806.0000 - fn: 9284.0000 - accuracy: 0.9538 - precision: 0.8849 - recall: 0.8839 - auc: 0.9514 - val_loss: 2.2958 - val_tp: 12369.0000 - val_fp: 7631.0000 - val_tn: 72369.0000 - val_fn: 7631.0000 - val_accuracy: 0.8474 - val_precision: 0.6184 - val_recall: 0.6184 - val_auc: 0.7763
[INFO] saving weights to checkpoints/dnn-epoch-001-files-17-18
[INFO] processing batch 100000-200000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0320 - tp: 79429.0000 - fp: 570.0000 - tn: 319430.0000 - fn: 571.0000 - accuracy: 0.9971 - precision: 0.9929 - recall: 0.9929 - auc: 0.9980 - val_loss: 5.0121e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-17-18
[INFO] processing batch 200000-300000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0074 - tp: 79896.0000 - fp: 103.0000 - tn: 319897.0000 - fn: 104.0000 - accuracy: 0.9995 - precision: 0.9987 - recall: 0.9987 - auc: 0.9994 - val_loss: 1.8850e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-17-18
[INFO] processing batch 300000-400000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3296 - tp: 74264.0000 - fp: 5733.0000 - tn: 314267.0000 - fn: 5736.0000 - accuracy: 0.9713 - precision: 0.9283 - recall: 0.9283 - auc: 0.9907 - val_loss: 0.0607 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-17-18
[INFO] processing batch 400000-500000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0066 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.5842e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-17-18
[INFO] processing batch 500000-600000/539690
[33m[INFO] loading file 19-19/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539690, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.1169e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.2452e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-18-19
[INFO] processing batch 100000-200000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1724e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-18-19
[INFO] processing batch 200000-300000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2726e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-18-19
[INFO] processing batch 300000-400000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.7503 - tp: 69282.0000 - fp: 10710.0000 - tn: 309290.0000 - fn: 10718.0000 - accuracy: 0.9464 - precision: 0.8661 - recall: 0.8660 - auc: 0.9240 - val_loss: 1.7968 - val_tp: 7882.0000 - val_fp: 12118.0000 - val_tn: 67882.0000 - val_fn: 12118.0000 - val_accuracy: 0.7576 - val_precision: 0.3941 - val_recall: 0.3941 - val_auc: 0.8549
[INFO] saving weights to checkpoints/dnn-epoch-001-files-18-19
[INFO] processing batch 400000-500000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2669 - tp: 71304.0000 - fp: 7933.0000 - tn: 312067.0000 - fn: 8696.0000 - accuracy: 0.9584 - precision: 0.8999 - recall: 0.8913 - auc: 0.9907 - val_loss: 11.5398 - val_tp: 7773.0000 - val_fp: 12227.0000 - val_tn: 67773.0000 - val_fn: 12227.0000 - val_accuracy: 0.7555 - val_precision: 0.3887 - val_recall: 0.3887 - val_auc: 0.6179
[INFO] saving weights to checkpoints/dnn-epoch-001-files-18-19
[INFO] processing batch 500000-600000/539690
[33m[INFO] loading file 20-20/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539690, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0159 - tp: 79893.0000 - fp: 107.0000 - tn: 319893.0000 - fn: 107.0000 - accuracy: 0.9995 - precision: 0.9987 - recall: 0.9987 - auc: 0.9994 - val_loss: 0.8905 - val_tp: 19491.0000 - val_fp: 509.0000 - val_tn: 79491.0000 - val_fn: 509.0000 - val_accuracy: 0.9898 - val_precision: 0.9746 - val_recall: 0.9746 - val_auc: 0.9841
[INFO] saving weights to checkpoints/dnn-epoch-001-files-19-20
[INFO] processing batch 100000-200000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0921 - tp: 78521.0000 - fp: 1478.0000 - tn: 318522.0000 - fn: 1479.0000 - accuracy: 0.9926 - precision: 0.9815 - recall: 0.9815 - auc: 0.9982 - val_loss: 0.0078 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-19-20
[INFO] processing batch 200000-300000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0145 - tp: 79782.0000 - fp: 216.0000 - tn: 319784.0000 - fn: 218.0000 - accuracy: 0.9989 - precision: 0.9973 - recall: 0.9973 - auc: 1.0000 - val_loss: 9.5856e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-19-20
[INFO] processing batch 300000-400000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1221 - tp: 75972.0000 - fp: 4027.0000 - tn: 315973.0000 - fn: 4028.0000 - accuracy: 0.9799 - precision: 0.9497 - recall: 0.9496 - auc: 0.9974 - val_loss: 0.0531 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-19-20
[INFO] processing batch 400000-500000/539690
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0017 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-19-20
[INFO] processing batch 500000-600000/539690
[33m[INFO] loading file 21-21/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539688, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539688
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.4265e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-20-21
[INFO] processing batch 100000-200000/539688
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1910e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-20-21
[INFO] processing batch 200000-300000/539688
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.4166e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-20-21
[INFO] processing batch 300000-400000/539688
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.8130e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-20-21
[INFO] processing batch 400000-500000/539688
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.1962e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-20-21
[INFO] processing batch 500000-600000/539688
[33m[INFO] loading file 22-22/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (308729, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/308729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.7375 - tp: 57214.0000 - fp: 22310.0000 - tn: 297690.0000 - fn: 22786.0000 - accuracy: 0.8873 - precision: 0.7195 - recall: 0.7152 - auc: 0.8510 - val_loss: 0.2259 - val_tp: 17847.0000 - val_fp: 2033.0000 - val_tn: 77967.0000 - val_fn: 2153.0000 - val_accuracy: 0.9581 - val_precision: 0.8977 - val_recall: 0.8924 - val_auc: 0.9961
[INFO] saving weights to checkpoints/dnn-epoch-001-files-21-22
[INFO] processing batch 100000-200000/308729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1115 - tp: 78490.0000 - fp: 1504.0000 - tn: 318496.0000 - fn: 1510.0000 - accuracy: 0.9925 - precision: 0.9812 - recall: 0.9811 - auc: 0.9943 - val_loss: 0.0050 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-21-22
[INFO] processing batch 200000-300000/308729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0591 - tp: 79436.0000 - fp: 564.0000 - tn: 319436.0000 - fn: 564.0000 - accuracy: 0.9972 - precision: 0.9930 - recall: 0.9930 - auc: 0.9967 - val_loss: 0.0031 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-21-22
[INFO] processing batch 300000-400000/308729
[33m[INFO] loading file 23-23/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (508729, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.7464 - tp: 63917.0000 - fp: 16068.0000 - tn: 303932.0000 - fn: 16083.0000 - accuracy: 0.9196 - precision: 0.7991 - recall: 0.7990 - auc: 0.9489 - val_loss: 0.0549 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-22-23
[INFO] processing batch 100000-200000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 79987.0000 - fp: 13.0000 - tn: 319987.0000 - fn: 13.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 6.9425e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-22-23
[INFO] processing batch 200000-300000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.6609e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.9987e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-22-23
[INFO] processing batch 300000-400000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.8277e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.2049e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-22-23
[INFO] processing batch 400000-500000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.2246e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.8460e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-22-23
[INFO] processing batch 500000-600000/508729
[33m[INFO] loading file 24-24/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (508729, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9087e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.5064e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-23-24
[INFO] processing batch 100000-200000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.1154e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-23-24
[INFO] processing batch 200000-300000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.3862e-07 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.3446e-11 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-23-24
[INFO] processing batch 300000-400000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.0132e-07 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-23-24
[INFO] processing batch 400000-500000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5053 - tp: 65168.0000 - fp: 14577.0000 - tn: 305423.0000 - fn: 14832.0000 - accuracy: 0.9265 - precision: 0.8172 - recall: 0.8146 - auc: 0.9763 - val_loss: 0.2874 - val_tp: 12899.0000 - val_fp: 7101.0000 - val_tn: 72899.0000 - val_fn: 7101.0000 - val_accuracy: 0.8580 - val_precision: 0.6449 - val_recall: 0.6449 - val_auc: 0.9685
[INFO] saving weights to checkpoints/dnn-epoch-001-files-23-24
[INFO] processing batch 500000-600000/508729
[33m[INFO] loading file 25-25/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (508729, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0152 - tp: 79615.0000 - fp: 383.0000 - tn: 319617.0000 - fn: 385.0000 - accuracy: 0.9981 - precision: 0.9952 - recall: 0.9952 - auc: 0.9999 - val_loss: 5.6910e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-24-25
[INFO] processing batch 100000-200000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0036 - tp: 79929.0000 - fp: 71.0000 - tn: 319929.0000 - fn: 71.0000 - accuracy: 0.9996 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 2.3521e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-24-25
[INFO] processing batch 200000-300000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0022 - tp: 79944.0000 - fp: 56.0000 - tn: 319944.0000 - fn: 56.0000 - accuracy: 0.9997 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 4.6265e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-24-25
[INFO] processing batch 300000-400000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0026 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 9.4843e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-24-25
[INFO] processing batch 400000-500000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0061 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-24-25
[INFO] processing batch 500000-600000/508729
[33m[INFO] loading file 26-26/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (508729, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4404 - tp: 71341.0000 - fp: 8659.0000 - tn: 311341.0000 - fn: 8659.0000 - accuracy: 0.9567 - precision: 0.8918 - recall: 0.8918 - auc: 0.9820 - val_loss: 0.7137 - val_tp: 12898.0000 - val_fp: 7102.0000 - val_tn: 72898.0000 - val_fn: 7102.0000 - val_accuracy: 0.8580 - val_precision: 0.6449 - val_recall: 0.6449 - val_auc: 0.9685
[INFO] saving weights to checkpoints/dnn-epoch-001-files-25-26
[INFO] processing batch 100000-200000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0101 - tp: 79749.0000 - fp: 251.0000 - tn: 319749.0000 - fn: 251.0000 - accuracy: 0.9987 - precision: 0.9969 - recall: 0.9969 - auc: 0.9997 - val_loss: 7.4381e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-25-26
[INFO] processing batch 200000-300000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0025 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 7.0276e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-25-26
[INFO] processing batch 300000-400000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0022 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0011 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-25-26
[INFO] processing batch 400000-500000/508729
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-25-26
[INFO] processing batch 500000-600000/508729
[33m[INFO] loading file 27-27/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (448859, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/448859
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9236e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-26-27
[INFO] processing batch 100000-200000/448859
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0027 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-26-27
[INFO] processing batch 200000-300000/448859
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8406e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-26-27
[INFO] processing batch 300000-400000/448859
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 3.0851e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-26-27
[INFO] processing batch 400000-500000/448859
[33m[INFO] loading file 28-28/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (382775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/382775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.8236e-04 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0027 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-27-28
[INFO] processing batch 100000-200000/382775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6873e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.1921e-11 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-27-28
[INFO] processing batch 200000-300000/382775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2053e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0040 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-27-28
[INFO] processing batch 300000-400000/382775
[33m[INFO] loading file 29-29/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2606e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-28-29
[INFO] processing batch 100000-200000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.8635e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-28-29
[INFO] processing batch 200000-300000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3315e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-28-29
[INFO] processing batch 300000-400000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-28-29
[INFO] processing batch 400000-500000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0019 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0170 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-28-29
[INFO] processing batch 500000-600000/582775
[33m[INFO] loading file 30-30/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7080e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-29-30
[INFO] processing batch 100000-200000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0023 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-29-30
[INFO] processing batch 200000-300000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0222e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0022 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-29-30
[INFO] processing batch 300000-400000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-29-30
[INFO] processing batch 400000-500000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0053 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0029 - val_tp: 19998.0000 - val_fp: 2.0000 - val_tn: 79998.0000 - val_fn: 2.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 0.9999
[INFO] saving weights to checkpoints/dnn-epoch-001-files-29-30
[INFO] processing batch 500000-600000/582775
[33m[INFO] loading file 31-31/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.3683e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-30-31
[INFO] processing batch 100000-200000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5655e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-30-31
[INFO] processing batch 200000-300000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8704e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-30-31
[INFO] processing batch 300000-400000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9495e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-30-31
[INFO] processing batch 400000-500000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-30-31
[INFO] processing batch 500000-600000/582775
[33m[INFO] loading file 32-32/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-31-32
[INFO] processing batch 100000-200000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0019 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0016 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-31-32
[INFO] processing batch 200000-300000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0651e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-31-32
[INFO] processing batch 300000-400000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.5463e-04 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-31-32
[INFO] processing batch 400000-500000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2805e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-31-32
[INFO] processing batch 500000-600000/582775
[33m[INFO] loading file 33-33/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582775, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4183e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 5.9605e-12 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-32-33
[INFO] processing batch 100000-200000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-32-33
[INFO] processing batch 200000-300000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-32-33
[INFO] processing batch 300000-400000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5275e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-32-33
[INFO] processing batch 400000-500000/582775
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0018 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-32-33
[INFO] processing batch 500000-600000/582775
[33m[INFO] loading file 34-34/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (424208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/424208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4306e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-33-34
[INFO] processing batch 100000-200000/424208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9666e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-33-34
[INFO] processing batch 200000-300000/424208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0063 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-33-34
[INFO] processing batch 300000-400000/424208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5312e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-33-34
[INFO] processing batch 400000-500000/424208
[33m[INFO] loading file 35-35/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.4561e-04 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-34-35
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9057e-04 - tp: 79951.0000 - fp: 49.0000 - tn: 319951.0000 - fn: 49.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0021 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-34-35
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9350e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-34-35
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7213e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-34-35
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2738e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-34-35
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 36-36/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7785e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-35-36
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9770e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 7.2278e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-35-36
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-35-36
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.9795e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-35-36
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0023 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-35-36
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 37-37/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3948e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-36-37
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3367e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-36-37
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6751e-04 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-36-37
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.3039e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 6.1357e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-36-37
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.3543e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-36-37
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 38-38/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 8.1822e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-37-38
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7669e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 5.5899e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-37-38
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79959.0000 - fp: 41.0000 - tn: 319959.0000 - fn: 41.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-37-38
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0028 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-37-38
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2688e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-37-38
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 39-39/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-38-39
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3053e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-38-39
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3017e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-38-39
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79958.0000 - fp: 42.0000 - tn: 319958.0000 - fn: 42.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-38-39
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7741e-04 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-38-39
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 40-40/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524208, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.8233e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-39-40
[INFO] processing batch 100000-200000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.4069e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-39-40
[INFO] processing batch 200000-300000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2031e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-39-40
[INFO] processing batch 300000-400000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1061e-04 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-39-40
[INFO] processing batch 400000-500000/524208
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2857e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-39-40
[INFO] processing batch 500000-600000/524208
[33m[INFO] loading file 41-41/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (464366, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/464366
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0078 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-40-41
[INFO] processing batch 100000-200000/464366
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2442e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-40-41
[INFO] processing batch 200000-300000/464366
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7203e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-40-41
[INFO] processing batch 300000-400000/464366
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0013 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-40-41
[INFO] processing batch 400000-500000/464366
[33m[INFO] loading file 42-42/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (407880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/407880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-41-42
[INFO] processing batch 100000-200000/407880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9861e-04 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-41-42
[INFO] processing batch 200000-300000/407880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0871e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-41-42
[INFO] processing batch 300000-400000/407880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0013 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-41-42
[INFO] processing batch 400000-500000/407880
[33m[INFO] loading file 43-43/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.8778e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-42-43
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8740e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 6.4927e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-42-43
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-42-43
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1822e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-42-43
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-42-43
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 44-44/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9452e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-43-44
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2579e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-43-44
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1088e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-43-44
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.6963e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 6.0783e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-43-44
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8059e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0029 - val_tp: 19998.0000 - val_fp: 2.0000 - val_tn: 79998.0000 - val_fn: 2.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 0.9999
[INFO] saving weights to checkpoints/dnn-epoch-001-files-43-44
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 45-45/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2706e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-44-45
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7338e-04 - tp: 79958.0000 - fp: 42.0000 - tn: 319958.0000 - fn: 42.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-44-45
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7441e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-44-45
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9901e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-44-45
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.9644e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-44-45
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 46-46/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0013 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-45-46
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.0171e-04 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-45-46
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2843e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0019 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-45-46
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0055 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.1921e-11 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-45-46
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4115e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-45-46
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 47-47/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8515e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-46-47
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 1.5497e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-46-47
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3846e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 5.4777e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-46-47
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8198e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 6.1989e-09 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-46-47
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.6430e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0101 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-46-47
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 48-48/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3266e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-47-48
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1504e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-47-48
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9023e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-47-48
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5884e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-47-48
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.0092e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 6.7500e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-47-48
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 49-49/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (507880, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5483e-04 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-48-49
[INFO] processing batch 100000-200000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.3153e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-48-49
[INFO] processing batch 200000-300000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1143e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-48-49
[INFO] processing batch 300000-400000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-48-49
[INFO] processing batch 400000-500000/507880
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0728e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-48-49
[INFO] processing batch 500000-600000/507880
[33m[INFO] loading file 50-50/50 on epoch 1/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (357862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/357862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7815e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-49-50
[INFO] processing batch 100000-200000/357862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7140e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 6.1672e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-49-50
[INFO] processing batch 200000-300000/357862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6777e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-49-50
[INFO] processing batch 300000-400000/357862
[33m[LOSS] 0.0[0m
[33m[INFO] epoch 2/30[0m
[33m[INFO] loading file 1-1/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2863 - tp: 71029.0000 - fp: 8971.0000 - tn: 311029.0000 - fn: 8971.0000 - accuracy: 0.9551 - precision: 0.8879 - recall: 0.8879 - auc: 0.9890 - val_loss: 0.4620 - val_tp: 12837.0000 - val_fp: 7163.0000 - val_tn: 72837.0000 - val_fn: 7163.0000 - val_accuracy: 0.8567 - val_precision: 0.6418 - val_recall: 0.6418 - val_auc: 0.9679
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0022 - tp: 79907.0000 - fp: 93.0000 - tn: 319907.0000 - fn: 93.0000 - accuracy: 0.9995 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 2.4120e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.6564e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7206e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4294 - tp: 60711.0000 - fp: 19289.0000 - tn: 300711.0000 - fn: 19289.0000 - accuracy: 0.9036 - precision: 0.7589 - recall: 0.7589 - auc: 0.9685 - val_loss: 0.3585 - val_tp: 13368.0000 - val_fp: 6632.0000 - val_tn: 73368.0000 - val_fn: 6632.0000 - val_accuracy: 0.8674 - val_precision: 0.6684 - val_recall: 0.6684 - val_auc: 0.9608
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0279 - tp: 79792.0000 - fp: 208.0000 - tn: 319792.0000 - fn: 208.0000 - accuracy: 0.9990 - precision: 0.9974 - recall: 0.9974 - auc: 0.9999 - val_loss: 0.0033 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 2-2/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0085 - tp: 79906.0000 - fp: 94.0000 - tn: 319906.0000 - fn: 94.0000 - accuracy: 0.9995 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0015 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-1-2
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0024 - tp: 79951.0000 - fp: 49.0000 - tn: 319951.0000 - fn: 49.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 22.1742 - val_tp: 12609.0000 - val_fp: 7391.0000 - val_tn: 72609.0000 - val_fn: 7391.0000 - val_accuracy: 0.8522 - val_precision: 0.6305 - val_recall: 0.6305 - val_auc: 0.7690
[INFO] saving weights to checkpoints/dnn-epoch-002-files-1-2
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0200 - tp: 79952.0000 - fp: 48.0000 - tn: 319952.0000 - fn: 48.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 0.9997 - val_loss: 3.9851e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-1-2
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3583 - tp: 66238.0000 - fp: 13762.0000 - tn: 306238.0000 - fn: 13762.0000 - accuracy: 0.9312 - precision: 0.8280 - recall: 0.8280 - auc: 0.9783 - val_loss: 0.1599 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-1-2
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0049 - tp: 79896.0000 - fp: 104.0000 - tn: 319896.0000 - fn: 104.0000 - accuracy: 0.9995 - precision: 0.9987 - recall: 0.9987 - auc: 1.0000 - val_loss: 11.8486 - val_tp: 12847.0000 - val_fp: 7153.0000 - val_tn: 72847.0000 - val_fn: 7153.0000 - val_accuracy: 0.8569 - val_precision: 0.6424 - val_recall: 0.6424 - val_auc: 0.7765
[INFO] saving weights to checkpoints/dnn-epoch-002-files-1-2
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 3-3/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2683 - tp: 72399.0000 - fp: 7601.0000 - tn: 312399.0000 - fn: 7601.0000 - accuracy: 0.9620 - precision: 0.9050 - recall: 0.9050 - auc: 0.9875 - val_loss: 0.7941 - val_tp: 10584.0000 - val_fp: 9416.0000 - val_tn: 70584.0000 - val_fn: 9416.0000 - val_accuracy: 0.8117 - val_precision: 0.5292 - val_recall: 0.5292 - val_auc: 0.9446
[INFO] saving weights to checkpoints/dnn-epoch-002-files-2-3
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0076 - tp: 79693.0000 - fp: 307.0000 - tn: 319693.0000 - fn: 307.0000 - accuracy: 0.9985 - precision: 0.9962 - recall: 0.9962 - auc: 1.0000 - val_loss: 1.4396e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-2-3
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1298 - tp: 77568.0000 - fp: 2432.0000 - tn: 317568.0000 - fn: 2432.0000 - accuracy: 0.9878 - precision: 0.9696 - recall: 0.9696 - auc: 0.9957 - val_loss: 0.0269 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-2-3
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0016 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.9346e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-2-3
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.6287e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.4173e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-2-3
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 4-4/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8490e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6988e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-3-4
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.8593e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.8257e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-3-4
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.2472e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.0309e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-3-4
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.5254e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.2190e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-3-4
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.6832 - tp: 58640.0000 - fp: 21360.0000 - tn: 298640.0000 - fn: 21360.0000 - accuracy: 0.8932 - precision: 0.7330 - recall: 0.7330 - auc: 0.9458 - val_loss: 0.3793 - val_tp: 17454.0000 - val_fp: 2546.0000 - val_tn: 77454.0000 - val_fn: 2546.0000 - val_accuracy: 0.9491 - val_precision: 0.8727 - val_recall: 0.8727 - val_auc: 0.9811
[INFO] saving weights to checkpoints/dnn-epoch-002-files-3-4
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 5-5/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0093 - tp: 79955.0000 - fp: 45.0000 - tn: 319955.0000 - fn: 45.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 1.8410 - val_tp: 16070.0000 - val_fp: 3930.0000 - val_tn: 76070.0000 - val_fn: 3930.0000 - val_accuracy: 0.9214 - val_precision: 0.8035 - val_recall: 0.8035 - val_auc: 0.8772
[INFO] saving weights to checkpoints/dnn-epoch-002-files-4-5
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0373 - tp: 79313.0000 - fp: 687.0000 - tn: 319313.0000 - fn: 687.0000 - accuracy: 0.9966 - precision: 0.9914 - recall: 0.9914 - auc: 0.9985 - val_loss: 1.7953e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-4-5
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2670 - tp: 67141.0000 - fp: 12859.0000 - tn: 307141.0000 - fn: 12859.0000 - accuracy: 0.9357 - precision: 0.8393 - recall: 0.8393 - auc: 0.9853 - val_loss: 0.2981 - val_tp: 12869.0000 - val_fp: 7131.0000 - val_tn: 72869.0000 - val_fn: 7131.0000 - val_accuracy: 0.8574 - val_precision: 0.6435 - val_recall: 0.6435 - val_auc: 0.9682
[INFO] saving weights to checkpoints/dnn-epoch-002-files-4-5
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0022 - tp: 79902.0000 - fp: 98.0000 - tn: 319902.0000 - fn: 98.0000 - accuracy: 0.9995 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 1.9987e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-4-5
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.6521e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.1634e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-4-5
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 6-6/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.5394e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6346e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-5-6
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.2017e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.4171e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-5-6
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1939e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6112e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-5-6
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.1902e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.4590e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-5-6
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2811 - tp: 66979.0000 - fp: 13021.0000 - tn: 306979.0000 - fn: 13021.0000 - accuracy: 0.9349 - precision: 0.8372 - recall: 0.8372 - auc: 0.9846 - val_loss: 0.3196 - val_tp: 12788.0000 - val_fp: 7212.0000 - val_tn: 72788.0000 - val_fn: 7212.0000 - val_accuracy: 0.8558 - val_precision: 0.6394 - val_recall: 0.6394 - val_auc: 0.9675
[INFO] saving weights to checkpoints/dnn-epoch-002-files-5-6
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 7-7/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0033 - tp: 79852.0000 - fp: 148.0000 - tn: 319852.0000 - fn: 148.0000 - accuracy: 0.9993 - precision: 0.9981 - recall: 0.9981 - auc: 1.0000 - val_loss: 4.7563e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-6-7
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0012 - tp: 79943.0000 - fp: 57.0000 - tn: 319943.0000 - fn: 57.0000 - accuracy: 0.9997 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 1.0734e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-6-7
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0013 - tp: 79956.0000 - fp: 44.0000 - tn: 319956.0000 - fn: 44.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.5851 - val_tp: 19583.0000 - val_fp: 417.0000 - val_tn: 79583.0000 - val_fn: 417.0000 - val_accuracy: 0.9917 - val_precision: 0.9791 - val_recall: 0.9791 - val_auc: 0.9870
[INFO] saving weights to checkpoints/dnn-epoch-002-files-6-7
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0304 - tp: 79836.0000 - fp: 164.0000 - tn: 319836.0000 - fn: 164.0000 - accuracy: 0.9992 - precision: 0.9980 - recall: 0.9980 - auc: 0.9991 - val_loss: 1.6896e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-6-7
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 1.9966e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4905e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-6-7
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 8-8/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 3.8685e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6499e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-7-8
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 1.9845e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.1856e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-7-8
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 9.6506e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4903e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-7-8
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 4.2951e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-7-8
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0408 - tp: 79377.0000 - fp: 623.0000 - tn: 319377.0000 - fn: 623.0000 - accuracy: 0.9969 - precision: 0.9922 - recall: 0.9922 - auc: 0.9982 - val_loss: 3.2627e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-7-8
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 9-9/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0017 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 6.8884e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-8-9
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0982 - tp: 76893.0000 - fp: 3107.0000 - tn: 316893.0000 - fn: 3107.0000 - accuracy: 0.9845 - precision: 0.9612 - recall: 0.9612 - auc: 0.9982 - val_loss: 0.3405 - val_tp: 16368.0000 - val_fp: 3632.0000 - val_tn: 76368.0000 - val_fn: 3632.0000 - val_accuracy: 0.9274 - val_precision: 0.8184 - val_recall: 0.8184 - val_auc: 0.9918
[INFO] saving weights to checkpoints/dnn-epoch-002-files-8-9
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0017 - tp: 79938.0000 - fp: 62.0000 - tn: 319938.0000 - fn: 62.0000 - accuracy: 0.9997 - precision: 0.9992 - recall: 0.9992 - auc: 1.0000 - val_loss: 1.5096e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-8-9
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.8378e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0731e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-8-9
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9373e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.9863e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-8-9
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 10-10/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.8613e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5202e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-9-10
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.8526e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0920e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-9-10
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2933 - tp: 65669.0000 - fp: 14331.0000 - tn: 305669.0000 - fn: 14331.0000 - accuracy: 0.9283 - precision: 0.8209 - recall: 0.8209 - auc: 0.9823 - val_loss: 0.2434 - val_tp: 16356.0000 - val_fp: 3644.0000 - val_tn: 76356.0000 - val_fn: 3644.0000 - val_accuracy: 0.9271 - val_precision: 0.8178 - val_recall: 0.8178 - val_auc: 0.9917
[INFO] saving weights to checkpoints/dnn-epoch-002-files-9-10
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2157 - tp: 71778.0000 - fp: 8222.0000 - tn: 311778.0000 - fn: 8222.0000 - accuracy: 0.9589 - precision: 0.8972 - recall: 0.8972 - auc: 0.9909 - val_loss: 0.1449 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-9-10
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0705e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-9-10
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 11-11/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1074e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.2387e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-10-11
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8282e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.3793e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-10-11
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.2544e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.5975e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-10-11
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.9528e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2523e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-10-11
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.4519e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0747e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-10-11
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 12-12/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0898e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-11-12
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.5316e-07 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-11-12
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4240 - tp: 67152.0000 - fp: 12848.0000 - tn: 307152.0000 - fn: 12848.0000 - accuracy: 0.9358 - precision: 0.8394 - recall: 0.8394 - auc: 0.9806 - val_loss: 0.4931 - val_tp: 12822.0000 - val_fp: 7178.0000 - val_tn: 72822.0000 - val_fn: 7178.0000 - val_accuracy: 0.8564 - val_precision: 0.6411 - val_recall: 0.6411 - val_auc: 0.9678
[INFO] saving weights to checkpoints/dnn-epoch-002-files-11-12
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3520 - tp: 65914.0000 - fp: 14086.0000 - tn: 305914.0000 - fn: 14086.0000 - accuracy: 0.9296 - precision: 0.8239 - recall: 0.8239 - auc: 0.9841 - val_loss: 0.2457 - val_tp: 17666.0000 - val_fp: 2334.0000 - val_tn: 77666.0000 - val_fn: 2334.0000 - val_accuracy: 0.9533 - val_precision: 0.8833 - val_recall: 0.8833 - val_auc: 0.9895
[INFO] saving weights to checkpoints/dnn-epoch-002-files-11-12
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0044 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 3.4720e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-11-12
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 13-13/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.6036e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0379e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-12-13
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4355e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.1549e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-12-13
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2990e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-12-13
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3242 - tp: 66618.0000 - fp: 13382.0000 - tn: 306618.0000 - fn: 13382.0000 - accuracy: 0.9331 - precision: 0.8327 - recall: 0.8327 - auc: 0.9832 - val_loss: 0.3220 - val_tp: 12833.0000 - val_fp: 7167.0000 - val_tn: 72833.0000 - val_fn: 7167.0000 - val_accuracy: 0.8567 - val_precision: 0.6417 - val_recall: 0.6417 - val_auc: 0.9679
[INFO] saving weights to checkpoints/dnn-epoch-002-files-12-13
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0035 - tp: 79906.0000 - fp: 94.0000 - tn: 319906.0000 - fn: 94.0000 - accuracy: 0.9995 - precision: 0.9988 - recall: 0.9988 - auc: 1.0000 - val_loss: 9.8979e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-12-13
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 14-14/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79949.0000 - fp: 51.0000 - tn: 319949.0000 - fn: 51.0000 - accuracy: 0.9997 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 3.4015e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-13-14
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1562 - tp: 77620.0000 - fp: 2380.0000 - tn: 317620.0000 - fn: 2380.0000 - accuracy: 0.9881 - precision: 0.9703 - recall: 0.9703 - auc: 0.9969 - val_loss: 0.0231 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-13-14
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0071 - tp: 79989.0000 - fp: 11.0000 - tn: 319989.0000 - fn: 11.0000 - accuracy: 0.9999 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.0015 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-13-14
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.9188e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-13-14
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.1766e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.3299e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-13-14
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 15-15/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (557862, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.8034e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.9084e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-14-15
[INFO] processing batch 100000-200000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.4888e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0120e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-14-15
[INFO] processing batch 200000-300000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.9588e-04 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6403e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-14-15
[INFO] processing batch 300000-400000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0599 - tp: 79999.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.9857e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-14-15
[INFO] processing batch 400000-500000/557862
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5560 - tp: 57249.0000 - fp: 22686.0000 - tn: 297314.0000 - fn: 22751.0000 - accuracy: 0.8864 - precision: 0.7162 - recall: 0.7156 - auc: 0.9069 - val_loss: 0.9557 - val_tp: 10669.0000 - val_fp: 9331.0000 - val_tn: 70669.0000 - val_fn: 9331.0000 - val_accuracy: 0.8134 - val_precision: 0.5335 - val_recall: 0.5335 - val_auc: 0.9456
[INFO] saving weights to checkpoints/dnn-epoch-002-files-14-15
[INFO] processing batch 500000-600000/557862
[33m[INFO] loading file 16-16/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (497552, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/497552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0632 - tp: 78595.0000 - fp: 1404.0000 - tn: 318596.0000 - fn: 1405.0000 - accuracy: 0.9930 - precision: 0.9824 - recall: 0.9824 - auc: 0.9980 - val_loss: 4.3525e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-15-16
[INFO] processing batch 100000-200000/497552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0061 - tp: 79950.0000 - fp: 50.0000 - tn: 319950.0000 - fn: 50.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 0.9997 - val_loss: 8.3361e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-15-16
[INFO] processing batch 200000-300000/497552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2630 - tp: 77960.0000 - fp: 2040.0000 - tn: 317960.0000 - fn: 2040.0000 - accuracy: 0.9898 - precision: 0.9745 - recall: 0.9745 - auc: 0.9893 - val_loss: 0.0072 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-15-16
[INFO] processing batch 300000-400000/497552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1689e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5700e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-15-16
[INFO] processing batch 400000-500000/497552
[33m[INFO] loading file 17-17/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_001940_105.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (597552, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.6041e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0801e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-16-17
[INFO] processing batch 100000-200000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0015 - tp: 79993.0000 - fp: 7.0000 - tn: 319993.0000 - fn: 7.0000 - accuracy: 1.0000 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 8.5456e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-16-17
[INFO] processing batch 200000-300000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5149e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.7756e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-16-17
[INFO] processing batch 300000-400000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0655 - tp: 79034.0000 - fp: 964.0000 - tn: 319036.0000 - fn: 966.0000 - accuracy: 0.9952 - precision: 0.9879 - recall: 0.9879 - auc: 0.9965 - val_loss: 0.7109 - val_tp: 16305.0000 - val_fp: 3695.0000 - val_tn: 76305.0000 - val_fn: 3695.0000 - val_accuracy: 0.9261 - val_precision: 0.8152 - val_recall: 0.8152 - val_auc: 0.9915
[INFO] saving weights to checkpoints/dnn-epoch-002-files-16-17
[INFO] processing batch 400000-500000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0055 - tp: 79867.0000 - fp: 133.0000 - tn: 319867.0000 - fn: 133.0000 - accuracy: 0.9993 - precision: 0.9983 - recall: 0.9983 - auc: 0.9998 - val_loss: 1.4252e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-16-17
[INFO] processing batch 500000-600000/597552
[33m[INFO] loading file 18-18/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_053515_106.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (597552, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1521 - tp: 72959.0000 - fp: 7037.0000 - tn: 312963.0000 - fn: 7041.0000 - accuracy: 0.9648 - precision: 0.9120 - recall: 0.9120 - auc: 0.9957 - val_loss: 0.1082 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-17-18
[INFO] processing batch 100000-200000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3749 - tp: 69854.0000 - fp: 10144.0000 - tn: 309856.0000 - fn: 10146.0000 - accuracy: 0.9493 - precision: 0.8732 - recall: 0.8732 - auc: 0.9836 - val_loss: 0.1137 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-17-18
[INFO] processing batch 200000-300000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0167 - tp: 79443.0000 - fp: 557.0000 - tn: 319443.0000 - fn: 557.0000 - accuracy: 0.9972 - precision: 0.9930 - recall: 0.9930 - auc: 0.9999 - val_loss: 3.8405e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-17-18
[INFO] processing batch 300000-400000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1566 - tp: 76379.0000 - fp: 3621.0000 - tn: 316379.0000 - fn: 3621.0000 - accuracy: 0.9819 - precision: 0.9547 - recall: 0.9547 - auc: 0.9951 - val_loss: 1.2208 - val_tp: 12403.0000 - val_fp: 7597.0000 - val_tn: 72403.0000 - val_fn: 7597.0000 - val_accuracy: 0.8481 - val_precision: 0.6202 - val_recall: 0.6202 - val_auc: 0.9639
[INFO] saving weights to checkpoints/dnn-epoch-002-files-17-18
[INFO] processing batch 400000-500000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0083 - tp: 79765.0000 - fp: 235.0000 - tn: 319765.0000 - fn: 235.0000 - accuracy: 0.9988 - precision: 0.9971 - recall: 0.9971 - auc: 0.9999 - val_loss: 9.2110e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-17-18
[INFO] processing batch 500000-600000/597552
[33m[INFO] loading file 19-19/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (597552, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.0455e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3480e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-18-19
[INFO] processing batch 100000-200000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.8450e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.2771e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-18-19
[INFO] processing batch 200000-300000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0137 - tp: 79995.0000 - fp: 5.0000 - tn: 319995.0000 - fn: 5.0000 - accuracy: 1.0000 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 2.4593e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-18-19
[INFO] processing batch 300000-400000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.9903e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-18-19
[INFO] processing batch 400000-500000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.6849 - tp: 61800.0000 - fp: 18069.0000 - tn: 301931.0000 - fn: 18200.0000 - accuracy: 0.9093 - precision: 0.7738 - recall: 0.7725 - auc: 0.9538 - val_loss: 0.2030 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-18-19
[INFO] processing batch 500000-600000/597552
[33m[INFO] loading file 20-20/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (597552, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4316 - tp: 55979.0000 - fp: 23861.0000 - tn: 296139.0000 - fn: 24021.0000 - accuracy: 0.8803 - precision: 0.7011 - recall: 0.6997 - auc: 0.9546 - val_loss: 0.3429 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-19-20
[INFO] processing batch 100000-200000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1336 - tp: 75349.0000 - fp: 4649.0000 - tn: 315351.0000 - fn: 4651.0000 - accuracy: 0.9768 - precision: 0.9419 - recall: 0.9419 - auc: 0.9969 - val_loss: 0.2415 - val_tp: 16317.0000 - val_fp: 3683.0000 - val_tn: 76317.0000 - val_fn: 3683.0000 - val_accuracy: 0.9263 - val_precision: 0.8159 - val_recall: 0.8159 - val_auc: 0.9915
[INFO] saving weights to checkpoints/dnn-epoch-002-files-19-20
[INFO] processing batch 200000-300000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0041 - tp: 79876.0000 - fp: 124.0000 - tn: 319876.0000 - fn: 124.0000 - accuracy: 0.9994 - precision: 0.9984 - recall: 0.9984 - auc: 1.0000 - val_loss: 8.5261e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-19-20
[INFO] processing batch 300000-400000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0204 - tp: 79774.0000 - fp: 226.0000 - tn: 319774.0000 - fn: 226.0000 - accuracy: 0.9989 - precision: 0.9972 - recall: 0.9972 - auc: 0.9995 - val_loss: 2.4599 - val_tp: 16368.0000 - val_fp: 3632.0000 - val_tn: 76368.0000 - val_fn: 3632.0000 - val_accuracy: 0.9274 - val_precision: 0.8184 - val_recall: 0.8184 - val_auc: 0.8865
[INFO] saving weights to checkpoints/dnn-epoch-002-files-19-20
[INFO] processing batch 400000-500000/597552
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0021 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9999 - val_loss: 1.6415e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-19-20
[INFO] processing batch 500000-600000/597552
[33m[INFO] loading file 21-21/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (597550, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/597550
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2934e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5044e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-20-21
[INFO] processing batch 100000-200000/597550
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.7481e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6174e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-20-21
[INFO] processing batch 200000-300000/597550
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.0015e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.7604e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-20-21
[INFO] processing batch 300000-400000/597550
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5317e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-20-21
[INFO] processing batch 400000-500000/597550
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0067 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-20-21
[INFO] processing batch 500000-600000/597550
[33m[INFO] loading file 22-22/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (366591, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/366591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.8169e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.2515 - val_tp: 12756.0000 - val_fp: 7244.0000 - val_tn: 72756.0000 - val_fn: 7244.0000 - val_accuracy: 0.8551 - val_precision: 0.6378 - val_recall: 0.6378 - val_auc: 0.7736
[INFO] saving weights to checkpoints/dnn-epoch-002-files-21-22
[INFO] processing batch 100000-200000/366591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4207 - tp: 59395.0000 - fp: 20605.0000 - tn: 299395.0000 - fn: 20605.0000 - accuracy: 0.8970 - precision: 0.7424 - recall: 0.7424 - auc: 0.9127 - val_loss: 6.9065e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-21-22
[INFO] processing batch 200000-300000/366591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0786 - tp: 78816.0000 - fp: 1184.0000 - tn: 318816.0000 - fn: 1184.0000 - accuracy: 0.9941 - precision: 0.9852 - recall: 0.9852 - auc: 0.9967 - val_loss: 0.0039 - val_tp: 19997.0000 - val_fp: 3.0000 - val_tn: 79997.0000 - val_fn: 3.0000 - val_accuracy: 0.9999 - val_precision: 0.9998 - val_recall: 0.9998 - val_auc: 0.9999
[INFO] saving weights to checkpoints/dnn-epoch-002-files-21-22
[INFO] processing batch 300000-400000/366591
[33m[INFO] loading file 23-23/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (566591, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0279 - tp: 79689.0000 - fp: 311.0000 - tn: 319689.0000 - fn: 311.0000 - accuracy: 0.9984 - precision: 0.9961 - recall: 0.9961 - auc: 0.9994 - val_loss: 24.1195 - val_tp: 13346.0000 - val_fp: 6654.0000 - val_tn: 73346.0000 - val_fn: 6654.0000 - val_accuracy: 0.8669 - val_precision: 0.6673 - val_recall: 0.6673 - val_auc: 0.7921
[INFO] saving weights to checkpoints/dnn-epoch-002-files-22-23
[INFO] processing batch 100000-200000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0911 - tp: 79792.0000 - fp: 208.0000 - tn: 319792.0000 - fn: 208.0000 - accuracy: 0.9990 - precision: 0.9974 - recall: 0.9974 - auc: 0.9986 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-22-23
[INFO] processing batch 200000-300000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9838e-11 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-22-23
[INFO] processing batch 300000-400000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1325e-10 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-22-23
[INFO] processing batch 400000-500000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.4731e-04 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-22-23
[INFO] processing batch 500000-600000/566591
[33m[INFO] loading file 24-24/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (566591, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.6037e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-23-24
[INFO] processing batch 100000-200000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.2219e-10 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-23-24
[INFO] processing batch 200000-300000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.3560e-10 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-23-24
[INFO] processing batch 300000-400000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0016 - tp: 79990.0000 - fp: 10.0000 - tn: 319990.0000 - fn: 10.0000 - accuracy: 0.9999 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-23-24
[INFO] processing batch 400000-500000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.8626e-10 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1551.9289 - val_tp: 18173.0000 - val_fp: 1827.0000 - val_tn: 78173.0000 - val_fn: 1827.0000 - val_accuracy: 0.9635 - val_precision: 0.9086 - val_recall: 0.9086 - val_auc: 0.9429
[INFO] saving weights to checkpoints/dnn-epoch-002-files-23-24
[INFO] processing batch 500000-600000/566591
[33m[INFO] loading file 25-25/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (566591, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 24.7810 - tp: 50875.0000 - fp: 29125.0000 - tn: 290875.0000 - fn: 29125.0000 - accuracy: 0.8544 - precision: 0.6359 - recall: 0.6359 - auc: 0.8758 - val_loss: 0.1920 - val_tp: 18710.0000 - val_fp: 1290.0000 - val_tn: 78710.0000 - val_fn: 1290.0000 - val_accuracy: 0.9742 - val_precision: 0.9355 - val_recall: 0.9355 - val_auc: 0.9977
[INFO] saving weights to checkpoints/dnn-epoch-002-files-24-25
[INFO] processing batch 100000-200000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3572 - tp: 70979.0000 - fp: 9021.0000 - tn: 310979.0000 - fn: 9021.0000 - accuracy: 0.9549 - precision: 0.8872 - recall: 0.8872 - auc: 0.9855 - val_loss: 0.0086 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-24-25
[INFO] processing batch 200000-300000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0650 - tp: 78422.0000 - fp: 1578.0000 - tn: 318422.0000 - fn: 1578.0000 - accuracy: 0.9921 - precision: 0.9803 - recall: 0.9803 - auc: 0.9986 - val_loss: 0.0017 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-24-25
[INFO] processing batch 300000-400000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0173 - tp: 79635.0000 - fp: 365.0000 - tn: 319635.0000 - fn: 365.0000 - accuracy: 0.9982 - precision: 0.9954 - recall: 0.9954 - auc: 0.9998 - val_loss: 0.0011 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-24-25
[INFO] processing batch 400000-500000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0083 - tp: 79833.0000 - fp: 167.0000 - tn: 319833.0000 - fn: 167.0000 - accuracy: 0.9992 - precision: 0.9979 - recall: 0.9979 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-24-25
[INFO] processing batch 500000-600000/566591
[33m[INFO] loading file 26-26/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (566591, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0043 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-25-26
[INFO] processing batch 100000-200000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.9115 - tp: 67252.0000 - fp: 12744.0000 - tn: 307256.0000 - fn: 12748.0000 - accuracy: 0.9363 - precision: 0.8407 - recall: 0.8407 - auc: 0.9645 - val_loss: 0.1143 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-25-26
[INFO] processing batch 200000-300000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0209 - tp: 79544.0000 - fp: 352.0000 - tn: 319648.0000 - fn: 456.0000 - accuracy: 0.9980 - precision: 0.9956 - recall: 0.9943 - auc: 0.9997 - val_loss: 3.1662e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-25-26
[INFO] processing batch 300000-400000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0863 - tp: 79742.0000 - fp: 116.0000 - tn: 319884.0000 - fn: 258.0000 - accuracy: 0.9991 - precision: 0.9985 - recall: 0.9968 - auc: 0.9997 - val_loss: 0.0010 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-25-26
[INFO] processing batch 400000-500000/566591
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0334 - tp: 79893.0000 - fp: 84.0000 - tn: 319916.0000 - fn: 107.0000 - accuracy: 0.9995 - precision: 0.9989 - recall: 0.9987 - auc: 0.9998 - val_loss: 4.5689e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-25-26
[INFO] processing batch 500000-600000/566591
[33m[INFO] loading file 27-27/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (506721, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/506721
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0071 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-26-27
[INFO] processing batch 100000-200000/506721
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0056 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0022 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-26-27
[INFO] processing batch 200000-300000/506721
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0044 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-26-27
[INFO] processing batch 300000-400000/506721
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0733 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-26-27
[INFO] processing batch 400000-500000/506721
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0036 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-26-27
[INFO] processing batch 500000-600000/506721
[33m[INFO] loading file 28-28/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (340637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/340637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0181 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-27-28
[INFO] processing batch 100000-200000/340637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0041 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-27-28
[INFO] processing batch 200000-300000/340637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0033 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-27-28
[INFO] processing batch 300000-400000/340637
[33m[INFO] loading file 29-29/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (540637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0020 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-28-29
[INFO] processing batch 100000-200000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0055 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-28-29
[INFO] processing batch 200000-300000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0050 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-28-29
[INFO] processing batch 300000-400000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0018 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 9.4474e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-28-29
[INFO] processing batch 400000-500000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0028 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-28-29
[INFO] processing batch 500000-600000/540637
[33m[INFO] loading file 30-30/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (540637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0013 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0439 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-29-30
[INFO] processing batch 100000-200000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 8.0296e-04 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-29-30
[INFO] processing batch 200000-300000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0011 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-29-30
[INFO] processing batch 300000-400000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0208 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0028 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-29-30
[INFO] processing batch 400000-500000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0034 - tp: 79958.0000 - fp: 42.0000 - tn: 319958.0000 - fn: 42.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 5.9363e-09 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-29-30
[INFO] processing batch 500000-600000/540637
[33m[INFO] loading file 31-31/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (540637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.6973e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 4.0531e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-30-31
[INFO] processing batch 100000-200000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.6826e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-30-31
[INFO] processing batch 200000-300000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.1186e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-30-31
[INFO] processing batch 300000-400000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0163 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-30-31
[INFO] processing batch 400000-500000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5267e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-30-31
[INFO] processing batch 500000-600000/540637
[33m[INFO] loading file 32-32/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (540637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0075 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 4.0238e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-31-32
[INFO] processing batch 100000-200000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.5025e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-31-32
[INFO] processing batch 200000-300000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5448e-04 - tp: 79983.0000 - fp: 17.0000 - tn: 319983.0000 - fn: 17.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-31-32
[INFO] processing batch 300000-400000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9317e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-31-32
[INFO] processing batch 400000-500000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0555e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-31-32
[INFO] processing batch 500000-600000/540637
[33m[INFO] loading file 33-33/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (540637, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1490e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-32-33
[INFO] processing batch 100000-200000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0123 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 3.5314e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-32-33
[INFO] processing batch 200000-300000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0098 - tp: 79961.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 0.9999 - val_loss: 4.6895e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-32-33
[INFO] processing batch 300000-400000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.5110e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 2.4452e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-32-33
[INFO] processing batch 400000-500000/540637
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.6764e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 3.5230e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-32-33
[INFO] processing batch 500000-600000/540637
[33m[INFO] loading file 34-34/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (382070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/382070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1362e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.6093e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-33-34
[INFO] processing batch 100000-200000/382070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3850e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-33-34
[INFO] processing batch 200000-300000/382070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-33-34
[INFO] processing batch 300000-400000/382070
[33m[INFO] loading file 35-35/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5024e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-34-35
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2117e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-34-35
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.7617e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-34-35
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4243e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 2.8610e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-34-35
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7990e-04 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-34-35
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 36-36/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4048e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-35-36
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0793 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-35-36
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0146 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 7.8309e-04 - val_tp: 19998.0000 - val_fp: 2.0000 - val_tn: 79998.0000 - val_fn: 2.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-35-36
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0140 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-35-36
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.5041e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 7.9894e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-35-36
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 37-37/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0024 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-36-37
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9286e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-36-37
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 5.6842e-04 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-36-37
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0039 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-36-37
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 8.7751e-04 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 9.5243e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-36-37
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 38-38/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 5.4743e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-37-38
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0038 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-37-38
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 7.9644e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-37-38
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 7.1212e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 5.2073e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-37-38
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0018 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 3.5084e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-37-38
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 39-39/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.5881e-04 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 2.6440e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-38-39
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8376e-04 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-38-39
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3749e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-38-39
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6590e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0363 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-38-39
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8666e-04 - tp: 79963.0000 - fp: 37.0000 - tn: 319963.0000 - fn: 37.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-38-39
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 40-40/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (582070, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1920e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-39-40
[INFO] processing batch 100000-200000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 1.8427e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-39-40
[INFO] processing batch 200000-300000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0170 - tp: 79938.0000 - fp: 62.0000 - tn: 319938.0000 - fn: 62.0000 - accuracy: 0.9997 - precision: 0.9992 - recall: 0.9992 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-39-40
[INFO] processing batch 300000-400000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1072e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-39-40
[INFO] processing batch 400000-500000/582070
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0488e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-39-40
[INFO] processing batch 500000-600000/582070
[33m[INFO] loading file 41-41/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (522228, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/522228
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2265e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-40-41
[INFO] processing batch 100000-200000/522228
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0054 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0481 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-40-41
[INFO] processing batch 200000-300000/522228
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7972e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 2.7584e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-40-41
[INFO] processing batch 300000-400000/522228
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8740e-04 - tp: 79964.0000 - fp: 36.0000 - tn: 319964.0000 - fn: 36.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 4.3540e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-40-41
[INFO] processing batch 400000-500000/522228
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9866e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0645 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-40-41
[INFO] processing batch 500000-600000/522228
[33m[INFO] loading file 42-42/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (365742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/365742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0171 - tp: 79947.0000 - fp: 49.0000 - tn: 319951.0000 - fn: 53.0000 - accuracy: 0.9997 - precision: 0.9994 - recall: 0.9993 - auc: 0.9999 - val_loss: 7.4834e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-41-42
[INFO] processing batch 100000-200000/365742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9289e-04 - tp: 79963.0000 - fp: 37.0000 - tn: 319963.0000 - fn: 37.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 4.6643e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-41-42
[INFO] processing batch 200000-300000/365742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.6220e-04 - tp: 79959.0000 - fp: 41.0000 - tn: 319959.0000 - fn: 41.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 2.2359e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-41-42
[INFO] processing batch 300000-400000/365742
[33m[INFO] loading file 43-43/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1100e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.8991e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-42-43
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9185e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 8.5210e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-42-43
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0095 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.0433e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-42-43
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2135e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-42-43
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0074e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-42-43
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 44-44/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3497e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-43-44
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.4994e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-43-44
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9126e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-43-44
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0119 - tp: 79952.0000 - fp: 48.0000 - tn: 319952.0000 - fn: 48.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-43-44
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0024 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 1.1474e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-43-44
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 45-45/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5241e-04 - tp: 79984.0000 - fp: 16.0000 - tn: 319984.0000 - fn: 16.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 6.3121e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-44-45
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3297e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 4.2284e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-44-45
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5103e-04 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-44-45
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4640e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-44-45
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2692e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 3.5927e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-44-45
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 46-46/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3188e-04 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-45-46
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0013 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 1.0183e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-45-46
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9258e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-45-46
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.7399e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.9605e-12 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-45-46
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0073 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-45-46
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 47-47/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8047e-04 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-46-47
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8313e-04 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-46-47
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.4299e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-46-47
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0018 - tp: 79956.0000 - fp: 44.0000 - tn: 319956.0000 - fn: 44.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-46-47
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9535e-04 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-46-47
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 48-48/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0019 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-47-48
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2769e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-47-48
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.4605e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 6.4016e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-47-48
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9335e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-47-48
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0909e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 2.5778e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-47-48
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 49-49/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (565742, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0092 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 2.9027e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-48-49
[INFO] processing batch 100000-200000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7615e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 3.3922e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-48-49
[INFO] processing batch 200000-300000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0019 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 3.3539e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-48-49
[INFO] processing batch 300000-400000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3322e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 4.6998e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-48-49
[INFO] processing batch 400000-500000/565742
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2165e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-48-49
[INFO] processing batch 500000-600000/565742
[33m[INFO] loading file 50-50/50 on epoch 2/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (415724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/415724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.2864e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-49-50
[INFO] processing batch 100000-200000/415724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.6612e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-49-50
[INFO] processing batch 200000-300000/415724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4324e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-49-50
[INFO] processing batch 300000-400000/415724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5078e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.0218e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-49-50
[INFO] processing batch 400000-500000/415724
[33m[LOSS] 5.021817684173584e-05[0m
[33m[INFO] epoch 3/30[0m
[33m[INFO] loading file 1-1/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3173 - tp: 73837.0000 - fp: 6113.0000 - tn: 313887.0000 - fn: 6163.0000 - accuracy: 0.9693 - precision: 0.9235 - recall: 0.9230 - auc: 0.9895 - val_loss: 0.0713 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0179 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0056 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1344 - val_tp: 17757.0000 - val_fp: 2243.0000 - val_tn: 77757.0000 - val_fn: 2243.0000 - val_accuracy: 0.9551 - val_precision: 0.8878 - val_recall: 0.8878 - val_auc: 0.9299
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0483 - tp: 54687.0000 - fp: 25313.0000 - tn: 294687.0000 - fn: 25313.0000 - accuracy: 0.8734 - precision: 0.6836 - recall: 0.6836 - auc: 0.9124 - val_loss: 0.2294 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0649 - tp: 79196.0000 - fp: 804.0000 - tn: 319196.0000 - fn: 804.0000 - accuracy: 0.9960 - precision: 0.9900 - recall: 0.9900 - auc: 0.9999 - val_loss: 0.0014 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 2-2/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0077 - tp: 79878.0000 - fp: 122.0000 - tn: 319878.0000 - fn: 122.0000 - accuracy: 0.9994 - precision: 0.9985 - recall: 0.9985 - auc: 1.0000 - val_loss: 4.7857e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-1-2
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3417 - tp: 61546.0000 - fp: 18454.0000 - tn: 301546.0000 - fn: 18454.0000 - accuracy: 0.9077 - precision: 0.7693 - recall: 0.7693 - auc: 0.9727 - val_loss: 0.3505 - val_tp: 10588.0000 - val_fp: 9412.0000 - val_tn: 70588.0000 - val_fn: 9412.0000 - val_accuracy: 0.8118 - val_precision: 0.5294 - val_recall: 0.5294 - val_auc: 0.9446
[INFO] saving weights to checkpoints/dnn-epoch-003-files-1-2
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0461 - tp: 79704.0000 - fp: 296.0000 - tn: 319704.0000 - fn: 296.0000 - accuracy: 0.9985 - precision: 0.9963 - recall: 0.9963 - auc: 0.9990 - val_loss: 4.3124 - val_tp: 10613.0000 - val_fp: 9387.0000 - val_tn: 70613.0000 - val_fn: 9387.0000 - val_accuracy: 0.8123 - val_precision: 0.5307 - val_recall: 0.5307 - val_auc: 0.7274
[INFO] saving weights to checkpoints/dnn-epoch-003-files-1-2
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0503 - tp: 79191.0000 - fp: 809.0000 - tn: 319191.0000 - fn: 809.0000 - accuracy: 0.9960 - precision: 0.9899 - recall: 0.9899 - auc: 0.9996 - val_loss: 0.0018 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-1-2
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3392 - tp: 61979.0000 - fp: 18021.0000 - tn: 301979.0000 - fn: 18021.0000 - accuracy: 0.9099 - precision: 0.7747 - recall: 0.7747 - auc: 0.9734 - val_loss: 0.3444 - val_tp: 11064.0000 - val_fp: 8936.0000 - val_tn: 71064.0000 - val_fn: 8936.0000 - val_accuracy: 0.8213 - val_precision: 0.5532 - val_recall: 0.5532 - val_auc: 0.9451
[INFO] saving weights to checkpoints/dnn-epoch-003-files-1-2
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 3-3/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2813 - tp: 69571.0000 - fp: 10429.0000 - tn: 309571.0000 - fn: 10429.0000 - accuracy: 0.9479 - precision: 0.8696 - recall: 0.8696 - auc: 0.9847 - val_loss: 0.1417 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-2-3
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2466 - tp: 71934.0000 - fp: 8066.0000 - tn: 311934.0000 - fn: 8066.0000 - accuracy: 0.9597 - precision: 0.8992 - recall: 0.8992 - auc: 0.9881 - val_loss: 0.7722 - val_tp: 10611.0000 - val_fp: 9389.0000 - val_tn: 70611.0000 - val_fn: 9389.0000 - val_accuracy: 0.8122 - val_precision: 0.5306 - val_recall: 0.5306 - val_auc: 0.9449
[INFO] saving weights to checkpoints/dnn-epoch-003-files-2-3
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0044 - tp: 79740.0000 - fp: 260.0000 - tn: 319740.0000 - fn: 260.0000 - accuracy: 0.9987 - precision: 0.9967 - recall: 0.9967 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-2-3
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79921.0000 - fp: 79.0000 - tn: 319921.0000 - fn: 79.0000 - accuracy: 0.9996 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-2-3
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9233e-04 - tp: 79946.0000 - fp: 54.0000 - tn: 319946.0000 - fn: 54.0000 - accuracy: 0.9997 - precision: 0.9993 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-2-3
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 4-4/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.8514e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-3-4
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0075 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-3-4
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.8248e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.8863e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-3-4
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.8326e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 370.4472 - val_tp: 8905.0000 - val_fp: 11095.0000 - val_tn: 68905.0000 - val_fn: 11095.0000 - val_accuracy: 0.7781 - val_precision: 0.4453 - val_recall: 0.4453 - val_auc: 0.6533
[INFO] saving weights to checkpoints/dnn-epoch-003-files-3-4
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.1693 - tp: 43026.0000 - fp: 36972.0000 - tn: 283028.0000 - fn: 36974.0000 - accuracy: 0.8151 - precision: 0.5378 - recall: 0.5378 - auc: 0.9037 - val_loss: 0.5478 - val_tp: 14814.0000 - val_fp: 5186.0000 - val_tn: 74814.0000 - val_fn: 5186.0000 - val_accuracy: 0.8963 - val_precision: 0.7407 - val_recall: 0.7407 - val_auc: 0.9704
[INFO] saving weights to checkpoints/dnn-epoch-003-files-3-4
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 5-5/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5883 - tp: 64498.0000 - fp: 15502.0000 - tn: 304498.0000 - fn: 15502.0000 - accuracy: 0.9225 - precision: 0.8062 - recall: 0.8062 - auc: 0.9503 - val_loss: 0.7687 - val_tp: 12788.0000 - val_fp: 7212.0000 - val_tn: 72788.0000 - val_fn: 7212.0000 - val_accuracy: 0.8558 - val_precision: 0.6394 - val_recall: 0.6394 - val_auc: 0.8831
[INFO] saving weights to checkpoints/dnn-epoch-003-files-4-5
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5913 - tp: 50900.0000 - fp: 29100.0000 - tn: 290900.0000 - fn: 29100.0000 - accuracy: 0.8545 - precision: 0.6363 - recall: 0.6363 - auc: 0.9118 - val_loss: 0.5703 - val_tp: 12818.0000 - val_fp: 7182.0000 - val_tn: 72818.0000 - val_fn: 7182.0000 - val_accuracy: 0.8564 - val_precision: 0.6409 - val_recall: 0.6409 - val_auc: 0.9041
[INFO] saving weights to checkpoints/dnn-epoch-003-files-4-5
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1373 - tp: 78582.0000 - fp: 1418.0000 - tn: 318582.0000 - fn: 1418.0000 - accuracy: 0.9929 - precision: 0.9823 - recall: 0.9823 - auc: 0.9959 - val_loss: 0.0303 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-4-5
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0065 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-4-5
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.7962e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-4-5
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 6-6/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.7071e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-5-6
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.8576e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-5-6
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5230e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.3625e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-5-6
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0029 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-5-6
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.7939 - tp: 52495.0000 - fp: 27505.0000 - tn: 292495.0000 - fn: 27505.0000 - accuracy: 0.8625 - precision: 0.6562 - recall: 0.6562 - auc: 0.9233 - val_loss: 0.5724 - val_tp: 12786.0000 - val_fp: 7214.0000 - val_tn: 72786.0000 - val_fn: 7214.0000 - val_accuracy: 0.8557 - val_precision: 0.6393 - val_recall: 0.6393 - val_auc: 0.9018
[INFO] saving weights to checkpoints/dnn-epoch-003-files-5-6
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 7-7/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5318 - tp: 53470.0000 - fp: 26530.0000 - tn: 293470.0000 - fn: 26530.0000 - accuracy: 0.8673 - precision: 0.6684 - recall: 0.6684 - auc: 0.9331 - val_loss: 0.1373 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-6-7
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0273 - tp: 79839.0000 - fp: 161.0000 - tn: 319839.0000 - fn: 161.0000 - accuracy: 0.9992 - precision: 0.9980 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-6-7
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2290 - tp: 71608.0000 - fp: 8392.0000 - tn: 311608.0000 - fn: 8392.0000 - accuracy: 0.9580 - precision: 0.8951 - recall: 0.8951 - auc: 0.9903 - val_loss: 0.4417 - val_tp: 12848.0000 - val_fp: 7152.0000 - val_tn: 72848.0000 - val_fn: 7152.0000 - val_accuracy: 0.8570 - val_precision: 0.6424 - val_recall: 0.6424 - val_auc: 0.9680
[INFO] saving weights to checkpoints/dnn-epoch-003-files-6-7
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0068 - tp: 79502.0000 - fp: 498.0000 - tn: 319502.0000 - fn: 498.0000 - accuracy: 0.9975 - precision: 0.9938 - recall: 0.9938 - auc: 1.0000 - val_loss: 5.5594e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-6-7
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7370e-04 - tp: 79929.0000 - fp: 71.0000 - tn: 319929.0000 - fn: 71.0000 - accuracy: 0.9996 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-6-7
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 8-8/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9568e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-7-8
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.1058e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-7-8
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.0468e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-7-8
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1632 - tp: 76004.0000 - fp: 3995.0000 - tn: 316005.0000 - fn: 3996.0000 - accuracy: 0.9800 - precision: 0.9501 - recall: 0.9500 - auc: 0.9946 - val_loss: 0.3929 - val_tp: 16360.0000 - val_fp: 3640.0000 - val_tn: 76360.0000 - val_fn: 3640.0000 - val_accuracy: 0.9272 - val_precision: 0.8180 - val_recall: 0.8180 - val_auc: 0.9798
[INFO] saving weights to checkpoints/dnn-epoch-003-files-7-8
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1610 - tp: 74433.0000 - fp: 5567.0000 - tn: 314433.0000 - fn: 5567.0000 - accuracy: 0.9722 - precision: 0.9304 - recall: 0.9304 - auc: 0.9963 - val_loss: 0.0060 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-7-8
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 9-9/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-28_164554_99.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0071 - tp: 79788.0000 - fp: 212.0000 - tn: 319788.0000 - fn: 212.0000 - accuracy: 0.9989 - precision: 0.9973 - recall: 0.9973 - auc: 1.0000 - val_loss: 3.7018e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-8-9
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1187 - tp: 75367.0000 - fp: 4633.0000 - tn: 315367.0000 - fn: 4633.0000 - accuracy: 0.9768 - precision: 0.9421 - recall: 0.9421 - auc: 0.9972 - val_loss: 0.0564 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-8-9
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0057 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-8-9
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4041e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-8-9
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.3747e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-8-9
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 10-10/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.1941e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-9-10
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.0193e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 10.9457 - val_tp: 18412.0000 - val_fp: 1588.0000 - val_tn: 78412.0000 - val_fn: 1588.0000 - val_accuracy: 0.9682 - val_precision: 0.9206 - val_recall: 0.9206 - val_auc: 0.9504
[INFO] saving weights to checkpoints/dnn-epoch-003-files-9-10
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4936 - tp: 68797.0000 - fp: 11203.0000 - tn: 308797.0000 - fn: 11203.0000 - accuracy: 0.9440 - precision: 0.8600 - recall: 0.8600 - auc: 0.9816 - val_loss: 1.0537 - val_tp: 13136.0000 - val_fp: 6864.0000 - val_tn: 73136.0000 - val_fn: 6864.0000 - val_accuracy: 0.8627 - val_precision: 0.6568 - val_recall: 0.6568 - val_auc: 0.9693
[INFO] saving weights to checkpoints/dnn-epoch-003-files-9-10
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0780 - tp: 77349.0000 - fp: 2651.0000 - tn: 317349.0000 - fn: 2651.0000 - accuracy: 0.9867 - precision: 0.9669 - recall: 0.9669 - auc: 0.9990 - val_loss: 1.8678e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-9-10
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.4233e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-9-10
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 11-11/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5937e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-10-11
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.7473e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0991e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-10-11
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0049e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-10-11
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.4606e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-10-11
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 3.5113e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-10-11
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 12-12/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_134830_103.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.9319e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0008e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-11-12
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.4893e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-11-12
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2816 - tp: 73276.0000 - fp: 6724.0000 - tn: 313276.0000 - fn: 6724.0000 - accuracy: 0.9664 - precision: 0.9160 - recall: 0.9160 - auc: 0.9916 - val_loss: 0.0164 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-11-12
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1252 - tp: 76846.0000 - fp: 3154.0000 - tn: 316846.0000 - fn: 3154.0000 - accuracy: 0.9842 - precision: 0.9606 - recall: 0.9606 - auc: 0.9966 - val_loss: 0.0341 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-11-12
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0036 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-11-12
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 13-13/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 3.0045e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-12-13
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 2.4595e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-12-13
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 3.4259e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-12-13
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.1908 - tp: 75113.0000 - fp: 4887.0000 - tn: 315113.0000 - fn: 4887.0000 - accuracy: 0.9756 - precision: 0.9389 - recall: 0.9389 - auc: 0.9957 - val_loss: 0.0042 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-12-13
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0037 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 9.4566e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-12-13
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 14-14/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1929 - tp: 74149.0000 - fp: 5851.0000 - tn: 314149.0000 - fn: 5851.0000 - accuracy: 0.9707 - precision: 0.9269 - recall: 0.9269 - auc: 0.9934 - val_loss: 0.6156 - val_tp: 12807.0000 - val_fp: 7193.0000 - val_tn: 72807.0000 - val_fn: 7193.0000 - val_accuracy: 0.8561 - val_precision: 0.6403 - val_recall: 0.6403 - val_auc: 0.9677
[INFO] saving weights to checkpoints/dnn-epoch-003-files-13-14
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0177 - tp: 79170.0000 - fp: 830.0000 - tn: 319170.0000 - fn: 830.0000 - accuracy: 0.9959 - precision: 0.9896 - recall: 0.9896 - auc: 0.9999 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-13-14
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.7782e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-13-14
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.9943e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-13-14
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.2033e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-13-14
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 15-15/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (515724, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0208 - tp: 79966.0000 - fp: 34.0000 - tn: 319966.0000 - fn: 34.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 1.0701e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-14-15
[INFO] processing batch 100000-200000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0177 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-14-15
[INFO] processing batch 200000-300000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0122e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.6642e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-14-15
[INFO] processing batch 300000-400000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.2631e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 31.4575 - val_tp: 18892.0000 - val_fp: 1108.0000 - val_tn: 78892.0000 - val_fn: 1108.0000 - val_accuracy: 0.9778 - val_precision: 0.9446 - val_recall: 0.9446 - val_auc: 0.9654
[INFO] saving weights to checkpoints/dnn-epoch-003-files-14-15
[INFO] processing batch 400000-500000/515724
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8710 - tp: 43961.0000 - fp: 36037.0000 - tn: 283963.0000 - fn: 36039.0000 - accuracy: 0.8198 - precision: 0.5495 - recall: 0.5495 - auc: 0.6883 - val_loss: 1.7523 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-14-15
[INFO] processing batch 500000-600000/515724
[33m[INFO] loading file 16-16/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-29_190411_104.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (455414, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/455414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.7523 - tp: 69389.0000 - fp: 10598.0000 - tn: 309402.0000 - fn: 10611.0000 - accuracy: 0.9470 - precision: 0.8675 - recall: 0.8674 - auc: 0.9757 - val_loss: 1.0471 - val_tp: 19997.0000 - val_fp: 3.0000 - val_tn: 79997.0000 - val_fn: 3.0000 - val_accuracy: 0.9999 - val_precision: 0.9998 - val_recall: 0.9998 - val_auc: 0.9999
[INFO] saving weights to checkpoints/dnn-epoch-003-files-15-16
[INFO] processing batch 100000-200000/455414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.2465 - tp: 64264.0000 - fp: 15617.0000 - tn: 304383.0000 - fn: 15736.0000 - accuracy: 0.9216 - precision: 0.8045 - recall: 0.8033 - auc: 0.9524 - val_loss: 0.9271 - val_tp: 10624.0000 - val_fp: 9376.0000 - val_tn: 70624.0000 - val_fn: 9376.0000 - val_accuracy: 0.8125 - val_precision: 0.5312 - val_recall: 0.5312 - val_auc: 0.9451
[INFO] saving weights to checkpoints/dnn-epoch-003-files-15-16
[INFO] processing batch 200000-300000/455414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0093 - tp: 79858.0000 - fp: 138.0000 - tn: 319862.0000 - fn: 142.0000 - accuracy: 0.9993 - precision: 0.9983 - recall: 0.9982 - auc: 0.9997 - val_loss: 1.3405e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-15-16
[INFO] processing batch 300000-400000/455414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4730e-04 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3955e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-15-16
[INFO] processing batch 400000-500000/455414
[33m[INFO] loading file 17-17/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_001940_105.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (555414, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5263e-05 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.7503e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-16-17
[INFO] processing batch 100000-200000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3023e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.4964e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-16-17
[INFO] processing batch 200000-300000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9226e-05 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1310e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-16-17
[INFO] processing batch 300000-400000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5195 - tp: 71407.0000 - fp: 8593.0000 - tn: 311407.0000 - fn: 8593.0000 - accuracy: 0.9570 - precision: 0.8926 - recall: 0.8926 - auc: 0.9325 - val_loss: 1.6949 - val_tp: 16345.0000 - val_fp: 3655.0000 - val_tn: 76345.0000 - val_fn: 3655.0000 - val_accuracy: 0.9269 - val_precision: 0.8173 - val_recall: 0.8173 - val_auc: 0.8858
[INFO] saving weights to checkpoints/dnn-epoch-003-files-16-17
[INFO] processing batch 400000-500000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.5258 - tp: 70584.0000 - fp: 9416.0000 - tn: 310584.0000 - fn: 9416.0000 - accuracy: 0.9529 - precision: 0.8823 - recall: 0.8823 - auc: 0.9602 - val_loss: 0.0223 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-16-17
[INFO] processing batch 500000-600000/555414
[33m[INFO] loading file 18-18/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_053515_106.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (555414, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.6708 - tp: 77328.0000 - fp: 2672.0000 - tn: 317328.0000 - fn: 2672.0000 - accuracy: 0.9866 - precision: 0.9666 - recall: 0.9666 - auc: 0.9873 - val_loss: 1.7668 - val_tp: 12393.0000 - val_fp: 7607.0000 - val_tn: 72393.0000 - val_fn: 7607.0000 - val_accuracy: 0.8479 - val_precision: 0.6197 - val_recall: 0.6197 - val_auc: 0.8519
[INFO] saving weights to checkpoints/dnn-epoch-003-files-17-18
[INFO] processing batch 100000-200000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.5199 - tp: 60978.0000 - fp: 19019.0000 - tn: 300981.0000 - fn: 19022.0000 - accuracy: 0.9049 - precision: 0.7623 - recall: 0.7622 - auc: 0.9627 - val_loss: 2.7801 - val_tp: 19996.0000 - val_fp: 4.0000 - val_tn: 79996.0000 - val_fn: 4.0000 - val_accuracy: 0.9999 - val_precision: 0.9998 - val_recall: 0.9998 - val_auc: 0.9998
[INFO] saving weights to checkpoints/dnn-epoch-003-files-17-18
[INFO] processing batch 200000-300000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.2225 - tp: 77922.0000 - fp: 2076.0000 - tn: 317924.0000 - fn: 2078.0000 - accuracy: 0.9896 - precision: 0.9740 - recall: 0.9740 - auc: 0.9991 - val_loss: 3.1607e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-17-18
[INFO] processing batch 300000-400000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3832 - tp: 66924.0000 - fp: 13067.0000 - tn: 306933.0000 - fn: 13076.0000 - accuracy: 0.9346 - precision: 0.8366 - recall: 0.8365 - auc: 0.9828 - val_loss: 0.1406 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-17-18
[INFO] processing batch 400000-500000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0111 - tp: 79982.0000 - fp: 18.0000 - tn: 319982.0000 - fn: 18.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0048 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-17-18
[INFO] processing batch 500000-600000/555414
[33m[INFO] loading file 19-19/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (555414, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0028 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0014 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-18-19
[INFO] processing batch 100000-200000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.5645e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.4308e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-18-19
[INFO] processing batch 200000-300000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.6483e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3676e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-18-19
[INFO] processing batch 300000-400000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1476 - tp: 78824.0000 - fp: 1176.0000 - tn: 318824.0000 - fn: 1176.0000 - accuracy: 0.9941 - precision: 0.9853 - recall: 0.9853 - auc: 0.9919 - val_loss: 4.7731 - val_tp: 7843.0000 - val_fp: 12157.0000 - val_tn: 67843.0000 - val_fn: 12157.0000 - val_accuracy: 0.7569 - val_precision: 0.3922 - val_recall: 0.3922 - val_auc: 0.6201
[INFO] saving weights to checkpoints/dnn-epoch-003-files-18-19
[INFO] processing batch 400000-500000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1751 - tp: 78564.0000 - fp: 1436.0000 - tn: 318564.0000 - fn: 1436.0000 - accuracy: 0.9928 - precision: 0.9821 - recall: 0.9821 - auc: 0.9893 - val_loss: 19.0010 - val_tp: 17270.0000 - val_fp: 2730.0000 - val_tn: 77270.0000 - val_fn: 2730.0000 - val_accuracy: 0.9454 - val_precision: 0.8635 - val_recall: 0.8635 - val_auc: 0.9147
[INFO] saving weights to checkpoints/dnn-epoch-003-files-18-19
[INFO] processing batch 500000-600000/555414
[33m[INFO] loading file 20-20/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_105053_107.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (555414, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1235 - tp: 79867.0000 - fp: 133.0000 - tn: 319867.0000 - fn: 133.0000 - accuracy: 0.9993 - precision: 0.9983 - recall: 0.9983 - auc: 0.9990 - val_loss: 1.0666e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-19-20
[INFO] processing batch 100000-200000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.3780 - tp: 72796.0000 - fp: 7204.0000 - tn: 312796.0000 - fn: 7204.0000 - accuracy: 0.9640 - precision: 0.9100 - recall: 0.9100 - auc: 0.9758 - val_loss: 0.0251 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-19-20
[INFO] processing batch 200000-300000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0278 - tp: 79465.0000 - fp: 535.0000 - tn: 319465.0000 - fn: 535.0000 - accuracy: 0.9973 - precision: 0.9933 - recall: 0.9933 - auc: 0.9997 - val_loss: 0.0018 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-19-20
[INFO] processing batch 300000-400000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1874 - tp: 72795.0000 - fp: 7205.0000 - tn: 312795.0000 - fn: 7205.0000 - accuracy: 0.9640 - precision: 0.9099 - recall: 0.9099 - auc: 0.9956 - val_loss: 0.0816 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-19-20
[INFO] processing batch 400000-500000/555414
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0016 - tp: 79983.0000 - fp: 17.0000 - tn: 319983.0000 - fn: 17.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 7.8815e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-19-20
[INFO] processing batch 500000-600000/555414
[33m[INFO] loading file 21-21/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (555412, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/555412
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7683e-05 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.3580e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-20-21
[INFO] processing batch 100000-200000/555412
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4227e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0872e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-20-21
[INFO] processing batch 200000-300000/555412
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6055e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-20-21
[INFO] processing batch 300000-400000/555412
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.4863e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6702e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-20-21
[INFO] processing batch 400000-500000/555412
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 4.4328e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-20-21
[INFO] processing batch 500000-600000/555412
[33m[INFO] loading file 22-22/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (324453, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/324453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4478 - tp: 58522.0000 - fp: 21478.0000 - tn: 298522.0000 - fn: 21478.0000 - accuracy: 0.8926 - precision: 0.7315 - recall: 0.7315 - auc: 0.9101 - val_loss: 1.0902 - val_tp: 10615.0000 - val_fp: 9385.0000 - val_tn: 70615.0000 - val_fn: 9385.0000 - val_accuracy: 0.8123 - val_precision: 0.5307 - val_recall: 0.5307 - val_auc: 0.9268
[INFO] saving weights to checkpoints/dnn-epoch-003-files-21-22
[INFO] processing batch 100000-200000/324453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4550 - tp: 70459.0000 - fp: 9541.0000 - tn: 310459.0000 - fn: 9541.0000 - accuracy: 0.9523 - precision: 0.8807 - recall: 0.8807 - auc: 0.9761 - val_loss: 7.9025e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-21-22
[INFO] processing batch 200000-300000/324453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0574 - tp: 79730.0000 - fp: 270.0000 - tn: 319730.0000 - fn: 270.0000 - accuracy: 0.9987 - precision: 0.9966 - recall: 0.9966 - auc: 0.9994 - val_loss: 2.7300e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-21-22
[INFO] processing batch 300000-400000/324453
[33m[INFO] loading file 23-23/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-30_212042_109.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524453, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.7420 - tp: 61039.0000 - fp: 18961.0000 - tn: 301039.0000 - fn: 18961.0000 - accuracy: 0.9052 - precision: 0.7630 - recall: 0.7630 - auc: 0.9590 - val_loss: 0.3387 - val_tp: 15293.0000 - val_fp: 4707.0000 - val_tn: 75293.0000 - val_fn: 4707.0000 - val_accuracy: 0.9059 - val_precision: 0.7646 - val_recall: 0.7646 - val_auc: 0.9692
[INFO] saving weights to checkpoints/dnn-epoch-003-files-22-23
[INFO] processing batch 100000-200000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 2.4472e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-22-23
[INFO] processing batch 200000-300000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.0229e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6411e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-22-23
[INFO] processing batch 300000-400000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.4482e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2063e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-22-23
[INFO] processing batch 400000-500000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 1.0106e-04 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.6251e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-22-23
[INFO] processing batch 500000-600000/524453
[33m[INFO] loading file 24-24/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524453, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5305e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5030e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-23-24
[INFO] processing batch 100000-200000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.2515e-05 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2580e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-23-24
[INFO] processing batch 200000-300000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7006e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.0823e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-23-24
[INFO] processing batch 300000-400000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 2.4813e-06 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2521e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-23-24
[INFO] processing batch 400000-500000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.4574 - tp: 68466.0000 - fp: 11534.0000 - tn: 308466.0000 - fn: 11534.0000 - accuracy: 0.9423 - precision: 0.8558 - recall: 0.8558 - auc: 0.9772 - val_loss: 0.7483 - val_tp: 12885.0000 - val_fp: 7115.0000 - val_tn: 72885.0000 - val_fn: 7115.0000 - val_accuracy: 0.8577 - val_precision: 0.6442 - val_recall: 0.6442 - val_auc: 0.9684
[INFO] saving weights to checkpoints/dnn-epoch-003-files-23-24
[INFO] processing batch 500000-600000/524453
[33m[INFO] loading file 25-25/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524453, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0253 - tp: 79458.0000 - fp: 542.0000 - tn: 319458.0000 - fn: 542.0000 - accuracy: 0.9973 - precision: 0.9932 - recall: 0.9932 - auc: 0.9995 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-24-25
[INFO] processing batch 100000-200000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0096 - tp: 79879.0000 - fp: 121.0000 - tn: 319879.0000 - fn: 121.0000 - accuracy: 0.9994 - precision: 0.9985 - recall: 0.9985 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-24-25
[INFO] processing batch 200000-300000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0046 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-24-25
[INFO] processing batch 300000-400000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0250 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-24-25
[INFO] processing batch 400000-500000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0024 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-24-25
[INFO] processing batch 500000-600000/524453
[33m[INFO] loading file 26-26/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (524453, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.1709 - tp: 78288.0000 - fp: 1712.0000 - tn: 318288.0000 - fn: 1712.0000 - accuracy: 0.9914 - precision: 0.9786 - recall: 0.9786 - auc: 0.9947 - val_loss: 1.5774 - val_tp: 12894.0000 - val_fp: 7106.0000 - val_tn: 72894.0000 - val_fn: 7106.0000 - val_accuracy: 0.8579 - val_precision: 0.6447 - val_recall: 0.6447 - val_auc: 0.9684
[INFO] saving weights to checkpoints/dnn-epoch-003-files-25-26
[INFO] processing batch 100000-200000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0163 - tp: 79647.0000 - fp: 353.0000 - tn: 319647.0000 - fn: 353.0000 - accuracy: 0.9982 - precision: 0.9956 - recall: 0.9956 - auc: 0.9996 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-25-26
[INFO] processing batch 200000-300000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0046 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-25-26
[INFO] processing batch 300000-400000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0045 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-25-26
[INFO] processing batch 400000-500000/524453
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0046 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-25-26
[INFO] processing batch 500000-600000/524453
[33m[INFO] loading file 27-27/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_023431_110.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (464583, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/464583
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0039 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-26-27
[INFO] processing batch 100000-200000/464583
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0579 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-26-27
[INFO] processing batch 200000-300000/464583
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0049 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-26-27
[INFO] processing batch 300000-400000/464583
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0042 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-26-27
[INFO] processing batch 400000-500000/464583
[33m[INFO] loading file 28-28/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (398499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/398499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0055 - tp: 79958.0000 - fp: 42.0000 - tn: 319958.0000 - fn: 42.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 0.9997 - val_loss: 3.1915e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-27-28
[INFO] processing batch 100000-200000/398499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-27-28
[INFO] processing batch 200000-300000/398499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0042 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-27-28
[INFO] processing batch 300000-400000/398499
[33m[INFO] loading file 29-29/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (598499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0574 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-28-29
[INFO] processing batch 100000-200000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0028 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-28-29
[INFO] processing batch 200000-300000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0029 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 0.9997 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-28-29
[INFO] processing batch 300000-400000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0023 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-28-29
[INFO] processing batch 400000-500000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0019 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-28-29
[INFO] processing batch 500000-600000/598499
[33m[INFO] loading file 30-30/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (598499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0168 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 0.9998 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-29-30
[INFO] processing batch 100000-200000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0138 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-29-30
[INFO] processing batch 200000-300000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0016 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-29-30
[INFO] processing batch 300000-400000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0031 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-29-30
[INFO] processing batch 400000-500000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0197 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-29-30
[INFO] processing batch 500000-600000/598499
[33m[INFO] loading file 31-31/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (598499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0120 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0012 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-30-31
[INFO] processing batch 100000-200000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-30-31
[INFO] processing batch 200000-300000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.0011e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 2.8342e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-30-31
[INFO] processing batch 300000-400000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.7980e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-30-31
[INFO] processing batch 400000-500000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9375e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-30-31
[INFO] processing batch 500000-600000/598499
[33m[INFO] loading file 32-32/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (598499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.1451e-04 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 2.8459e-08 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-31-32
[INFO] processing batch 100000-200000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0021 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-31-32
[INFO] processing batch 200000-300000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0271 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-31-32
[INFO] processing batch 300000-400000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5377e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-31-32
[INFO] processing batch 400000-500000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3256e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-31-32
[INFO] processing batch 500000-600000/598499
[33m[INFO] loading file 33-33/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (598499, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2772e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-32-33
[INFO] processing batch 100000-200000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0088 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-32-33
[INFO] processing batch 200000-300000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0021 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-32-33
[INFO] processing batch 300000-400000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2522e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-32-33
[INFO] processing batch 400000-500000/598499
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0014 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-32-33
[INFO] processing batch 500000-600000/598499
[33m[INFO] loading file 34-34/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (439932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/439932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 9.9572e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-33-34
[INFO] processing batch 100000-200000/439932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2345e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 3.5323e-04 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-33-34
[INFO] processing batch 200000-300000/439932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0046 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0172 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-33-34
[INFO] processing batch 300000-400000/439932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4231e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-33-34
[INFO] processing batch 400000-500000/439932
[33m[INFO] loading file 35-35/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2632e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-34-35
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.9675e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0024 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-34-35
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7691e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.3608e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-34-35
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5249e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-34-35
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1214e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-34-35
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 36-36/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.1965e-04 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-35-36
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3270e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-35-36
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0079 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-35-36
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8619e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-35-36
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0090 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0018 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-35-36
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 37-37/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part10_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.8712e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-36-37
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7277e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-36-37
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2947e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-36-37
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0012 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-36-37
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.4306e-04 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-36-37
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 38-38/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part11_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2655e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-37-38
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1591e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-37-38
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.4336e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0017 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-37-38
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0062 - tp: 79962.0000 - fp: 38.0000 - tn: 319962.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-37-38
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9537e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-37-38
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 39-39/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0269 - tp: 79985.0000 - fp: 15.0000 - tn: 319985.0000 - fn: 15.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 6.1676e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-38-39
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.0558e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-38-39
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7861e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-38-39
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7484e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-38-39
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3299e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-38-39
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 40-40/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part13_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (539932, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7213e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0015 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-39-40
[INFO] processing batch 100000-200000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0047 - tp: 79951.0000 - fp: 49.0000 - tn: 319951.0000 - fn: 49.0000 - accuracy: 0.9998 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-39-40
[INFO] processing batch 200000-300000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5216e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-39-40
[INFO] processing batch 300000-400000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8901e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-39-40
[INFO] processing batch 400000-500000/539932
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.6621e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-39-40
[INFO] processing batch 500000-600000/539932
[33m[INFO] loading file 41-41/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_074818_111.log.part14_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (480090, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/480090
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0041 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-40-41
[INFO] processing batch 100000-200000/480090
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 8.8983e-04 - tp: 79961.0000 - fp: 39.0000 - tn: 319961.0000 - fn: 39.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 6.4969e-10 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-40-41
[INFO] processing batch 200000-300000/480090
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0046 - tp: 79959.0000 - fp: 41.0000 - tn: 319959.0000 - fn: 41.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-40-41
[INFO] processing batch 300000-400000/480090
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-40-41
[INFO] processing batch 400000-500000/480090
[33m[INFO] loading file 42-42/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part01_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (423604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/423604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.3668e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 1.7881e-11 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-41-42
[INFO] processing batch 100000-200000/423604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.0164e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-41-42
[INFO] processing batch 200000-300000/423604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3345e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-41-42
[INFO] processing batch 300000-400000/423604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4163e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 2.3628e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-41-42
[INFO] processing batch 400000-500000/423604
[33m[INFO] loading file 43-43/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part02_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5106e-04 - tp: 79979.0000 - fp: 21.0000 - tn: 319979.0000 - fn: 21.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-42-43
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.8738e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 9.4734e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-42-43
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0016 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-42-43
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.6156e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-42-43
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8447e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-42-43
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 44-44/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part03_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.3153e-04 - tp: 79982.0000 - fp: 18.0000 - tn: 319982.0000 - fn: 18.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-43-44
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.2017e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-43-44
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5641e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.0270e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-43-44
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3364e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 7.2559e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-43-44
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2829e-04 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-43-44
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 45-45/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part04_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0068 - tp: 79965.0000 - fp: 35.0000 - tn: 319965.0000 - fn: 35.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-44-45
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3418e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.4340e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-44-45
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3324e-04 - tp: 79974.0000 - fp: 26.0000 - tn: 319974.0000 - fn: 26.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-44-45
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1277e-04 - tp: 79963.0000 - fp: 37.0000 - tn: 319963.0000 - fn: 37.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-44-45
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.8350e-04 - tp: 79960.0000 - fp: 40.0000 - tn: 319960.0000 - fn: 40.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-44-45
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 46-46/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part05_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4146e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.0714e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-45-46
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7430e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 5.9605e-12 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-45-46
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.4018e-04 - tp: 79976.0000 - fp: 24.0000 - tn: 319976.0000 - fn: 24.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-45-46
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0011 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0111 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-45-46
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9066e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-45-46
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 47-47/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part06_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1688e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-46-47
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0059 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-46-47
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.1965e-04 - tp: 79978.0000 - fp: 22.0000 - tn: 319978.0000 - fn: 22.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 1.1264e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-46-47
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.7762e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-46-47
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.1832e-04 - tp: 79971.0000 - fp: 29.0000 - tn: 319971.0000 - fn: 29.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-46-47
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 48-48/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part07_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0015 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-47-48
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8502e-04 - tp: 79968.0000 - fp: 32.0000 - tn: 319968.0000 - fn: 32.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-47-48
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.5933e-04 - tp: 79980.0000 - fp: 20.0000 - tn: 319980.0000 - fn: 20.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-47-48
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.9462e-04 - tp: 79975.0000 - fp: 25.0000 - tn: 319975.0000 - fn: 25.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-47-48
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.7282e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-47-48
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 49-49/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part08_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (523604, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 5.9044e-04 - tp: 79977.0000 - fp: 23.0000 - tn: 319977.0000 - fn: 23.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 9.4785e-05 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-48-49
[INFO] processing batch 100000-200000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 7.2313e-04 - tp: 79970.0000 - fp: 30.0000 - tn: 319970.0000 - fn: 30.0000 - accuracy: 0.9999 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-48-49
[INFO] processing batch 200000-300000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.5838e-04 - tp: 79969.0000 - fp: 31.0000 - tn: 319969.0000 - fn: 31.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-48-49
[INFO] processing batch 300000-400000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0010 - tp: 79957.0000 - fp: 43.0000 - tn: 319957.0000 - fn: 43.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 19999.0000 - val_fp: 1.0000 - val_tn: 79999.0000 - val_fn: 1.0000 - val_accuracy: 1.0000 - val_precision: 0.9999 - val_recall: 0.9999 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-48-49
[INFO] processing batch 400000-500000/523604
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3609e-04 - tp: 79973.0000 - fp: 27.0000 - tn: 319973.0000 - fn: 27.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-48-49
[INFO] processing batch 500000-600000/523604
[33m[INFO] loading file 50-50/50 on epoch 3/30[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/train/2015-12-31_130240_112.log.part09_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (373586, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['appi_name', 'classification', 'dst', 'i/f_dir', 'i/f_name',
       'modbus_function_code', 'modbus_function_description',
       'modbus_transaction_id', 'orig', 'proto', 'proxy_src_ip', 's_port',
       'scada_tag', 'service', 'src', 'type'],
      dtype='object')
[INFO] processing batch 0-100000/373586
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.8294e-04 - tp: 79972.0000 - fp: 28.0000 - tn: 319972.0000 - fn: 28.0000 - accuracy: 0.9999 - precision: 0.9997 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-49-50
[INFO] processing batch 100000-200000/373586
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 6.3007e-04 - tp: 79967.0000 - fp: 33.0000 - tn: 319967.0000 - fn: 33.0000 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-49-50
[INFO] processing batch 200000-300000/373586
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 4s - loss: 0.0054 - tp: 79963.0000 - fp: 37.0000 - tn: 319963.0000 - fn: 37.0000 - accuracy: 0.9998 - precision: 0.9995 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
Using TensorFlow backend.
train.py:185: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass 'sort=False'.

To retain the current behavior and silence the warning, pass 'sort=True'.

  df = pd.concat(df_from_each_file, ignore_index=True)
[INFO] saving weights to checkpoints/dnn-epoch-003-files-49-50
[INFO] processing batch 300000-400000/373586
[33m[LOSS] 0.0[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.0  <  0.001
[STOPPING EARLY]: currentLoss < min_delta => 0.0  <  0.001
--- 3069.4825279712677 seconds ---
2020-02-03 03:45:14.810159: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-03 03:45:14.824639: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f913f553890 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-03 03:45:14.824672: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
=============================
        SCORING v0.3
=============================
Date: 2020-02-03 03:45:14.806454
[33m[INFO] using Sequential Dense layers[0m
[INFO] adding core layer 0
[INFO] created DNN
loading weights: checkpoints/*
loading file checkpoints/dnn-epoch-003-files-9-10
[33m[INFO] model summary:[0m
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 10)                160       
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 10)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 50)                550       
_________________________________________________________________
leaky_re_lu_2 (LeakyReLU)    (None, 50)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 10)                510       
_________________________________________________________________
leaky_re_lu_3 (LeakyReLU)    (None, 10)                0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 10)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 11        
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 1)                 0         
_________________________________________________________________
dense_5 (Dense)              (None, 5)                 10        
=================================================================
Total params: 1,241
Trainable params: 1,241
Non-trainable params: 0
_________________________________________________________________
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2015-12-31_130240_112.log.part10_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9992%,1:0.0008%]
[INFO] ** type:[0:99.9984%,1:0.0014%,2:0.0002%]
[INFO] ** i/f_name:[2:99.9984%,3:0.0016%]
[INFO] ** i/f_dir:[1:99.9986%,0:0.0014%]
[INFO] ** src:[22:38.114%,9:26.0964%,7:18.2424%,1:17.543%,14:0.0016%,11:0.0012%,10:0.001%,15:0.0004%]
[INFO] ** dst:[21:43.3686%,10:20.8412%,8:18.2422%,9:17.5434%,15:0.0016%,22:0.0014%,6:0.0006%,25:0.0004%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.997%,1:0.0016%,2:0.0014%]
[INFO] ** appi_name:[21:99.9962%,30:0.0016%,26:0.0004%,0:0.0004%,31:0.0002%,27:0.0002%,24:0.0002%,22:0.0002%,16:0.0002%,15:0.0002%,5:0.0002%]
[INFO] ** proxy_src_ip:[18:38.114%,20:26.0964%,13:18.2424%,6:17.543%,17:0.0016%,10:0.0012%,11:0.001%,16:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.9962%,-33.760870000000004:0.0036%,0.9188299999999999:0.0002%]
[INFO] ** modbus_function_description:[7:49.9988%,11:49.9974%,0:0.0036%,15:0.0002%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.0954%,1:20.841%,5:18.242%,4:17.5424%,2:17.2726%,0:0.0066%]
[INFO] ** service:[0.0048200000000000005:99.997%,-211.73542999999998:0.0016%,-186.44547:0.0004%,-211.08346:0.0004%,-186.43601999999998:0.0002%,-211.08818:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:26.0956%,-0.32151:20.8412%,-1.27613:18.242%,-0.23351:17.5424%,-0.33484:17.2728%,-71.33197:0.0016%,-9.54647:0.0008%,-0.24151:0.0006%,-64.19493:0.0004%,-1.02014:0.0004%,-71.14798:0.0004%,1.45709:0.0004%,-1.27347:0.0002%,1.46509:0.0002%,-68.79874000000001:0.0002%,-71.14931:0.0002%,-3.22139:0.0002%,-1.2708:0.0002%,1.47576:0.0002%]
[INFO] ** classification:[normal:79.7796%,Single Stage Single Point:20.2204%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 1 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.42245[0m
[33m[INFO] metrics:[0m
loss :  83.95364510375977
tp :  42245.0
fp :  57755.0
tn :  342245.0
fn :  57755.0
accuracy :  0.7689813375473022
precision :  0.4224500060081482
recall :  0.4224500060081482
auc :  0.7481288313865662

y_eval {0: 64261, 1: 35739}
pred {0: 77984, 1: 22014, 3: 2}
[INFO] confusion matrix for file 
[[42245 22014     0     2     0]
 [35739     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[42245 22014     0     2     0]
 [35739     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 1 1] (100000,)
[INFO] Validation score: [33m0.4225[0m
[33m[INFO] metrics:[0m
loss :  84.07940419433594
tp :  42250.0
fp :  57750.0
tn :  342250.0
fn :  57750.0
accuracy :  0.7690012454986572
precision :  0.42250001430511475
recall :  0.42250001430511475
auc :  0.7479932904243469

y_eval {0: 64231, 1: 35769}
pred {0: 78016, 1: 21971, 3: 13}
[INFO] confusion matrix for file 
[[42250 21971     0    10     0]
 [35766     0     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[84495 43985     0    12     0]
 [71505     0     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.4844[0m
[33m[INFO] metrics:[0m
loss :  69.64402383748055
tp :  48440.0
fp :  51560.0
tn :  348440.0
fn :  51560.0
accuracy :  0.7937652468681335
precision :  0.4844000041484833
recall :  0.4844000041484833
auc :  0.7870927453041077

y_eval {0: 70406, 1: 29594}
pred {0: 78034, 1: 21964, 3: 2}
[INFO] confusion matrix for file 
[[48440 21964     0     2     0]
 [29594     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[132935  65949      0     14      0]
 [101099      0      0      3      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77939[0m
[33m[INFO] metrics:[0m
loss :  0.35238112788200376
tp :  77939.0
fp :  22061.0
tn :  377939.0
fn :  22061.0
accuracy :  0.9117662310600281
precision :  0.7793899774551392
recall :  0.7793899774551392
auc :  0.9741954207420349

y_eval {0: 100000}
pred {0: 77939, 1: 22061}
[INFO] confusion matrix for file 
[[77939 22061     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[210874  88010      0     14      0]
 [101099      0      0      3      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77985[0m
[33m[INFO] metrics:[0m
loss :  0.35807328452110293
tp :  77984.0
fp :  22015.0
tn :  377985.0
fn :  22016.0
accuracy :  0.9119473695755005
precision :  0.7798478007316589
recall :  0.7798399925231934
auc :  0.9743019938468933

y_eval {0: 100000}
pred {0: 77985, 1: 22014, 3: 1}
[INFO] confusion matrix for file 
[[77985 22014     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[288859 110024      0     15      0]
 [101099      0      0      3      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2015-12-31_181648_113.log.part08_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9948%,2:0.0052%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9948%,3:0.0052%]
[INFO] ** i/f_dir:[1:99.9948%,0:0.0052%]
[INFO] ** src:[22:38.4402%,9:25.9738%,7:18.1202%,1:17.4598%,21:0.0052%,2:0.0004%,15:0.0002%,11:0.0002%]
[INFO] ** dst:[21:43.345%,10:21.0682%,8:18.1202%,9:17.4594%,15:0.0052%,22:0.0012%,19:0.0004%,26:0.0002%,24:0.0002%]
[INFO] ** proto:[0:99.9946%,1:0.0052%,2:0.0002%]
[INFO] ** appi_name:[21:99.9942%,30:0.0052%,23:0.0002%,22:0.0002%,18:0.0002%]
[INFO] ** proxy_src_ip:[18:38.4402%,20:25.9738%,13:18.1202%,6:17.4598%,17:0.0052%,14:0.0004%,16:0.0002%,10:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9942%,-33.760870000000004:0.0058%]
[INFO] ** modbus_function_description:[7:49.998%,11:49.9962%,0:0.0058%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:25.9726%,1:21.0682%,5:18.1198%,4:17.459%,2:17.3718%,0:0.0086%]
[INFO] ** service:[0.0048200000000000005:99.9942%,-211.73542999999998:0.0052%,-211.07873999999998:0.0004%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:25.973%,-0.32151:21.0682%,-1.27613:18.12%,-0.23351:17.4592%,-0.33484:17.372%,-71.33197:0.0052%,-0.24151:0.0006%,1.45709:0.0004%,8.38348:0.0004%,-1.27347:0.0002%,1.46509:0.0002%,1.47576:0.0002%,-71.14931:0.0002%,-1.02014:0.0002%]
[INFO] ** classification:[normal:80.1196%,Multi Stage Multi Point:14.47%,Single Stage Multi Point:5.4104%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.54871[0m
[33m[INFO] metrics:[0m
loss :  30.995479546995163
tp :  54871.0
fp :  45129.0
tn :  354871.0
fn :  45129.0
accuracy :  0.8194930553436279
precision :  0.5487099885940552
recall :  0.5487099885940552
auc :  0.7838841080665588

y_eval {0: 72948, 2: 27052}
pred {0: 77861, 1: 22139}
[INFO] confusion matrix for file 
[[54871 18077     0     0     0]
 [    0     0     0     0     0]
 [22990  4062     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[343730 128101      0     15      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77753[0m
[33m[INFO] metrics:[0m
loss :  0.35500496432304385
tp :  77753.0
fp :  22247.0
tn :  377753.0
fn :  22247.0
accuracy :  0.9110242128372192
precision :  0.7775300145149231
recall :  0.7775300145149231
auc :  0.973670482635498

y_eval {0: 100000}
pred {0: 77753, 1: 22247}
[INFO] confusion matrix for file 
[[77753 22247     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[421483 150348      0     15      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77757[0m
[33m[INFO] metrics:[0m
loss :  0.4190681452178955
tp :  77756.0
fp :  22243.0
tn :  377757.0
fn :  22244.0
accuracy :  0.9110355973243713
precision :  0.7775678038597107
recall :  0.7775599956512451
auc :  0.9736359119415283

y_eval {0: 100000}
pred {0: 77757, 1: 22232, 3: 11}
[INFO] confusion matrix for file 
[[77757 22232     0    11     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[499240 172580      0     26      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.59591[0m
[33m[INFO] metrics:[0m
loss :  16.772761665964126
tp :  59591.0
fp :  40409.0
tn :  359591.0
fn :  40409.0
accuracy :  0.8383603096008301
precision :  0.5959100127220154
recall :  0.5959100127220154
auc :  0.8085543513298035

y_eval {0: 74770, 4: 25230}
pred {0: 77847, 1: 22145, 3: 8}
[INFO] confusion matrix for file 
[[59591 15171     0     8     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [18256  6974     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[558831 187751      0     34      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [ 18256   6974      0      0      0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 4 0 ... 4 0 0] (100000,)
[INFO] Validation score: [33m0.4368[0m
[33m[INFO] metrics:[0m
loss :  31.104908711853028
tp :  43680.0
fp :  56320.0
tn :  343680.0
fn :  56320.0
accuracy :  0.7747178077697754
precision :  0.4368000030517578
recall :  0.4368000030517578
auc :  0.6650304794311523

y_eval {0: 52880, 4: 47120}
pred {0: 77772, 1: 22218, 3: 10}
[INFO] confusion matrix for file 
[[43680  9190     0    10     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [34092 13028     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[602511 196941      0     44      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [ 52348  20002      0      0      0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2015-12-31_181648_113.log.part09_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9966%,2:0.0034%]
[INFO] ** type:[0:99.9996%,1:0.0004%]
[INFO] ** i/f_name:[2:99.9938%,3:0.0062%]
[INFO] ** i/f_dir:[1:99.9942%,0:0.0058%]
[INFO] ** src:[22:38.4204%,9:25.9634%,7:18.123%,1:17.486%,21:0.0034%,14:0.0028%,15:0.0006%,2:0.0004%]
[INFO] ** dst:[21:43.3316%,10:21.0518%,8:18.123%,9:17.486%,15:0.0062%,22:0.0008%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.9934%,1:0.0062%,2:0.0004%]
[INFO] ** appi_name:[21:99.9934%,30:0.0062%,31:0.0002%,27:0.0002%]
[INFO] ** proxy_src_ip:[18:38.4204%,20:25.9634%,13:18.123%,6:17.486%,17:0.0062%,16:0.0006%,14:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.9934%,-33.760870000000004:0.0066%]
[INFO] ** modbus_function_description:[7:49.9972%,11:49.9962%,0:0.0066%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:25.9624%,1:21.0518%,5:18.1226%,4:17.4856%,2:17.3686%,0:0.009%]
[INFO] ** service:[0.0048200000000000005:99.9934%,-211.73542999999998:0.0062%,-186.43601999999998:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:25.9628%,-0.32151:21.0518%,-1.27613:18.1226%,-0.23351:17.4858%,-0.33484:17.3686%,-71.33197:0.0062%,-1.02014:0.0006%,-1.27347:0.0002%,11.862:0.0002%,1.45709:0.0002%,-0.24950999999999998:0.0002%,1.46509:0.0002%,1.47576:0.0002%,13.431270000000001:0.0002%,-1.2708:0.0002%]
[INFO] ** classification:[normal:73.9096%,Multi Stage Multi Point:26.0904%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 4 0 ... 4 4 4] (100000,)
[INFO] Validation score: [33m0.43789[0m
[33m[INFO] metrics:[0m
loss :  30.98780936431885
tp :  43788.0
fp :  56211.0
tn :  343789.0
fn :  56212.0
accuracy :  0.7751513123512268
precision :  0.4378843903541565
recall :  0.4378800094127655
auc :  0.6657189130783081

y_eval {0: 52975, 4: 47025}
pred {0: 77804, 1: 22188, 3: 8}
[INFO] confusion matrix for file 
[[43789  9178     0     8     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [34015 13010     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[646300 206119      0     52      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [ 86363  33012      0      0      0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [4 0 4 ... 0 4 0] (100000,)
[INFO] Validation score: [33m0.4389[0m
[33m[INFO] metrics:[0m
loss :  30.92674069732666
tp :  43890.0
fp :  56110.0
tn :  343890.0
fn :  56110.0
accuracy :  0.7755573987960815
precision :  0.4388999938964844
recall :  0.4388999938964844
auc :  0.6662832498550415

y_eval {0: 53029, 4: 46971}
pred {0: 77886, 1: 22107, 3: 7}
[INFO] confusion matrix for file 
[[43890  9132     0     7     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [33996 12975     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[690190 215251      0     59      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [120359  45987      0      0      0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [4 4 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.51514[0m
[33m[INFO] metrics:[0m
loss :  24.152304584198
tp :  51514.0
fp :  48486.0
tn :  351514.0
fn :  48486.0
accuracy :  0.8060608506202698
precision :  0.5151399970054626
recall :  0.5151399970054626
auc :  0.7351706624031067

y_eval {0: 63544, 4: 36456}
pred {0: 77865, 1: 22132, 3: 3}
[INFO] confusion matrix for file 
[[51514 12027     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [26351 10105     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[741704 227278      0     62      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [146710  56092      0      0      0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77754[0m
[33m[INFO] metrics:[0m
loss :  0.44310088661193847
tp :  77753.0
fp :  22246.0
tn :  377754.0
fn :  22247.0
accuracy :  0.9110254049301147
precision :  0.7775377631187439
recall :  0.7775300145149231
auc :  0.973548412322998

y_eval {0: 100000}
pred {0: 77754, 1: 22231, 3: 15}
[INFO] confusion matrix for file 
[[77754 22231     0    15     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[819458 249509      0     77      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [146710  56092      0      0      0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77791[0m
[33m[INFO] metrics:[0m
loss :  0.35471981920242307
tp :  77791.0
fp :  22209.0
tn :  377791.0
fn :  22209.0
accuracy :  0.9111743569374084
precision :  0.7779099941253662
recall :  0.7779099941253662
auc :  0.9736981391906738

y_eval {0: 100000}
pred {0: 77791, 1: 22209}
[INFO] confusion matrix for file 
[[77791 22209     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[897249 271718      0     77      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [146710  56092      0      0      0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2015-12-31_233049_114.log.part11_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9996%,2:0.0004%]
[INFO] ** type:[0:99.9994%,1:0.0004%,2:0.0002%]
[INFO] ** i/f_name:[2:99.9968%,3:0.0032%]
[INFO] ** i/f_dir:[1:99.9972%,0:0.0028%]
[INFO] ** src:[22:38.4512%,9:25.9642%,7:18.089%,1:17.4908%,14:0.0028%,11:0.0008%,21:0.0004%,15:0.0004%,2:0.0004%]
[INFO] ** dst:[21:43.3436%,10:21.0714%,8:18.0896%,9:17.4906%,15:0.0032%,22:0.001%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.9964%,1:0.0032%,2:0.0004%]
[INFO] ** appi_name:[21:99.9956%,30:0.0032%,31:0.0002%,27:0.0002%,24:0.0002%,16:0.0002%,15:0.0002%,5:0.0002%]
[INFO] ** proxy_src_ip:[18:38.4512%,20:25.9642%,13:18.089%,6:17.4908%,17:0.0032%,10:0.0008%,16:0.0004%,14:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.9956%,-33.760870000000004:0.0042%,0.9188299999999999:0.0002%]
[INFO] ** modbus_function_description:[7:49.999%,11:49.9966%,0:0.0042%,15:0.0002%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:25.9632%,1:21.0714%,5:18.0886%,4:17.4902%,2:17.3796%,0:0.007%]
[INFO] ** service:[0.0048200000000000005:99.9964%,-211.73542999999998:0.0032%,-186.43601999999998:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:25.9636%,-0.32151:21.0714%,-1.27613:18.0886%,-0.23351:17.4904%,-0.33484:17.3798%,-71.33197:0.0032%,6.286230000000001:0.0008%,1.45709:0.0004%,-1.02014:0.0004%,-1.27347:0.0002%,11.862:0.0002%,-0.24950999999999998:0.0002%,1.47576:0.0002%,-0.24151:0.0002%,13.431270000000001:0.0002%,-1.2708:0.0002%]
[INFO] ** classification:[normal:95.0874%,Single Stage Single Point:4.9126%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77794[0m
[33m[INFO] metrics:[0m
loss :  0.35489431540489197
tp :  77794.0
fp :  22206.0
tn :  377794.0
fn :  22206.0
accuracy :  0.9111853837966919
precision :  0.7779399752616882
recall :  0.7779399752616882
auc :  0.9736870527267456

y_eval {0: 100000}
pred {0: 77794, 1: 22206}
[INFO] confusion matrix for file 
[[77794 22206     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[975043 293924      0     77      0]
 [101099      0      0      3      0]
 [ 22990   4062      0      0      0]
 [     0      0      0      0      0]
 [146710  56092      0      0      0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77832[0m
[33m[INFO] metrics:[0m
loss :  0.360553650970459
tp :  77832.0
fp :  22168.0
tn :  377832.0
fn :  22168.0
accuracy :  0.9113383889198303
precision :  0.7783200144767761
recall :  0.7783200144767761
auc :  0.9737603068351746

y_eval {0: 100000}
pred {0: 77832, 1: 22167, 3: 1}
[INFO] confusion matrix for file 
[[77832 22167     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1052875  316091       0      78       0]
 [ 101099       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.70416[0m
[33m[INFO] metrics:[0m
loss :  4.637631320142746
tp :  70416.0
fp :  29584.0
tn :  370416.0
fn :  29584.0
accuracy :  0.8816707134246826
precision :  0.7041599750518799
recall :  0.7041599750518799
auc :  0.9372002482414246

y_eval {0: 92639, 1: 7361}
pred {0: 77777, 1: 22222, 3: 1}
[INFO] confusion matrix for file 
[[70416 22222     0     1     0]
 [ 7361     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1123291  338313       0      79       0]
 [ 108460       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.60575[0m
[33m[INFO] metrics:[0m
loss :  10.64063508649826
tp :  60575.0
fp :  39425.0
tn :  360575.0
fn :  39425.0
accuracy :  0.8423066139221191
precision :  0.6057500243186951
recall :  0.6057500243186951
auc :  0.8884781002998352

y_eval {0: 82798, 1: 17202}
pred {0: 77777, 1: 22207, 3: 16}
[INFO] confusion matrix for file 
[[60575 22207     0    16     0]
 [17202     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1183866  360520       0      95       0]
 [ 125662       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77821[0m
[33m[INFO] metrics:[0m
loss :  0.35415075088500975
tp :  77820.0
fp :  22179.0
tn :  377821.0
fn :  22180.0
accuracy :  0.9112929105758667
precision :  0.7782077789306641
recall :  0.7781999707221985
auc :  0.9738260507583618

y_eval {0: 100000}
pred {0: 77821, 1: 22179}
[INFO] confusion matrix for file 
[[77821 22179     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1261687  382699       0      95       0]
 [ 125662       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-01_151435_117.log.part03_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9976%,2:0.0024%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9966%,3:0.0034%]
[INFO] ** i/f_dir:[1:99.9966%,0:0.0034%]
[INFO] ** src:[22:38.1462%,9:26.099%,7:18.1904%,1:17.5592%,21:0.0028%,14:0.001%,3:0.0006%,15:0.0004%,2:0.0004%]
[INFO] ** dst:[21:43.376%,10:20.8688%,8:18.1902%,9:17.5592%,15:0.0034%,22:0.0008%,18:0.0004%,16:0.0004%,6:0.0004%,24:0.0002%,2:0.0002%]
[INFO] ** proto:[0:99.9958%,1:0.0034%,2:0.0008%]
[INFO] ** appi_name:[21:99.9952%,30:0.0034%,7:0.0006%,31:0.0004%,26:0.0002%,22:0.0002%]
[INFO] ** proxy_src_ip:[18:38.1462%,20:26.099%,13:18.1904%,6:17.5592%,17:0.0034%,1:0.0006%,16:0.0004%,14:0.0004%,7:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.9952%,-33.760870000000004:0.0048%]
[INFO] ** modbus_function_description:[7:49.9978%,11:49.9974%,0:0.0048%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.0982%,1:20.8688%,5:18.19%,4:17.5588%,2:17.2772%,0:0.007%]
[INFO] ** service:[0.0048200000000000005:99.9952%,-211.73542999999998:0.0034%,-211.09762999999998:0.0006%,-186.43601999999998:0.0004%,-211.08346:0.0002%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.0986%,-0.32151:20.8688%,-1.27613:18.1902%,-0.23351:17.559%,-0.33484:17.2774%,-71.33197:0.0034%,-1.02014:0.0004%,-71.14931:0.0002%,-0.24151:0.0002%,4.42897:0.0002%,-71.14798:0.0002%,1.47576:0.0002%,-1.27347:0.0002%,-0.78282:0.0002%,-2.92807:0.0002%,-4.88132:0.0002%,1.45709:0.0002%,9.18344:0.0002%]
[INFO] ** classification:[normal:84.2104%,Single Stage Single Point:15.7896%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.48712[0m
[33m[INFO] metrics:[0m
loss :  69.16506037918091
tp :  48712.0
fp :  51288.0
tn :  348712.0
fn :  51288.0
accuracy :  0.794849693775177
precision :  0.48712000250816345
recall :  0.48712000250816345
auc :  0.7889013886451721

y_eval {0: 70708, 1: 29292}
pred {0: 78004, 1: 21987, 3: 9}
[INFO] confusion matrix for file 
[[48712 21987     0     9     0]
 [29292     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1310399  404686       0     104       0]
 [ 154954       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 1 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.42159[0m
[33m[INFO] metrics:[0m
loss :  84.3380038317871
tp :  42157.0
fp :  57841.0
tn :  342159.0
fn :  57843.0
accuracy :  0.7686326503753662
precision :  0.42157843708992004
recall :  0.4215700030326843
auc :  0.7477217316627502

y_eval {0: 64215, 1: 35785}
pred {0: 77944, 1: 22050, 3: 6}
[INFO] confusion matrix for file 
[[42159 22050     0     6     0]
 [35785     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1352558  426736       0     110       0]
 [ 190739       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.64178[0m
[33m[INFO] metrics:[0m
loss :  32.92783191550255
tp :  64178.0
fp :  35822.0
tn :  364178.0
fn :  35822.0
accuracy :  0.8567214608192444
precision :  0.6417800188064575
recall :  0.6417800188064575
auc :  0.8865700364112854

y_eval {0: 86129, 1: 13871}
pred {0: 78049, 1: 21948, 3: 3}
[INFO] confusion matrix for file 
[[64178 21948     0     3     0]
 [13871     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1416736  448684       0     113       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77958[0m
[33m[INFO] metrics:[0m
loss :  0.3712890101623535
tp :  77958.0
fp :  22042.0
tn :  377958.0
fn :  22042.0
accuracy :  0.9118421673774719
precision :  0.7795799970626831
recall :  0.7795799970626831
auc :  0.9740781784057617

y_eval {0: 100000}
pred {0: 77958, 1: 22039, 3: 3}
[INFO] confusion matrix for file 
[[77958 22039     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1494694  470723       0     116       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77899[0m
[33m[INFO] metrics:[0m
loss :  0.36692334065437315
tp :  77899.0
fp :  22101.0
tn :  377899.0
fn :  22101.0
accuracy :  0.9116066694259644
precision :  0.7789899706840515
recall :  0.7789899706840515
auc :  0.9740051031112671

y_eval {0: 100000}
pred {0: 77899, 1: 22098, 3: 3}
[INFO] confusion matrix for file 
[[77899 22098     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1572593  492821       0     119       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-01_151435_117.log.part12_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9964%,2:0.0036%]
[INFO] ** type:[0:99.9994%,1:0.0004%,2:0.0002%]
[INFO] ** i/f_name:[2:99.996%,3:0.004%]
[INFO] ** i/f_dir:[1:99.9964%,0:0.0036%]
[INFO] ** src:[22:38.049%,9:26.1306%,7:18.2264%,1:17.587%,21:0.0036%,11:0.0014%,10:0.001%,14:0.0004%,15:0.0002%,8:0.0002%,4:0.0002%]
[INFO] ** dst:[21:43.355%,10:20.8256%,8:18.226%,9:17.5868%,15:0.004%,6:0.001%,22:0.0006%,25:0.0004%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.9942%,1:0.004%,2:0.0018%]
[INFO] ** appi_name:[21:99.9932%,30:0.004%,22:0.0006%,26:0.0004%,0:0.0004%,31:0.0002%,27:0.0002%,24:0.0002%,16:0.0002%,15:0.0002%,5:0.0002%,3:0.0002%]
[INFO] ** proxy_src_ip:[18:38.049%,20:26.1306%,13:18.2264%,6:17.587%,17:0.004%,10:0.0014%,11:0.001%,21:0.0002%,16:0.0002%,0:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9932%,-33.760870000000004:0.0066%,0.9188299999999999:0.0002%]
[INFO] ** modbus_function_description:[7:49.9974%,11:49.9958%,0:0.0066%,15:0.0002%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.13%,1:20.8244%,5:18.2258%,4:17.5864%,2:17.2242%,0:0.0092%]
[INFO] ** service:[0.0048200000000000005:99.9942%,-211.73542999999998:0.004%,-211.08818:0.0006%,-186.44547:0.0004%,-211.08346:0.0004%,-186.43601999999998:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:26.1304%,-0.32151:20.8246%,-1.27613:18.226%,-0.23351:17.5866%,-0.33484:17.2244%,-71.33197:0.004%,-6.49859:0.0008%,-71.14931:0.0006%,-0.24151:0.0004%,-64.19493:0.0004%,-71.14798:0.0004%,-68.79874000000001:0.0002%,-1.81611:0.0002%,1.47576:0.0002%,-1.27347:0.0002%,-61.95768:0.0002%,-1.02014:0.0002%,-1.2708:0.0002%]
[INFO] ** classification:[normal:97.1716%,Single Stage Single Point:2.8284%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78006[0m
[33m[INFO] metrics:[0m
loss :  0.3582409832859039
tp :  78006.0
fp :  21994.0
tn :  378006.0
fn :  21994.0
accuracy :  0.9120338559150696
precision :  0.7800599932670593
recall :  0.7800599932670593
auc :  0.9742340445518494

y_eval {0: 100000}
pred {0: 78006, 1: 21993, 3: 1}
[INFO] confusion matrix for file 
[[78006 21993     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1650599  514814       0     120       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78046[0m
[33m[INFO] metrics:[0m
loss :  0.3712045392894745
tp :  78046.0
fp :  21954.0
tn :  378046.0
fn :  21954.0
accuracy :  0.9121943116188049
precision :  0.780460000038147
recall :  0.780460000038147
auc :  0.9742923378944397

y_eval {0: 100000}
pred {0: 78046, 1: 21950, 3: 4}
[INFO] confusion matrix for file 
[[78046 21950     0     4     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1728645  536764       0     124       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77999[0m
[33m[INFO] metrics:[0m
loss :  0.35819489100456237
tp :  77998.0
fp :  22001.0
tn :  377999.0
fn :  22002.0
accuracy :  0.9120042324066162
precision :  0.7799878120422363
recall :  0.7799800038337708
auc :  0.9742681384086609

y_eval {0: 100000}
pred {0: 77999, 1: 22000, 3: 1}
[INFO] confusion matrix for file 
[[77999 22000     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1806644  558764       0     125       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78063[0m
[33m[INFO] metrics:[0m
loss :  0.42210769110679625
tp :  78063.0
fp :  21937.0
tn :  378063.0
fn :  21937.0
accuracy :  0.912261426448822
precision :  0.7806299924850464
recall :  0.7806299924850464
auc :  0.9743478298187256

y_eval {0: 100000}
pred {0: 78063, 1: 21924, 3: 13}
[INFO] confusion matrix for file 
[[78063 21924     0    13     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1884707  580688       0     138       0]
 [ 204610       0       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 1 1] (100000,)
[INFO] Validation score: [33m0.71616[0m
[33m[INFO] metrics:[0m
loss :  3.86033008764267
tp :  71615.0
fp :  28384.0
tn :  371616.0
fn :  28385.0
accuracy :  0.8864625096321106
precision :  0.7161571383476257
recall :  0.7161499857902527
auc :  0.9367609024047852

y_eval {0: 85858, 1: 14142}
pred {0: 78038, 1: 21950, 3: 12}
[INFO] confusion matrix for file 
[[67756 18090     0    12     0]
 [10282  3860     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1952463  598778       0     150       0]
 [ 214892    3860       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-01_151435_117.log.part13_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9986%,2:0.0014%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.998%,3:0.002%]
[INFO] ** i/f_dir:[1:99.998%,0:0.002%]
[INFO] ** src:[22:38.1186%,9:26.0918%,7:18.2288%,1:17.558%,21:0.0018%,14:0.0006%,15:0.0004%]
[INFO] ** dst:[21:43.3702%,10:20.8392%,8:18.2288%,9:17.5578%,15:0.002%,22:0.0012%,24:0.0004%,18:0.0002%,6:0.0002%]
[INFO] ** proto:[0:99.9976%,1:0.002%,2:0.0004%]
[INFO] ** appi_name:[21:99.9976%,30:0.002%,31:0.0002%,22:0.0002%]
[INFO] ** proxy_src_ip:[18:38.1186%,20:26.0918%,13:18.2288%,6:17.558%,17:0.002%,16:0.0004%,7:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.9976%,-33.760870000000004:0.0024%]
[INFO] ** modbus_function_description:[7:49.9998%,11:49.9978%,0:0.0024%]
[INFO] ** modbus_transaction_id:65534 (13.1068%)
[INFO] ** scada_tag:[3:26.0906%,1:20.839%,5:18.2282%,4:17.5572%,2:17.2792%,0:0.0058%]
[INFO] ** service:[0.0048200000000000005:99.9976%,-211.73542999999998:0.002%,-186.43601999999998:0.0002%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.0908%,-0.32151:20.8392%,-1.27613:18.2284%,-0.23351:17.5576%,-0.33484:17.2794%,-71.33197:0.002%,1.45709:0.0006%,-1.27347:0.0004%,-1.02014:0.0004%,-0.24950999999999998:0.0002%,1.46509:0.0002%,1.47576:0.0002%,4.42897:0.0002%,-0.24151:0.0002%,-71.14931:0.0002%]
[INFO] ** classification:[normal:87.5394%,Single Stage Single Point:12.4606%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 0 ... 1 0 1] (100000,)
[INFO] Validation score: [33m0.56808[0m
[33m[INFO] metrics:[0m
loss :  11.628356636810302
tp :  56808.0
fp :  43192.0
tn :  356808.0
fn :  43192.0
accuracy :  0.8272291421890259
precision :  0.5680800080299377
recall :  0.5680800080299377
auc :  0.8500350117683411

y_eval {0: 53028, 1: 46972}
pred {0: 78016, 1: 21980, 3: 4}
[INFO] confusion matrix for file 
[[43926  9098     0     4     0]
 [34090 12882     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[1996389  607876       0     154       0]
 [ 248982   16742       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.70994[0m
[33m[INFO] metrics:[0m
loss :  4.000039430212975
tp :  70994.0
fp :  29006.0
tn :  370994.0
fn :  29006.0
accuracy :  0.8839870095252991
precision :  0.7099400162696838
recall :  0.7099400162696838
auc :  0.9334980845451355

y_eval {0: 84669, 1: 15331}
pred {0: 77969, 1: 22027, 3: 4}
[INFO] confusion matrix for file 
[[66816 17849     0     4     0]
 [11153  4178     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2063205  625725       0     158       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78002[0m
[33m[INFO] metrics:[0m
loss :  0.35859201847076416
tp :  78002.0
fp :  21998.0
tn :  378002.0
fn :  21998.0
accuracy :  0.9120182991027832
precision :  0.780019998550415
recall :  0.780019998550415
auc :  0.9741823077201843

y_eval {0: 100000}
pred {0: 78002, 1: 21997, 3: 1}
[INFO] confusion matrix for file 
[[78002 21997     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2141207  647722       0     159       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77986[0m
[33m[INFO] metrics:[0m
loss :  0.3584114909553528
tp :  77986.0
fp :  22014.0
tn :  377986.0
fn :  22014.0
accuracy :  0.9119539260864258
precision :  0.7798600196838379
recall :  0.7798600196838379
auc :  0.9742316007614136

y_eval {0: 100000}
pred {0: 77986, 1: 22013, 3: 1}
[INFO] confusion matrix for file 
[[77986 22013     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2219193  669735       0     160       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77987[0m
[33m[INFO] metrics:[0m
loss :  0.3642177435207367
tp :  77987.0
fp :  22013.0
tn :  377987.0
fn :  22013.0
accuracy :  0.9119586944580078
precision :  0.7798699736595154
recall :  0.7798699736595154
auc :  0.9742701053619385

y_eval {0: 100000}
pred {0: 77987, 1: 22011, 3: 2}
[INFO] confusion matrix for file 
[[77987 22011     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2297180  691746       0     162       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-01_203011_118.log.part06_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9988%,2:0.0008%,1:0.0004%]
[INFO] ** type:[0:99.9986%,1:0.001%,2:0.0004%]
[INFO] ** i/f_name:[2:99.998%,3:0.002%]
[INFO] ** i/f_dir:[1:99.9982%,0:0.0018%]
[INFO] ** src:[22:38.138%,9:26.0904%,7:18.1762%,1:17.5892%,10:0.0018%,11:0.0016%,14:0.0012%,21:0.0008%,15:0.0006%,4:0.0002%]
[INFO] ** dst:[21:43.3838%,10:20.845%,8:18.1758%,9:17.589%,15:0.002%,22:0.0014%,6:0.0014%,25:0.0004%,24:0.0004%,18:0.0004%,7:0.0004%]
[INFO] ** proto:[0:99.9954%,2:0.0026%,1:0.002%]
[INFO] ** appi_name:[21:99.9944%,30:0.002%,26:0.0008%,22:0.0006%,31:0.0004%,27:0.0004%,15:0.0004%,0:0.0004%,24:0.0002%,16:0.0002%,5:0.0002%]
[INFO] ** proxy_src_ip:[18:38.138%,20:26.0904%,13:18.1762%,6:17.5892%,17:0.002%,11:0.0018%,10:0.0016%,16:0.0006%,21:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9944%,-33.760870000000004:0.0052%,0.9188299999999999:0.0004%]
[INFO] ** modbus_function_description:[7:49.998%,11:49.9964%,0:0.0052%,15:0.0004%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.0894%,1:20.8438%,5:18.1758%,4:17.5888%,2:17.294%,0:0.0082%]
[INFO] ** service:[0.0048200000000000005:99.9954%,-211.73542999999998:0.002%,-211.08346:0.0008%,-211.08818:0.0006%,-186.44547:0.0004%,-186.43601999999998:0.0004%,-202.75898:0.0004%]
[INFO] ** s_port:[1.45442:26.0898%,-0.32151:20.844%,-1.27613:18.1758%,-0.23351:17.589%,-0.33484:17.294%,-71.33197:0.002%,-6.46392:0.0008%,-71.14798:0.0008%,1.45709:0.0006%,-71.14931:0.0006%,-1.02014:0.0006%,-1.27347:0.0004%,-68.79874000000001:0.0004%,-64.19493:0.0004%,-1.81611:0.0004%,-6.49859:0.0002%,-0.24151:0.0002%]
[INFO] ** classification:[normal:90.7904%,Single Stage Multi Point:5.7378%,Single Stage Single Point:3.4718%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77965[0m
[33m[INFO] metrics:[0m
loss :  0.35822380276203153
tp :  77965.0
fp :  22035.0
tn :  377965.0
fn :  22035.0
accuracy :  0.9118702411651611
precision :  0.7796499729156494
recall :  0.7796499729156494
auc :  0.9742269515991211

y_eval {0: 100000}
pred {0: 77965, 1: 22034, 3: 1}
[INFO] confusion matrix for file 
[[77965 22034     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2375145  713780       0     163       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77949[0m
[33m[INFO] metrics:[0m
loss :  0.3524736291027069
tp :  77948.0
fp :  22051.0
tn :  377949.0
fn :  22052.0
accuracy :  0.9118043184280396
precision :  0.7794877886772156
recall :  0.77947998046875
auc :  0.9741760492324829

y_eval {0: 100000}
pred {0: 77949, 1: 22051}
[INFO] confusion matrix for file 
[[77949 22051     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2453094  735831       0     163       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78034[0m
[33m[INFO] metrics:[0m
loss :  0.3891417717552185
tp :  78033.0
fp :  21966.0
tn :  378034.0
fn :  21967.0
accuracy :  0.9121448397636414
precision :  0.7803378105163574
recall :  0.7803300023078918
auc :  0.9741809368133545

y_eval {0: 100000}
pred {0: 78034, 1: 21960, 3: 6}
[INFO] confusion matrix for file 
[[78034 21960     0     6     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2531128  757791       0     169       0]
 [ 260135   20920       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.70154[0m
[33m[INFO] metrics:[0m
loss :  4.59920884018898
tp :  70154.0
fp :  29846.0
tn :  370154.0
fn :  29846.0
accuracy :  0.880628228187561
precision :  0.7015399932861328
recall :  0.7015399932861328
auc :  0.9283726811408997

y_eval {0: 82641, 1: 17359}
pred {0: 77977, 1: 22008, 3: 15}
[INFO] confusion matrix for file 
[[65386 17240     0    15     0]
 [12591  4768     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2596514  775031       0     184       0]
 [ 272726   25688       0       3       0]
 [  22990    4062       0       0       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 2 2] (100000,)
[INFO] Validation score: [33m0.57159[0m
[33m[INFO] metrics:[0m
loss :  10.214609872922898
tp :  57159.0
fp :  42840.0
tn :  357160.0
fn :  42841.0
accuracy :  0.8286325931549072
precision :  0.5715957283973694
recall :  0.571590006351471
auc :  0.7860865592956543

y_eval {0: 71311, 2: 28689}
pred {0: 77962, 1: 22036, 3: 2}
[INFO] confusion matrix for file 
[[57159 14151     0     1     0]
 [    0     0     0     0     0]
 [20803  7885     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2653673  789182       0     185       0]
 [ 272726   25688       0       3       0]
 [  43793   11947       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-01_203011_118.log.part07_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9996%,2:0.0004%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9994%,3:0.0006%]
[INFO] ** i/f_dir:[1:99.9994%,0:0.0006%]
[INFO] ** src:[22:38.0718%,9:26.1174%,7:18.2344%,1:17.5754%,21:0.0008%,14:0.0002%]
[INFO] ** dst:[21:43.3758%,10:20.813%,8:18.2342%,9:17.5754%,15:0.0006%,22:0.0004%,24:0.0002%,18:0.0002%,6:0.0002%]
[INFO] ** proto:[0:99.999%,1:0.0006%,2:0.0004%]
[INFO] ** appi_name:[21:99.999%,30:0.0006%,31:0.0002%,22:0.0002%]
[INFO] ** proxy_src_ip:[18:38.0718%,20:26.1174%,13:18.2344%,6:17.5754%,17:0.0006%,7:0.0004%]
[INFO] ** modbus_function_code:[0.02961:99.999%,-33.760870000000004:0.001%]
[INFO] ** modbus_function_description:[7:49.9996%,11:49.9994%,0:0.001%]
[INFO] ** modbus_transaction_id:65301 (13.0602%)
[INFO] ** scada_tag:[3:26.1164%,1:20.813%,5:18.2338%,4:17.575%,2:17.2586%,0:0.0032%]
[INFO] ** service:[0.0048200000000000005:99.999%,-211.73542999999998:0.0006%,-186.43601999999998:0.0002%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.1168%,-0.32151:20.813%,-1.27613:18.234%,-0.23351:17.5752%,-0.33484:17.2588%,-71.33197:0.0006%,-1.27347:0.0002%,1.45709:0.0002%,1.46509:0.0002%,1.47576:0.0002%,4.42897:0.0002%,-0.24151:0.0002%,-71.14931:0.0002%,-1.2708:0.0002%]
[INFO] ** classification:[normal:89.2092%,Single Stage Multi Point:10.7908%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [2 0 0 ... 2 0 0] (100000,)
[INFO] Validation score: [33m0.43895[0m
[33m[INFO] metrics:[0m
loss :  16.505438119812013
tp :  43895.0
fp :  56105.0
tn :  343895.0
fn :  56105.0
accuracy :  0.7755786776542664
precision :  0.4389500021934509
recall :  0.4389500021934509
auc :  0.6662840843200684

y_eval {0: 53027, 2: 46973}
pred {0: 77976, 1: 22024}
[INFO] confusion matrix for file 
[[43895  9132     0     0     0]
 [    0     0     0     0     0]
 [34081 12892     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2697568  798314       0     185       0]
 [ 272726   25688       0       3       0]
 [  77874   24839       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 2 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.72976[0m
[33m[INFO] metrics:[0m
loss :  2.796014540719986
tp :  72976.0
fp :  27024.0
tn :  372976.0
fn :  27024.0
accuracy :  0.8919138312339783
precision :  0.7297599911689758
recall :  0.7297599911689758
auc :  0.9285025596618652

y_eval {0: 93019, 2: 6981}
pred {0: 78055, 1: 21942, 3: 3}
[INFO] confusion matrix for file 
[[72976 20040     0     3     0]
 [    0     0     0     0     0]
 [ 5079  1902     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2770544  818354       0     188       0]
 [ 272726   25688       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.7794[0m
[33m[INFO] metrics:[0m
loss :  0.3584214402389526
tp :  77940.0
fp :  22060.0
tn :  377940.0
fn :  22060.0
accuracy :  0.9117704629898071
precision :  0.7793999910354614
recall :  0.7793999910354614
auc :  0.9742242693901062

y_eval {0: 100000}
pred {0: 77940, 1: 22059, 3: 1}
[INFO] confusion matrix for file 
[[77940 22059     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2848484  840413       0     189       0]
 [ 272726   25688       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.7802[0m
[33m[INFO] metrics:[0m
loss :  0.35175194147109984
tp :  78018.0
fp :  21980.0
tn :  378020.0
fn :  21982.0
accuracy :  0.9120866656303406
precision :  0.7801955938339233
recall :  0.7801799774169922
auc :  0.9743186831474304

y_eval {0: 100000}
pred {0: 78020, 1: 21980}
[INFO] confusion matrix for file 
[[78020 21980     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[2926504  862393       0     189       0]
 [ 272726   25688       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78125[0m
[33m[INFO] metrics:[0m
loss :  0.3576532582378387
tp :  78125.0
fp :  21875.0
tn :  378125.0
fn :  21875.0
accuracy :  0.9125102162361145
precision :  0.78125
recall :  0.78125
auc :  0.974382221698761

y_eval {0: 100000}
pred {0: 78125, 1: 21874, 3: 1}
[INFO] confusion matrix for file 
[[78125 21874     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3004629  884267       0     190       0]
 [ 272726   25688       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_014558_119.log.part06_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.997%,2:0.003%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9964%,3:0.0036%]
[INFO] ** i/f_dir:[1:99.9964%,0:0.0036%]
[INFO] ** src:[22:38.0288%,9:26.1242%,7:18.2114%,1:17.6294%,21:0.003%,2:0.0008%,14:0.0006%,3:0.0006%,15:0.0004%,4:0.0004%,11:0.0002%,8:0.0002%]
[INFO] ** dst:[21:43.357%,10:20.7952%,8:18.2116%,9:17.629%,15:0.0036%,22:0.0012%,16:0.0008%,19:0.0004%,17:0.0004%,6:0.0004%,26:0.0002%,24:0.0002%]
[INFO] ** proto:[0:99.9958%,1:0.0036%,2:0.0006%]
[INFO] ** appi_name:[21:99.9942%,30:0.0036%,17:0.0008%,22:0.0004%,28:0.0002%,26:0.0002%,23:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proxy_src_ip:[18:38.0288%,20:26.1242%,13:18.2114%,6:17.6294%,17:0.0036%,14:0.0008%,1:0.0006%,21:0.0004%,16:0.0004%,10:0.0002%,0:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9942%,-33.760870000000004:0.0058%]
[INFO] ** modbus_function_description:[7:49.9974%,11:49.9968%,0:0.0058%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.123%,1:20.7952%,5:18.211%,4:17.6288%,2:17.2336%,0:0.0084%]
[INFO] ** service:[0.0048200000000000005:99.9942%,-211.73542999999998:0.0036%,-205.44247:0.0008%,-211.07873999999998:0.0004%,-211.08818:0.0004%,-211.09762999999998:0.0002%,-211.08346:0.0002%,-185.9872:0.0002%]
[INFO] ** s_port:[1.45442:26.1234%,-0.32151:20.7952%,-1.27613:18.2112%,-0.23351:17.629%,-0.33484:17.2336%,-71.33197:0.0036%,1.45709:0.0006%,9.89942:0.0004%,-1.02014:0.0004%,-71.14931:0.0004%,-58.035180000000004:0.0002%,-5.1466400000000005:0.0002%,-0.24151:0.0002%,-5.08665:0.0002%,1.46509:0.0002%,-71.14798:0.0002%,-0.24950999999999998:0.0002%,2.05573:0.0002%,-43.76509:0.0002%,-1.27347:0.0002%,-58.032509999999995:0.0002%]
[INFO] ** classification:[normal:81.41%,Single Stage Single Point:18.59%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 1 0 1] (100000,)
[INFO] Validation score: [33m0.63268[0m
[33m[INFO] metrics:[0m
loss :  8.229602374353409
tp :  63267.0
fp :  36732.0
tn :  363268.0
fn :  36733.0
accuracy :  0.8530663847923279
precision :  0.6326763033866882
recall :  0.632669985294342
auc :  0.887881875038147

y_eval {0: 67398, 1: 32602}
pred {0: 78014, 1: 21974, 3: 12}
[INFO] confusion matrix for file 
[[54340 13046     0    12     0]
 [23674  8928     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3058969  897313       0     202       0]
 [ 296400   34616       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 1 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.56883[0m
[33m[INFO] metrics:[0m
loss :  11.643799451828002
tp :  56883.0
fp :  43117.0
tn :  356883.0
fn :  43117.0
accuracy :  0.827529788017273
precision :  0.5688300132751465
recall :  0.5688300132751465
auc :  0.8501821756362915

y_eval {0: 53052, 1: 46948}
pred {0: 78003, 1: 21989, 3: 8}
[INFO] confusion matrix for file 
[[43969  9075     0     8     0]
 [34034 12914     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3102938  906388       0     210       0]
 [ 330434   47530       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.72029[0m
[33m[INFO] metrics:[0m
loss :  3.564474764089584
tp :  72028.0
fp :  27971.0
tn :  372029.0
fn :  27972.0
accuracy :  0.8881250023841858
precision :  0.7202872037887573
recall :  0.7202799916267395
auc :  0.9389720559120178

y_eval {0: 86600, 1: 13400}
pred {0: 78083, 1: 21917}
[INFO] confusion matrix for file 
[[68356 18244     0     0     0]
 [ 9727  3673     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3171294  924632       0     210       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78048[0m
[33m[INFO] metrics:[0m
loss :  0.3799365932226181
tp :  78048.0
fp :  21952.0
tn :  378048.0
fn :  21952.0
accuracy :  0.9122021198272705
precision :  0.7804800271987915
recall :  0.7804800271987915
auc :  0.9742667078971863

y_eval {0: 100000}
pred {0: 78048, 1: 21947, 3: 5}
[INFO] confusion matrix for file 
[[78048 21947     0     5     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3249342  946579       0     215       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78102[0m
[33m[INFO] metrics:[0m
loss :  0.3720121374130249
tp :  78102.0
fp :  21898.0
tn :  378102.0
fn :  21898.0
accuracy :  0.9124176502227783
precision :  0.7810199856758118
recall :  0.7810199856758118
auc :  0.9744170904159546

y_eval {0: 100000}
pred {0: 78102, 1: 21894, 3: 4}
[INFO] confusion matrix for file 
[[78102 21894     0     4     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3327444  968473       0     219       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_121729_121.log.part12_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9986%,1:0.0008%,2:0.0006%]
[INFO] ** type:[0:99.9984%,1:0.0014%,2:0.0002%]
[INFO] ** i/f_name:[2:99.9978%,3:0.0022%]
[INFO] ** i/f_dir:[1:99.998%,0:0.002%]
[INFO] ** src:[22:38.1136%,9:26.1176%,7:18.2158%,1:17.5472%,14:0.0016%,11:0.0014%,10:0.001%,21:0.0006%,15:0.0004%,2:0.0004%,4:0.0002%,3:0.0002%]
[INFO] ** dst:[21:43.4022%,10:20.8284%,8:18.2156%,9:17.547%,15:0.0022%,22:0.002%,6:0.0008%,26:0.0004%,25:0.0004%,19:0.0004%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.996%,1:0.0022%,2:0.0018%]
[INFO] ** appi_name:[21:99.9946%,30:0.0022%,22:0.0006%,26:0.0004%,0:0.0004%,31:0.0002%,27:0.0002%,24:0.0002%,23:0.0002%,18:0.0002%,16:0.0002%,15:0.0002%,14:0.0002%,5:0.0002%]
[INFO] ** proxy_src_ip:[18:38.1136%,20:26.1176%,13:18.2158%,6:17.5472%,17:0.0022%,10:0.0014%,11:0.001%,16:0.0004%,14:0.0004%,21:0.0002%,1:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9946%,-33.760870000000004:0.0052%,0.9188299999999999:0.0002%]
[INFO] ** modbus_function_description:[7:49.9984%,11:49.9962%,0:0.0052%,15:0.0002%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.1164%,1:20.8284%,5:18.2154%,4:17.5466%,2:17.285%,0:0.0082%]
[INFO] ** service:[0.0048200000000000005:99.9954%,-211.73542999999998:0.0022%,-211.08818:0.0006%,-186.44547:0.0004%,-211.08346:0.0004%,-211.07873999999998:0.0004%,-186.42657:0.0002%,-186.43601999999998:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:26.1168%,-0.32151:20.8284%,-1.27613:18.2154%,-0.23351:17.5468%,-0.33484:17.2852%,-71.33197:0.0022%,-10.59042:0.0008%,-71.14931:0.0006%,-64.19493:0.0004%,-1.02014:0.0004%,-0.24151:0.0004%,-71.14798:0.0004%,1.45709:0.0004%,10.54739:0.0004%,1.47576:0.0002%,1.46509:0.0002%,-1.27347:0.0002%,-68.79874000000001:0.0002%,13.34061:0.0002%,-1.2708:0.0002%,15.264529999999999:0.0002%]
[INFO] ** classification:[normal:99.9996%,Single Stage Multi Point:0.0004%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77999[0m
[33m[INFO] metrics:[0m
loss :  0.41397070677757264
tp :  77999.0
fp :  22001.0
tn :  377999.0
fn :  22001.0
accuracy :  0.9120063185691833
precision :  0.779990017414093
recall :  0.779990017414093
auc :  0.9741302132606506

y_eval {0: 100000}
pred {0: 77999, 1: 21990, 3: 11}
[INFO] confusion matrix for file 
[[77999 21990     0    11     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3405443  990463       0     230       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77968[0m
[33m[INFO] metrics:[0m
loss :  0.39235361946105957
tp :  77968.0
fp :  22032.0
tn :  377968.0
fn :  22032.0
accuracy :  0.9118832945823669
precision :  0.7796800136566162
recall :  0.7796800136566162
auc :  0.9741408228874207

y_eval {0: 100000}
pred {0: 77968, 1: 22022, 3: 10}
[INFO] confusion matrix for file 
[[77968 22022     0    10     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3483411 1012485       0     240       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78042[0m
[33m[INFO] metrics:[0m
loss :  0.3704585788726807
tp :  78042.0
fp :  21958.0
tn :  378042.0
fn :  21958.0
accuracy :  0.9121786952018738
precision :  0.7804200053215027
recall :  0.7804200053215027
auc :  0.9742642045021057

y_eval {0: 100000}
pred {0: 78042, 1: 21955, 3: 3}
[INFO] confusion matrix for file 
[[78042 21955     0     3     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3561453 1034440       0     243       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78006[0m
[33m[INFO] metrics:[0m
loss :  0.36421903527259825
tp :  78005.0
fp :  21994.0
tn :  378006.0
fn :  21995.0
accuracy :  0.9120324850082397
precision :  0.7800577878952026
recall :  0.7800499796867371
auc :  0.9742805361747742

y_eval {0: 100000}
pred {0: 78006, 1: 21992, 3: 2}
[INFO] confusion matrix for file 
[[78006 21992     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3639459 1056432       0     245       0]
 [ 340161   51203       0       3       0]
 [  82953   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77952[0m
[33m[INFO] metrics:[0m
loss :  0.35306012231826783
tp :  77952.0
fp :  22048.0
tn :  377952.0
fn :  22048.0
accuracy :  0.9118185043334961
precision :  0.7795199751853943
recall :  0.7795199751853943
auc :  0.9741262197494507

y_eval {0: 99998, 2: 2}
pred {0: 77954, 1: 22046}
[INFO] confusion matrix for file 
[[77952 22046     0     0     0]
 [    0     0     0     0     0]
 [    2     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3717411 1078478       0     245       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_121729_121.log.part13_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9996%,2:0.0004%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9994%,3:0.0006%]
[INFO] ** i/f_dir:[1:99.9994%,0:0.0006%]
[INFO] ** src:[22:38.162%,9:26.1664%,7:18.0742%,1:17.5958%,21:0.0008%,17:0.0002%,15:0.0002%,14:0.0002%,8:0.0002%]
[INFO] ** dst:[21:43.4352%,10:20.8932%,8:18.0744%,9:17.5952%,22:0.0006%,18:0.0006%,15:0.0006%,6:0.0002%]
[INFO] ** proto:[0:99.9986%,2:0.0008%,1:0.0006%]
[INFO] ** appi_name:[21:99.9986%,31:0.0006%,30:0.0006%,22:0.0002%]
[INFO] ** proxy_src_ip:[18:38.162%,20:26.1664%,13:18.0742%,6:17.5958%,17:0.0006%,7:0.0004%,16:0.0002%,2:0.0002%,0:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9986%,-33.760870000000004:0.0014%]
[INFO] ** modbus_function_description:[7:49.9998%,11:49.9988%,0:0.0014%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.166%,1:20.893%,5:18.0738%,4:17.5948%,2:17.2688%,0:0.0036%]
[INFO] ** service:[0.0048200000000000005:99.9986%,-211.73542999999998:0.0006%,-186.43601999999998:0.0006%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.1662%,-0.32151:20.8932%,-1.27613:18.074%,-0.23351:17.5952%,-0.33484:17.2688%,-71.33197:0.0006%,15.43653:0.0004%,-0.24151:0.0004%,-0.24950999999999998:0.0002%,1.46509:0.0002%,4.42897:0.0002%,-71.14931:0.0002%,-1.02014:0.0002%,-1.2708:0.0002%]
[INFO] ** classification:[normal:92.8338%,Multi Stage Single Point:7.1662%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77993[0m
[33m[INFO] metrics:[0m
loss :  0.3627344995355606
tp :  77993.0
fp :  22007.0
tn :  377993.0
fn :  22007.0
accuracy :  0.9119818210601807
precision :  0.7799299955368042
recall :  0.7799299955368042
auc :  0.9741114974021912

y_eval {0: 100000}
pred {0: 77993, 1: 22005, 3: 2}
[INFO] confusion matrix for file 
[[77993 22005     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3795404 1100483       0     247       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [      0       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.66794[0m
[33m[INFO] metrics:[0m
loss :  16.396016711711884
tp :  66793.0
fp :  33206.0
tn :  366794.0
fn :  33207.0
accuracy :  0.8671793937683105
precision :  0.6679366827011108
recall :  0.667930006980896
auc :  0.8921868801116943

y_eval {0: 88734, 3: 11266}
pred {0: 78060, 1: 21939, 3: 1}
[INFO] confusion matrix for file 
[[66794 21939     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [11266     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3862198 1122422       0     248       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  11266       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.7038[0m
[33m[INFO] metrics:[0m
loss :  11.176414454965592
tp :  70380.0
fp :  29620.0
tn :  370380.0
fn :  29620.0
accuracy :  0.8815289735794067
precision :  0.7038000226020813
recall :  0.7038000226020813
auc :  0.9189841747283936

y_eval {0: 92435, 3: 7565}
pred {0: 77945, 1: 22054, 3: 1}
[INFO] confusion matrix for file 
[[70380 22054     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [ 7565     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[3932578 1144476       0     249       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  18831       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77883[0m
[33m[INFO] metrics:[0m
loss :  0.3593450528240204
tp :  77883.0
fp :  22117.0
tn :  377883.0
fn :  22117.0
accuracy :  0.9115427136421204
precision :  0.7788299918174744
recall :  0.7788299918174744
auc :  0.9739747047424316

y_eval {0: 100000}
pred {0: 77883, 1: 22116, 3: 1}
[INFO] confusion matrix for file 
[[77883 22116     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4010461 1166592       0     250       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  18831       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.60977[0m
[33m[INFO] metrics:[0m
loss :  24.68358611085892
tp :  60976.0
fp :  39023.0
tn :  360977.0
fn :  39024.0
accuracy :  0.8439106941223145
precision :  0.6097661256790161
recall :  0.6097599864006042
auc :  0.8502824902534485

y_eval {0: 83000, 3: 17000}
pred {0: 77977, 1: 22021, 3: 2}
[INFO] confusion matrix for file 
[[60977 22021     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [17000     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4071438 1188613       0     252       0]
 [ 340161   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  35831       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_121729_121.log.part14_sorted-labeled.csv
[INFO] process dataset, shape: (439824, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (439824, 16)

[INFO] analyzing data
[INFO] 439824 rows
[INFO] ** orig:[0:99.99955%,2:0.00045%]
[INFO] ** type:[0:99.99977%,1:0.00023%]
[INFO] ** i/f_name:[2:99.99864%,3:0.00136%]
[INFO] ** i/f_dir:[1:99.99886%,0:0.00114%]
[INFO] ** src:[22:38.09456%,9:26.09703%,7:18.22161%,1:17.58408%,14:0.00091%,21:0.00068%,15:0.00068%,4:0.00023%,3:0.00023%]
[INFO] ** dst:[21:43.35416%,10:20.83674%,8:18.22115%,9:17.58385%,22:0.00159%,15:0.00136%,24:0.00045%,25:0.00023%,6:0.00023%,2:0.00023%]
[INFO] ** proto:[0:99.99818%,1:0.00136%,2:0.00045%]
[INFO] ** appi_name:[21:99.99795%,30:0.00136%,26:0.00023%,8:0.00023%,0:0.00023%]
[INFO] ** proxy_src_ip:[18:38.09456%,20:26.09703%,13:18.22161%,6:17.58408%,17:0.00136%,16:0.00068%,21:0.00023%,7:0.00023%,1:0.00023%]
[INFO] ** modbus_function_code:[0.02961:99.99795%,-33.760870000000004:0.00205%]
[INFO] ** modbus_function_description:[7:49.99955%,11:49.99841%,0:0.00205%]
[INFO] ** modbus_transaction_id:65536 (14.90051%)
[INFO] ** scada_tag:[3:26.09612%,1:20.83652%,5:18.22092%,4:17.5834%,2:17.25758%,0:0.00546%]
[INFO] ** service:[0.0048200000000000005:99.99795%,-211.73542999999998:0.00136%,-186.44547:0.00023%,-211.08346:0.00023%,-211.35747999999998:0.00023%]
[INFO] ** s_port:[1.45442:26.09635%,-0.32151:20.83674%,-1.27613:18.22115%,-0.23351:17.58362%,-0.33484:17.25781%,-71.33197:0.00136%,-1.02014:0.00068%,-1.27347:0.00045%,1.45709:0.00045%,-0.24151:0.00045%,-71.14798:0.00023%,1.47576:0.00023%,-64.19493:0.00023%,-1.2708:0.00023%]
[INFO] ** classification:[normal:93.34597%,Single Stage Single Point:4.48861%,Multi Stage Single Point:2.16541%]
[INFO] processing batch 0-100000/439824
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [3 0 0 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.6483[0m
[33m[INFO] metrics:[0m
loss :  15.969801916809082
tp :  64830.0
fp :  35170.0
tn :  364830.0
fn :  35170.0
accuracy :  0.8593282103538513
precision :  0.6482999920845032
recall :  0.6482999920845032
auc :  0.8865804076194763

y_eval {0: 86777, 1: 3699, 3: 9524}
pred {0: 78053, 1: 21946, 3: 1}
[INFO] confusion matrix for file 
[[64830 21946     0     1     0]
 [ 3699     0     0     0     0]
 [    0     0     0     0     0]
 [ 9524     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4136268 1210559       0     253       0]
 [ 343860   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/439824
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.62022[0m
[33m[INFO] metrics:[0m
loss :  9.863491999311448
tp :  62022.0
fp :  37978.0
tn :  362022.0
fn :  37978.0
accuracy :  0.8480942249298096
precision :  0.6202200055122375
recall :  0.6202200055122375
auc :  0.8950231671333313

y_eval {0: 83957, 1: 16043}
pred {0: 78065, 1: 21932, 3: 3}
[INFO] confusion matrix for file 
[[62022 21932     0     3     0]
 [16043     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4198290 1232491       0     256       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/439824
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.7801[0m
[33m[INFO] metrics:[0m
loss :  0.3745191198253632
tp :  78010.0
fp :  21990.0
tn :  378010.0
fn :  21990.0
accuracy :  0.9120509624481201
precision :  0.7800999879837036
recall :  0.7800999879837036
auc :  0.9742359519004822

y_eval {0: 100000}
pred {0: 78010, 1: 21986, 3: 4}
[INFO] confusion matrix for file 
[[78010 21986     0     4     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4276300 1254477       0     260       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/439824
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77995[0m
[33m[INFO] metrics:[0m
loss :  0.3522907134628296
tp :  77994.0
fp :  22005.0
tn :  377995.0
fn :  22006.0
accuracy :  0.9119876027107239
precision :  0.779947817325592
recall :  0.7799400091171265
auc :  0.9742259979248047

y_eval {0: 100000}
pred {0: 77995, 1: 22005}
[INFO] confusion matrix for file 
[[77995 22005     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4354295 1276482       0     260       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/439824
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_172943_122.log.part03_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9976%,2:0.002%,1:0.0004%]
[INFO] ** type:[0:99.9988%,1:0.001%,2:0.0002%]
[INFO] ** i/f_name:[2:99.9944%,3:0.0056%]
[INFO] ** i/f_dir:[1:99.9946%,0:0.0054%]
[INFO] ** src:[22:38.2234%,9:26.124%,7:18.075%,1:17.571%,14:0.0036%,21:0.0022%,2:0.0004%,11:0.0002%,10:0.0002%]
[INFO] ** dst:[21:43.4342%,10:20.913%,8:18.0748%,9:17.5708%,15:0.0056%,22:0.0006%,25:0.0004%,24:0.0002%,18:0.0002%,7:0.0002%]
[INFO] ** proto:[0:99.9936%,1:0.0056%,2:0.0008%]
[INFO] ** appi_name:[21:99.9934%,30:0.0056%,0:0.0004%,31:0.0002%,27:0.0002%,15:0.0002%]
[INFO] ** proxy_src_ip:[18:38.2234%,20:26.124%,13:18.075%,6:17.571%,17:0.0056%,14:0.0004%,11:0.0002%,10:0.0002%,7:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9934%,-33.760870000000004:0.0064%,0.9188299999999999:0.0002%]
[INFO] ** modbus_function_description:[7:49.997%,11:49.9964%,0:0.0064%,15:0.0002%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.1234%,1:20.9128%,5:18.0748%,4:17.5704%,2:17.3102%,0:0.0084%]
[INFO] ** service:[0.0048200000000000005:99.9936%,-211.73542999999998:0.0056%,-186.44547:0.0004%,-186.43601999999998:0.0002%,-202.75898:0.0002%]
[INFO] ** s_port:[1.45442:26.1238%,-0.32151:20.913%,-1.27613:18.0748%,-0.23351:17.5706%,-0.33484:17.3104%,-71.33197:0.0056%,-0.24151:0.0004%,-64.19493:0.0004%,-1.27347:0.0002%,-10.59042:0.0002%,11.862:0.0002%,1.47576:0.0002%,13.431270000000001:0.0002%]
[INFO] ** classification:[normal:94.9296%,Single Stage Single Point:5.0704%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77981[0m
[33m[INFO] metrics:[0m
loss :  0.3524741684436798
tp :  77981.0
fp :  22019.0
tn :  377981.0
fn :  22019.0
accuracy :  0.9119341969490051
precision :  0.7798100113868713
recall :  0.7798100113868713
auc :  0.9741564393043518

y_eval {0: 100000}
pred {0: 77981, 1: 22019}
[INFO] confusion matrix for file 
[[77981 22019     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4432276 1298501       0     260       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77988[0m
[33m[INFO] metrics:[0m
loss :  0.3644720757865906
tp :  77988.0
fp :  22012.0
tn :  377988.0
fn :  22012.0
accuracy :  0.9119628071784973
precision :  0.7798799872398376
recall :  0.7798799872398376
auc :  0.9742233753204346

y_eval {0: 100000}
pred {0: 77988, 1: 22010, 3: 2}
[INFO] confusion matrix for file 
[[77988 22010     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4510264 1320511       0     262       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77966[0m
[33m[INFO] metrics:[0m
loss :  0.3578523955631256
tp :  77966.0
fp :  22034.0
tn :  377966.0
fn :  22034.0
accuracy :  0.9118757843971252
precision :  0.7796599864959717
recall :  0.7796599864959717
auc :  0.9741761684417725

y_eval {0: 100000}
pred {0: 77966, 1: 22033, 3: 1}
[INFO] confusion matrix for file 
[[77966 22033     0     1     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4588230 1342544       0     263       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77804[0m
[33m[INFO] metrics:[0m
loss :  0.49057146931648254
tp :  77804.0
fp :  22196.0
tn :  377804.0
fn :  22196.0
accuracy :  0.9112269282341003
precision :  0.7780399918556213
recall :  0.7780399918556213
auc :  0.9737246036529541

y_eval {0: 100000}
pred {0: 77804, 1: 22173, 3: 23}
[INFO] confusion matrix for file 
[[77804 22173     0    23     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4666034 1364717       0     286       0]
 [ 359903   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.525[0m
[33m[INFO] metrics:[0m
loss :  60.212256165981294
tp :  52497.0
fp :  47500.0
tn :  352500.0
fn :  47503.0
accuracy :  0.8099960088729858
precision :  0.5249857306480408
recall :  0.5249699950218201
auc :  0.813109815120697

y_eval {0: 74648, 1: 25352}
pred {0: 77852, 1: 22142, 3: 6}
[INFO] confusion matrix for file 
[[52500 22142     0     6     0]
 [25352     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4718534 1386859       0     292       0]
 [ 385255   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_172943_122.log.part04_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.995%,2:0.005%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9944%,3:0.0056%]
[INFO] ** i/f_dir:[1:99.9944%,0:0.0056%]
[INFO] ** src:[22:38.203%,9:26.1368%,7:18.1014%,1:17.5508%,21:0.005%,3:0.0008%,15:0.0006%,14:0.0006%,4:0.0004%,2:0.0004%,8:0.0002%]
[INFO] ** dst:[21:43.4462%,10:20.893%,8:18.1012%,9:17.5504%,15:0.0056%,22:0.0016%,16:0.0008%,6:0.0006%,17:0.0004%,24:0.0002%]
[INFO] ** proto:[0:99.9938%,1:0.0056%,2:0.0006%]
[INFO] ** appi_name:[21:99.9926%,30:0.0056%,17:0.0008%,26:0.0004%,28:0.0002%,22:0.0002%,7:0.0002%]
[INFO] ** proxy_src_ip:[18:38.203%,20:26.1368%,13:18.1014%,6:17.5508%,17:0.0056%,1:0.0008%,16:0.0006%,21:0.0004%,14:0.0004%,0:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9926%,-33.760870000000004:0.0074%]
[INFO] ** modbus_function_description:[7:49.9972%,11:49.9954%,0:0.0074%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.1358%,1:20.8928%,5:18.1008%,4:17.55%,2:17.31%,0:0.0106%]
[INFO] ** service:[0.0048200000000000005:99.9926%,-211.73542999999998:0.0056%,-205.44247:0.0008%,-211.08346:0.0004%,-211.09762999999998:0.0002%,-185.9872:0.0002%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.136%,-0.32151:20.893%,-1.27613:18.101%,-0.23351:17.5502%,-0.33484:17.31%,-71.33197:0.0056%,1.45709:0.0006%,-1.02014:0.0006%,-71.14798:0.0004%,-0.24151:0.0004%,-5.1466400000000005:0.0002%,-58.035180000000004:0.0002%,-1.2708:0.0002%,-71.14931:0.0002%,1.47576:0.0002%,-1.27347:0.0002%,-5.08665:0.0002%,-0.24950999999999998:0.0002%,2.05573:0.0002%,-43.76509:0.0002%,-58.032509999999995:0.0002%]
[INFO] ** classification:[normal:64.3482%,Single Stage Single Point:35.6518%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.42377[0m
[33m[INFO] metrics:[0m
loss :  83.86673141479493
tp :  42377.0
fp :  57623.0
tn :  342377.0
fn :  57623.0
accuracy :  0.7695096135139465
precision :  0.423770010471344
recall :  0.423770010471344
auc :  0.7487490177154541

y_eval {0: 64457, 1: 35543}
pred {0: 77920, 1: 22073, 3: 7}
[INFO] confusion matrix for file 
[[42377 22073     0     7     0]
 [35543     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4760911 1408932       0     299       0]
 [ 420798   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.42475[0m
[33m[INFO] metrics:[0m
loss :  83.84289545166016
tp :  42475.0
fp :  57525.0
tn :  342475.0
fn :  57525.0
accuracy :  0.7699021697044373
precision :  0.4247500002384186
recall :  0.4247500002384186
auc :  0.7492674589157104

y_eval {0: 64588, 1: 35412}
pred {0: 77887, 1: 22107, 3: 6}
[INFO] confusion matrix for file 
[[42475 22107     0     6     0]
 [35412     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4803386 1431039       0     305       0]
 [ 456210   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 0 ... 1 0 0] (100000,)
[INFO] Validation score: [33m0.423[0m
[33m[INFO] metrics:[0m
loss :  84.25052659118653
tp :  42300.0
fp :  57700.0
tn :  342300.0
fn :  57700.0
accuracy :  0.769202470779419
precision :  0.4230000078678131
recall :  0.4230000078678131
auc :  0.7481082677841187

y_eval {0: 64276, 1: 35724}
pred {0: 78024, 1: 21972, 3: 4}
[INFO] confusion matrix for file 
[[42300 21972     0     4     0]
 [35724     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4845686 1453011       0     309       0]
 [ 491934   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 1 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.42211[0m
[33m[INFO] metrics:[0m
loss :  84.47270874633789
tp :  42211.0
fp :  57789.0
tn :  342211.0
fn :  57789.0
accuracy :  0.7688449025154114
precision :  0.422109991312027
recall :  0.422109991312027
auc :  0.7479966282844543

y_eval {0: 64281, 1: 35719}
pred {0: 77930, 1: 22057, 3: 13}
[INFO] confusion matrix for file 
[[42211 22057     0    13     0]
 [35719     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4887897 1475068       0     322       0]
 [ 527653   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 0 ... 1 1 1] (100000,)
[INFO] Validation score: [33m0.42147[0m
[33m[INFO] metrics:[0m
loss :  84.27059005371093
tp :  42147.0
fp :  57853.0
tn :  342147.0
fn :  57853.0
accuracy :  0.7685885429382324
precision :  0.4214699864387512
recall :  0.4214699864387512
auc :  0.7475770711898804

y_eval {0: 64139, 1: 35861}
pred {0: 78008, 1: 21985, 3: 7}
[INFO] confusion matrix for file 
[[42147 21985     0     7     0]
 [35861     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4930044 1497053       0     329       0]
 [ 563514   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] reading file data/SWaT2015-Attack-Files-v0.2/eval/2016-01-02_172943_122.log.part05_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0:99.9992%,2:0.0008%]
[INFO] ** type:[0:100.0%]
[INFO] ** i/f_name:[2:99.9984%,3:0.0016%]
[INFO] ** i/f_dir:[1:99.9984%,0:0.0016%]
[INFO] ** src:[22:38.0648%,9:26.1164%,7:18.2044%,1:17.6118%,21:0.001%,14:0.0008%,3:0.0004%,2:0.0004%]
[INFO] ** dst:[21:43.368%,10:20.8128%,8:18.2044%,9:17.6114%,15:0.0016%,22:0.0006%,16:0.0004%,24:0.0002%,18:0.0002%,6:0.0002%,2:0.0002%]
[INFO] ** proto:[0:99.998%,1:0.0016%,2:0.0004%]
[INFO] ** appi_name:[21:99.9974%,30:0.0016%,7:0.0006%,31:0.0002%,22:0.0002%]
[INFO] ** proxy_src_ip:[18:38.0648%,20:26.1164%,13:18.2044%,6:17.6118%,17:0.0016%,14:0.0004%,1:0.0004%,7:0.0002%]
[INFO] ** modbus_function_code:[0.02961:99.9974%,-33.760870000000004:0.0026%]
[INFO] ** modbus_function_description:[7:49.9992%,11:49.9982%,0:0.0026%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[3:26.1154%,1:20.8126%,5:18.204%,4:17.6112%,2:17.2518%,0:0.005%]
[INFO] ** service:[0.0048200000000000005:99.9974%,-211.73542999999998:0.0016%,-211.09762999999998:0.0006%,-186.43601999999998:0.0002%,-211.08818:0.0002%]
[INFO] ** s_port:[1.45442:26.1158%,-0.32151:20.8128%,-1.27613:18.204%,-0.23351:17.6114%,-0.33484:17.252%,-71.33197:0.0016%,1.45709:0.0004%,-1.2708:0.0002%,-71.14931:0.0002%,-0.24151:0.0002%,-2.02544:0.0002%,1.46509:0.0002%,10.73138:0.0002%,-0.24950999999999998:0.0002%,14.88321:0.0002%,-1.27347:0.0002%,-2.81074:0.0002%]
[INFO] ** classification:[normal:97.0224%,Single Stage Single Point:2.9776%]
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.63121[0m
[33m[INFO] metrics:[0m
loss :  35.24686325051308
tp :  63120.0
fp :  36879.0
tn :  363121.0
fn :  36880.0
accuracy :  0.8524919748306274
precision :  0.6312063336372375
recall :  0.6312000155448914
auc :  0.8799664378166199

y_eval {0: 85112, 1: 14888}
pred {0: 78009, 1: 21989, 3: 2}
[INFO] confusion matrix for file 
[[63121 21989     0     2     0]
 [14888     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[4993165 1519042       0     331       0]
 [ 578402   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78001[0m
[33m[INFO] metrics:[0m
loss :  0.3641624159240723
tp :  78000.0
fp :  21999.0
tn :  378001.0
fn :  22000.0
accuracy :  0.9120129942893982
precision :  0.7800077795982361
recall :  0.7799999713897705
auc :  0.9742793440818787

y_eval {0: 100000}
pred {0: 78001, 1: 21997, 3: 2}
[INFO] confusion matrix for file 
[[78001 21997     0     2     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:Using TensorFlow backend.

[[5071166 1541039       0     333       0]
 [ 578402   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78071[0m
[33m[INFO] metrics:[0m
loss :  0.3743244291591644
tp :  78071.0
fp :  21929.0
tn :  378071.0
fn :  21929.0
accuracy :  0.9122936129570007
precision :  0.780709981918335
recall :  0.780709981918335
auc :  0.9742932319641113

y_eval {0: 100000}
pred {0: 78071, 1: 21925, 3: 4}
[INFO] confusion matrix for file 
[[78071 21925     0     4     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[5149237 1562964       0     337       0]
 [ 578402   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.78108[0m
[33m[INFO] metrics:[0m
loss :  0.3514619186115265
tp :  78108.0
fp :  21892.0
tn :  378108.0
fn :  21892.0
accuracy :  0.912442684173584
precision :  0.7810800075531006
recall :  0.7810800075531006
auc :  0.9743629097938538

y_eval {0: 100000}
pred {0: 78108, 1: 21892}
[INFO] confusion matrix for file 
[[78108 21892     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[5227345 1584856       0     337       0]
 [ 578402   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.77989[0m
[33m[INFO] metrics:[0m
loss :  0.3777062187576294
tp :  77989.0
fp :  22011.0
tn :  377989.0
fn :  22011.0
accuracy :  0.9119663834571838
precision :  0.7798900008201599
recall :  0.7798900008201599
auc :  0.9742320775985718

y_eval {0: 100000}
pred {0: 77989, 1: 22006, 3: 5}
[INFO] confusion matrix for file 
[[77989 22006     0     5     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[5305334 1606862       0     342       0]
 [ 578402   51203       0       3       0]
 [  82955   26741       0       1       0]
 [  45355       0       0       0       0]
 [ 146710   56092       0       0       0]]
--- 259.7218029499054 seconds ---
