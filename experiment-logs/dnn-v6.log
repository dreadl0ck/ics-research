2020-02-06 02:08:02.589440: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-06 02:08:02.602692: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbd2b9874f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-06 02:08:02.602724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow backend.
WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
=============================
        TRAINING v0.4
=============================
Date: 2020-02-06 02:08:02.585828
[33m[INFO] using Sequential Dense layers[0m
[INFO] adding core layer 0
[INFO] created DNN
[33m[INFO] epoch 1/10[0m
[33m[INFO] loading file 1-1/1 on epoch 1/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.3659 - tp: 62186.0000 - fp: 242.0000 - tn: 319758.0000 - fn: 17814.0000 - accuracy: 0.9549 - precision: 0.9961 - recall: 0.7773 - auc: 0.9991 - val_loss: 1.0162e-07 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0428 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0000e+00 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0331 - tp: 79766.0000 - fp: 234.0000 - tn: 319766.0000 - fn: 234.0000 - accuracy: 0.9988 - precision: 0.9971 - recall: 0.9971 - auc: 0.9982 - val_loss: 3.0006 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.8526 - tp: 53456.0000 - fp: 26544.0000 - tn: 293456.0000 - fn: 26544.0000 - accuracy: 0.8673 - precision: 0.6682 - recall: 0.6682 - auc: 0.9166 - val_loss: 0.2058 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.5128 - tp: 58627.0000 - fp: 21373.0000 - tn: 298627.0000 - fn: 21373.0000 - accuracy: 0.8931 - precision: 0.7328 - recall: 0.7328 - auc: 0.9465 - val_loss: 0.2229 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-001-files-0-1
[33m[LOSS] 0.2228963176727295[0m
[33m[INFO] epoch 2/10[0m
[33m[INFO] loading file 1-1/1 on epoch 2/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0936 - tp: 79923.0000 - fp: 77.0000 - tn: 319923.0000 - fn: 77.0000 - accuracy: 0.9996 - precision: 0.9990 - recall: 0.9990 - auc: 0.9994 - val_loss: 0.0197 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0105 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0050 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0202 - tp: 79766.0000 - fp: 234.0000 - tn: 319766.0000 - fn: 234.0000 - accuracy: 0.9988 - precision: 0.9971 - recall: 0.9971 - auc: 0.9982 - val_loss: 2.6856 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 1.0342 - tp: 45016.0000 - fp: 34984.0000 - tn: 285016.0000 - fn: 34984.0000 - accuracy: 0.8251 - precision: 0.5627 - recall: 0.5627 - auc: 0.8907 - val_loss: 0.5565 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9610
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6425 - tp: 47813.0000 - fp: 32187.0000 - tn: 287813.0000 - fn: 32187.0000 - accuracy: 0.8391 - precision: 0.5977 - recall: 0.5977 - auc: 0.9022 - val_loss: 0.4152 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-002-files-0-1
[33m[LOSS] 0.41515131821632384[0m
[33m[INFO] epoch 3/10[0m
[33m[INFO] loading file 1-1/1 on epoch 3/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0898 - tp: 79981.0000 - fp: 19.0000 - tn: 319981.0000 - fn: 19.0000 - accuracy: 0.9999 - precision: 0.9998 - recall: 0.9998 - auc: 0.9998 - val_loss: 6.0922e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0108 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.9421e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0219 - tp: 79766.0000 - fp: 234.0000 - tn: 319766.0000 - fn: 234.0000 - accuracy: 0.9988 - precision: 0.9971 - recall: 0.9971 - auc: 0.9988 - val_loss: 2.6410 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9011 - tp: 43832.0000 - fp: 36168.0000 - tn: 283832.0000 - fn: 36168.0000 - accuracy: 0.8192 - precision: 0.5479 - recall: 0.5479 - auc: 0.8887 - val_loss: 0.6246 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9339
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6726 - tp: 44521.0000 - fp: 35479.0000 - tn: 284521.0000 - fn: 35479.0000 - accuracy: 0.8226 - precision: 0.5565 - recall: 0.5565 - auc: 0.8903 - val_loss: 0.5669 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-003-files-0-1
[33m[LOSS] 0.5668838532447815[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.5668838532447815  <  0.001
[33m[INFO] epoch 4/10[0m
[33m[INFO] loading file 1-1/1 on epoch 4/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0762 - tp: 79996.0000 - fp: 4.0000 - tn: 319996.0000 - fn: 4.0000 - accuracy: 1.0000 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 4.1795e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-004-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0103 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.7980e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-004-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0234 - tp: 79763.0000 - fp: 237.0000 - tn: 319763.0000 - fn: 237.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9988 - val_loss: 2.6881 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-004-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9262 - tp: 43822.0000 - fp: 36178.0000 - tn: 283822.0000 - fn: 36178.0000 - accuracy: 0.8191 - precision: 0.5478 - recall: 0.5478 - auc: 0.8881 - val_loss: 0.6156 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9339
[INFO] saving weights to checkpoints/dnn-epoch-004-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6734 - tp: 44428.0000 - fp: 35572.0000 - tn: 284428.0000 - fn: 35572.0000 - accuracy: 0.8221 - precision: 0.5554 - recall: 0.5554 - auc: 0.8904 - val_loss: 0.5441 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-004-files-0-1
[33m[LOSS] 0.5441243013381958[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.5441243013381958  <  0.001
[33m[INFO] epoch 5/10[0m
[33m[INFO] loading file 1-1/1 on epoch 5/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0738 - tp: 79997.0000 - fp: 3.0000 - tn: 319997.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3567e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-005-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0099 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.8432e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-005-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0272 - tp: 79762.0000 - fp: 238.0000 - tn: 319762.0000 - fn: 238.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9988 - val_loss: 2.8379 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-005-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9255 - tp: 43863.0000 - fp: 36137.0000 - tn: 283863.0000 - fn: 36137.0000 - accuracy: 0.8193 - precision: 0.5483 - recall: 0.5483 - auc: 0.8888 - val_loss: 0.6151 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9387
[INFO] saving weights to checkpoints/dnn-epoch-005-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6736 - tp: 44374.0000 - fp: 35626.0000 - tn: 284374.0000 - fn: 35626.0000 - accuracy: 0.8219 - precision: 0.5547 - recall: 0.5547 - auc: 0.8904 - val_loss: 0.6286 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-005-files-0-1
[33m[LOSS] 0.6286374074935913[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.6286374074935913  <  0.001
[33m[INFO] epoch 6/10[0m
[33m[INFO] loading file 1-1/1 on epoch 6/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0706 - tp: 79999.0000 - fp: 1.0000 - tn: 319999.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.5948e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-006-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0096 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.9682e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-006-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0241 - tp: 79759.0000 - fp: 241.0000 - tn: 319759.0000 - fn: 241.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9986 - val_loss: 2.6463 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-006-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9234 - tp: 43897.0000 - fp: 36103.0000 - tn: 283897.0000 - fn: 36103.0000 - accuracy: 0.8195 - precision: 0.5487 - recall: 0.5487 - auc: 0.8886 - val_loss: 0.6154 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9609
[INFO] saving weights to checkpoints/dnn-epoch-006-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6732 - tp: 44438.0000 - fp: 35562.0000 - tn: 284438.0000 - fn: 35562.0000 - accuracy: 0.8222 - precision: 0.5555 - recall: 0.5555 - auc: 0.8906 - val_loss: 0.5575 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-006-files-0-1
[33m[LOSS] 0.5575207685470581[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.5575207685470581  <  0.001
[33m[INFO] epoch 7/10[0m
[33m[INFO] loading file 1-1/1 on epoch 7/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0707 - tp: 79996.0000 - fp: 4.0000 - tn: 319996.0000 - fn: 4.0000 - accuracy: 1.0000 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 4.0814e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-007-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0096 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.2130e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-007-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0268 - tp: 79762.0000 - fp: 238.0000 - tn: 319762.0000 - fn: 238.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9986 - val_loss: 2.6290 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-007-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9253 - tp: 44039.0000 - fp: 35961.0000 - tn: 284039.0000 - fn: 35961.0000 - accuracy: 0.8202 - precision: 0.5505 - recall: 0.5505 - auc: 0.8888 - val_loss: 0.6222 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9339
[INFO] saving weights to checkpoints/dnn-epoch-007-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6742 - tp: 44380.0000 - fp: 35620.0000 - tn: 284380.0000 - fn: 35620.0000 - accuracy: 0.8219 - precision: 0.5548 - recall: 0.5548 - auc: 0.8900 - val_loss: 0.6266 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-007-files-0-1
[33m[LOSS] 0.6266236924171448[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.6266236924171448  <  0.001
[33m[INFO] epoch 8/10[0m
[33m[INFO] loading file 1-1/1 on epoch 8/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0700 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.8555e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-008-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0095 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.5604e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-008-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0224 - tp: 79763.0000 - fp: 237.0000 - tn: 319763.0000 - fn: 237.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9986 - val_loss: 2.5889 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-008-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9719 - tp: 44060.0000 - fp: 35940.0000 - tn: 284060.0000 - fn: 35940.0000 - accuracy: 0.8203 - precision: 0.5508 - recall: 0.5508 - auc: 0.8880 - val_loss: 0.5966 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9449
[INFO] saving weights to checkpoints/dnn-epoch-008-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6742 - tp: 44329.0000 - fp: 35671.0000 - tn: 284329.0000 - fn: 35671.0000 - accuracy: 0.8216 - precision: 0.5541 - recall: 0.5541 - auc: 0.8905 - val_loss: 0.5583 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-008-files-0-1
[33m[LOSS] 0.5582606091499328[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.5582606091499328  <  0.001
[33m[INFO] epoch 9/10[0m
[33m[INFO] loading file 1-1/1 on epoch 9/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0716 - tp: 79996.0000 - fp: 4.0000 - tn: 319996.0000 - fn: 4.0000 - accuracy: 1.0000 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 4.5586e-04 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-009-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0098 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.9553e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-009-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0267 - tp: 79763.0000 - fp: 237.0000 - tn: 319763.0000 - fn: 237.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9986 - val_loss: 2.5921 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-009-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9551 - tp: 43926.0000 - fp: 36074.0000 - tn: 283926.0000 - fn: 36074.0000 - accuracy: 0.8196 - precision: 0.5491 - recall: 0.5491 - auc: 0.8879 - val_loss: 0.5502 - val_tp: 15796.0000 - val_fp: 4204.0000 - val_tn: 75796.0000 - val_fn: 4204.0000 - val_accuracy: 0.9159 - val_precision: 0.7898 - val_recall: 0.7898 - val_auc: 0.9610
[INFO] saving weights to checkpoints/dnn-epoch-009-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6737 - tp: 44306.0000 - fp: 35694.0000 - tn: 284306.0000 - fn: 35694.0000 - accuracy: 0.8215 - precision: 0.5538 - recall: 0.5538 - auc: 0.8904 - val_loss: 0.5494 - val_tp: 15804.0000 - val_fp: 4196.0000 - val_tn: 75804.0000 - val_fn: 4196.0000 - val_accuracy: 0.9161 - val_precision: 0.7902 - val_recall: 0.7902 - val_auc: 0.9612
[INFO] saving weights to checkpoints/dnn-epoch-009-files-0-1
[33m[LOSS] 0.5493700493812561[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.5493700493812561  <  0.001
[33m[INFO] epoch 10/10[0m
[33m[INFO] loading file 1-1/1 on epoch 10/10[0m
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part12_sorted-labeled.csv
[INFO] concatenate the files
[INFO] process dataset, shape: (500000, 18)
[INFO] sampling 1.0
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] processing batch 0-100000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0716 - tp: 79998.0000 - fp: 2.0000 - tn: 319998.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.7336e-05 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-010-files-0-1
[INFO] processing batch 100000-200000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0098 - tp: 80000.0000 - fp: 0.0000e+00 - tn: 320000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.1088e-06 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-010-files-0-1
[INFO] processing batch 200000-300000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.0222 - tp: 79764.0000 - fp: 236.0000 - tn: 319764.0000 - fn: 236.0000 - accuracy: 0.9988 - precision: 0.9970 - recall: 0.9970 - auc: 0.9986 - val_loss: 2.5849 - val_tp: 10606.0000 - val_fp: 9394.0000 - val_tn: 70606.0000 - val_fn: 9394.0000 - val_accuracy: 0.8121 - val_precision: 0.5303 - val_recall: 0.5303 - val_auc: 0.7064
[INFO] saving weights to checkpoints/dnn-epoch-010-files-0-1
[INFO] processing batch 300000-400000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.9189 - tp: 47283.0000 - fp: 32717.0000 - tn: 287283.0000 - fn: 32717.0000 - accuracy: 0.8364 - precision: 0.5910 - recall: 0.5910 - auc: 0.8989 - val_loss: 0.4065 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-010-files-0-1
[INFO] processing batch 400000-500000/500000
[INFO] breaking into predictors and prediction...
[INFO] creating train/test split: 0.2
[INFO] fitting model
Train on 80000 samples, validate on 20000 samples
Epoch 1/1
 - 5s - loss: 0.6428 - tp: 47722.0000 - fp: 32278.0000 - tn: 287722.0000 - fn: 32278.0000 - accuracy: 0.8386 - precision: 0.5965 - recall: 0.5965 - auc: 0.9021 - val_loss: 0.3784 - val_tp: 20000.0000 - val_fp: 0.0000e+00 - val_tn: 80000.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000
[INFO] saving weights to checkpoints/dnn-epoch-010-files-0-1
[33m[LOSS] 0.3784349170684814[0m
[CHECKING EARLY STOP]: currentLoss < min_delta ? => 0.3784349170684814  <  0.001
--- 271.48834204673767 seconds ---
2020-02-06 02:12:38.444852: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-06 02:12:38.457643: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb1324cf420 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-02-06 02:12:38.457670: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Using TensorFlow backend.
WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
=============================
        SCORING v0.4
=============================
Date: 2020-02-06 02:12:38.441345
[33m[INFO] using Sequential Dense layers[0m
[INFO] adding core layer 0
[INFO] created DNN
loading weights: checkpoints/*
loading file checkpoints/dnn-epoch-010-files-0-1
[33m[INFO] model summary:[0m
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 16)                256       
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 64)                1088      
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                1040      
_________________________________________________________________
dropout_3 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 17        
_________________________________________________________________
dropout_4 (Dropout)          (None, 1)                 0         
_________________________________________________________________
dense_5 (Dense)              (None, 5)                 10        
=================================================================
Total params: 2,411
Trainable params: 2,411
Non-trainable params: 0
_________________________________________________________________
[INFO] reading file data/SWaT2015-Attack-Files-v0.4-minmax/train/2015-12-28_113021_98.log.part13_sorted-labeled.csv
[INFO] process dataset, shape: (500000, 18)
[33mdropping column: modbus_value[0m
dropping all time related columns...
[33mdropping column: unixtime[0m
[INFO] columns: Index(['orig', 'type', 'i/f_name', 'i/f_dir', 'src', 'dst', 'proto',
       'appi_name', 'proxy_src_ip', 'modbus_function_code',
       'modbus_function_description', 'modbus_transaction_id', 'scada_tag',
       'service', 's_port', 'classification'],
      dtype='object')
[INFO] analyze dataset: (500000, 16)

[INFO] analyzing data
[INFO] 500000 rows
[INFO] ** orig:[0.5:100.0%]
[INFO] ** type:[1.0:100.0%]
[INFO] ** i/f_name:[0.0:99.9996%,0.5:0.0004%]
[INFO] ** i/f_dir:[0.0:99.9996%,1.0:0.0004%]
[INFO] ** src:[0.0666666667:38.3832%,0.9333333333:25.9606%,0.6666666667:18.1352%,0.3333333333:17.52%,0.1333333333:0.0004%,1.0:0.0004%,0.5333333333:0.0002%]
[INFO] ** dst:[0.1764705882:43.3108%,0.9411764706:21.0326%,0.8823529412000001:18.135%,0.5294117647:17.5198%,0.0:0.0008%,0.2941176471:0.0004%,0.4117647059:0.0004%,0.23529411760000002:0.0002%]
[INFO] ** proto:[0.5:99.9996%,0.0:0.0004%]
[INFO] ** appi_name:[0.0:99.9992%,0.0384615385:0.0004%,0.5384615385:0.0002%,0.7307692308:0.0002%]
[INFO] ** proxy_src_ip:[0.6666666667:38.3832%,0.0:25.9606%,0.9333333333:18.1352%,0.4:17.52%,0.5333333333:0.0004%,0.7333333333:0.0004%,0.6:0.0002%]
[INFO] ** modbus_function_code:[0.9743589744:99.9992%,0.0:0.0008%]
[INFO] ** modbus_function_description:[0.2:50.0%,0.0:49.9992%,0.4:0.0008%]
[INFO] ** modbus_transaction_id:65536 (13.1072%)
[INFO] ** scada_tag:[1.0:25.9596%,0.6:21.0326%,0.8:18.1348%,0.0:17.5194%,0.4:17.3504%,0.2:0.0032%]
[INFO] ** service:[0.7025755984000001:99.9992%,0.0:0.0004%,0.0012540954:0.0002%,0.002116286:0.0002%]
[INFO] ** s_port:[0.834280824:25.96%,0.8139250565:21.0326%,0.8029830673999999:18.1348%,0.8149336757:17.5196%,0.8137722355:17.3506%,0.8343113882:0.0004%,0.0:0.0004%,0.8059172320999999:0.0002%,0.8030136315999999:0.0002%,0.8147502904:0.0002%,0.8030441959000001:0.0002%,0.814841983:0.0002%,0.9621003729000001:0.0002%,0.8345253376999999:0.0002%,0.7788831836:0.0002%]
[INFO] ** classification:[normal:72.7902%,Single Stage Single Point:27.2098%]
[INFO] columns with count within 2-10 {'i/f_name': 2, 'i/f_dir': 2, 'src': 7, 'dst': 8, 'proto': 2, 'appi_name': 4, 'proxy_src_ip': 7, 'modbus_function_code': 2, 'modbus_function_description': 3, 'scada_tag': 6, 'service': 4, 'classification': 2}
[INFO] processing batch 0-100000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 1 ... 0 1 0] (100000,)
[INFO] Validation score: [33m0.99999[0m
[33m[INFO] metrics:[0m
loss :  0.3786623117542267
tp :  99999.0
fp :  1.0
tn :  399999.0
fn :  1.0
accuracy :  0.9999960064888
precision :  0.9999899864196777
recall :  0.9999899864196777
auc :  0.9999963045120239

y_eval {0: 53024, 1: 46976}
pred {0: 53023, 1: 46977}
[INFO] confusion matrix for file 
[[53023     1     0     0     0]
 [    0 46976     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[53023     1     0     0     0]
 [    0 46976     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] processing batch 100000-200000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 1 0 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.6544[0m
[33m[INFO] metrics:[0m
loss :  2.5352114553070066
tp :  65440.0
fp :  34560.0
tn :  365440.0
fn :  34560.0
accuracy :  0.861760139465332
precision :  0.6543999910354614
recall :  0.6543999910354614
auc :  0.7960712909698486

y_eval {0: 87619, 1: 12381}
pred {0: 53059, 1: 46941}
[INFO] confusion matrix for file 
[[53059 34560     0     0     0]
 [    0 12381     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[106082  34561      0      0      0]
 [     0  59357      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 200000-300000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 0 ... 1 0 1] (100000,)
[INFO] Validation score: [33m0.70058[0m
[33m[INFO] metrics:[0m
loss :  2.245003558511734
tp :  70058.0
fp :  29942.0
tn :  370058.0
fn :  29942.0
accuracy :  0.8802317380905151
precision :  0.7005800008773804
recall :  0.7005800008773804
auc :  0.8233433365821838

y_eval {0: 83025, 1: 16975}
pred {0: 53083, 1: 46917}
[INFO] confusion matrix for file 
[[53083 29942     0     0     0]
 [    0 16975     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[159165  64503      0      0      0]
 [     0  76332      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 300000-400000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [1 0 0 ... 1 1 0] (100000,)
[INFO] Validation score: [33m0.99999[0m
[33m[INFO] metrics:[0m
loss :  0.3785499263572693
tp :  99999.0
fp :  1.0
tn :  399999.0
fn :  1.0
accuracy :  0.9999960064888
precision :  0.9999899864196777
recall :  0.9999899864196777
auc :  0.9999973773956299

y_eval {0: 52923, 1: 47077}
pred {0: 52922, 1: 47078}
[INFO] confusion matrix for file 
[[52922     1     0     0     0]
 [    0 47077     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[212087  64504      0      0      0]
 [     0 123409      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
[INFO] processing batch 400000-500000/500000
[33m[INFO] measuring accuracy...[0m
x_test.shape: (100000, 15)
y_eval [0 0 1 ... 0 0 0] (100000,)
[INFO] Validation score: [33m0.65583[0m
[33m[INFO] metrics:[0m
loss :  2.5262717546367646
tp :  65583.0
fp :  34417.0
tn :  365583.0
fn :  34417.0
accuracy :  0.8623335361480713
precision :  0.6558300256729126
recall :  0.6558300256729126
auc :  0.7969686388969421

y_eval {0: 87360, 1: 12640}
pred {0: 52943, 1: 47057}
[INFO] confusion matrix for file 
[[52943 34417     0     0     0]
 [    0 12640     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]
 [    0     0     0     0     0]]
[INFO] confusion matrix after adding it to total:
[[265030  98921      0      0      0]
 [     0 136049      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]
 [     0      0      0      0      0]]
--- 20.403682708740234 seconds ---
